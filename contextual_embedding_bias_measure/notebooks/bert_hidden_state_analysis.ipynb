{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_utils import Config, BertPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    model_type=\"bert-base-uncased\",\n",
    "    max_seq_len=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BertPreprocessor(config.model_type, config.max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import BertConfig, BertForMaskedLM\n",
    "model = BertForMaskedLM.from_pretrained(config.model_type)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_output, pooled_output = model.bert(processor.to_bert_model_input(\"hello world\"),\n",
    "                                            output_all_encoded_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(sentence: str, word: str, n_calc: int=10):\n",
    "    idx = processor.get_index(sentence, word)\n",
    "    outputs = None\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_calc):\n",
    "            sequence_output, _ = model.bert(processor.to_bert_model_input(sentence),\n",
    "                                            output_all_encoded_layers=False)\n",
    "            sequence_output.squeeze_(0)\n",
    "            if outputs is None: outputs = torch.zeros_like(sequence_output)\n",
    "            outputs = sequence_output + outputs\n",
    "    return outputs.detach().cpu().numpy()[idx] / n_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(x, y):\n",
    "    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1 = get_word_vector(\"he is a programmer.\", \"programmer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec2 = get_word_vector(\"she is a programmer.\", \"programmer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1893053"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(vec1 - vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff1 = vec1 - vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_softmax = model.cls.predictions.decoder.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30522, 768)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_softmax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordering = ((out_softmax @ vec1)).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30522,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out_softmax @ vec1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20273"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.token_to_index(\"programmer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordering = (-(out_softmax @ vec1)).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.8011818, -1.7252083, -1.6536064, ...,  3.104794 ,  4.3238416,\n",
       "        4.811603 ], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out_softmax @ vec1)[(out_softmax @ vec1).argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{29024: 'programmer',\n",
       " 22224: '[CLS]',\n",
       " 19482: '##kato',\n",
       " 7975: 'programmers',\n",
       " 12416: 'computational',\n",
       " 24599: 'mathematicians',\n",
       " 14957: 'hindwings',\n",
       " 24939: 'constructions',\n",
       " 8656: 'mathematical',\n",
       " 16697: 'planner'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordering = (-(out_softmax @ vec1)).argsort()\n",
    "{\n",
    "    ordering[i] + 1: processor.index_to_token(i)\n",
    "    for i in ordering[:10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{20919: '[CLS]',\n",
       " 12791: 'programmer',\n",
       " 21291: '##kato',\n",
       " 6802: 'mathematical',\n",
       " 6215: 'programmers',\n",
       " 10774: 'computational',\n",
       " 7348: 'constructions',\n",
       " 5195: 'mathematicians',\n",
       " 29172: 'mathematician',\n",
       " 16189: 'nguyen'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordering = (-(out_softmax @ vec2)).argsort()\n",
    "{\n",
    "    ordering[i] + 1: processor.index_to_token(i)\n",
    "    for i in ordering[:10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{29632: 'lublin',\n",
       " 13620: 'princess',\n",
       " 25972: '##lika',\n",
       " 8296: '##sell',\n",
       " 20209: 'selma',\n",
       " 23939: '##rgen',\n",
       " 12656: '##ouk',\n",
       " 8478: '##rock',\n",
       " 1013: '##sant',\n",
       " 18130: 'joan'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordering = (-(out_softmax @ (vec1 - vec2))).argsort()\n",
    "{\n",
    "    ordering[i] + 1: processor.index_to_token(i)\n",
    "    for i in ordering[:10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1 = get_word_vector(\"he is a person\", \"person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec2 = get_word_vector(\"she is a person\", \"person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6573544"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(vec1 - vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff2 = vec1 - vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14555529"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(diff1, diff2) / (np.linalg.norm(diff1) * np.linalg.norm(diff2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No [MASK] tokenn tokens [he, is, [, [UNK]] found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-4061ffbc85c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvec1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_word_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"he is [MASK].\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"[MASK]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mvec2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_word_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"she is [MASK].\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"[MASK]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdiff3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mvec2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-91000d448f3a>\u001b[0m in \u001b[0;36mget_word_vector\u001b[1;34m(sentence, word, n_calc)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_word_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_calc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_calc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\programming\\python\\LING78000\\contextual_embedding_bias_measure\\lib\\bert_utils.py\u001b[0m in \u001b[0;36mget_index\u001b[1;34m(self, sentence, word, accept_wordpiece, last)\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"No {word} tokenn tokens {toks} found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_bert_model_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_sentence\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No [MASK] tokenn tokens [he, is, [, [UNK]] found"
     ]
    }
   ],
   "source": [
    "vec1 = get_word_vector(\"he is [MASK].\", \"[MASK]\")\n",
    "vec2 = get_word_vector(\"she is [MASK].\", \"[MASK]\")\n",
    "diff3 = vec1 - vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2478: '[CLS]',\n",
       " 1001: '[MASK]',\n",
       " 5380: 'a',\n",
       " 6562: 'person',\n",
       " 2202: 'an',\n",
       " 1164: 'the',\n",
       " 7948: '.',\n",
       " 28087: 'pilgrim',\n",
       " 4535: 'something',\n",
       " 24484: '##erving'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordering = (-(out_softmax @ vec1)).argsort()\n",
    "{\n",
    "    ordering[i] + 1: processor.index_to_token(i)\n",
    "    for i in ordering[:10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{17933: '[CLS]',\n",
       " 19149: '[MASK]',\n",
       " 27416: 'a',\n",
       " 2832: 'an',\n",
       " 17111: 'the',\n",
       " 10499: 'person',\n",
       " 8820: '.',\n",
       " 13042: 'is',\n",
       " 4283: ',',\n",
       " 8700: 'and'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordering = (-(out_softmax @ vec2)).argsort()\n",
    "{\n",
    "    ordering[i] + 1: processor.index_to_token(i)\n",
    "    for i in ordering[:10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{22855: '##odies',\n",
       " 11857: 'turing',\n",
       " 25162: 'nipples',\n",
       " 10123: 'brotherhood',\n",
       " 14365: '##duced',\n",
       " 18019: '##boys',\n",
       " 8307: '##verted',\n",
       " 23766: '##gues',\n",
       " 16504: 'bikini',\n",
       " 16188: 'beaux'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordering = (-(out_softmax @ (vec1 - vec2))).argsort()\n",
    "{\n",
    "    ordering[i] + 1: processor.index_to_token(i)\n",
    "    for i in ordering[:10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax layer analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '[PAD]',\n",
       " 1: '[unused0]',\n",
       " 2: '[unused1]',\n",
       " 3: '[unused2]',\n",
       " 4: '[unused3]',\n",
       " 5: '[unused4]',\n",
       " 6: '[unused5]',\n",
       " 7: '[unused6]',\n",
       " 8: '[unused7]',\n",
       " 9: '[unused8]',\n",
       " 10: '[unused9]',\n",
       " 11: '[unused10]',\n",
       " 12: '[unused11]',\n",
       " 13: '[unused12]',\n",
       " 14: '[unused13]',\n",
       " 15: '[unused14]',\n",
       " 16: '[unused15]',\n",
       " 17: '[unused16]',\n",
       " 18: '[unused17]',\n",
       " 19: '[unused18]',\n",
       " 20: '[unused19]',\n",
       " 21: '[unused20]',\n",
       " 22: '[unused21]',\n",
       " 23: '[unused22]',\n",
       " 24: '[unused23]',\n",
       " 25: '[unused24]',\n",
       " 26: '[unused25]',\n",
       " 27: '[unused26]',\n",
       " 28: '[unused27]',\n",
       " 29: '[unused28]',\n",
       " 30: '[unused29]',\n",
       " 31: '[unused30]',\n",
       " 32: '[unused31]',\n",
       " 33: '[unused32]',\n",
       " 34: '[unused33]',\n",
       " 35: '[unused34]',\n",
       " 36: '[unused35]',\n",
       " 37: '[unused36]',\n",
       " 38: '[unused37]',\n",
       " 39: '[unused38]',\n",
       " 40: '[unused39]',\n",
       " 41: '[unused40]',\n",
       " 42: '[unused41]',\n",
       " 43: '[unused42]',\n",
       " 44: '[unused43]',\n",
       " 45: '[unused44]',\n",
       " 46: '[unused45]',\n",
       " 47: '[unused46]',\n",
       " 48: '[unused47]',\n",
       " 49: '[unused48]',\n",
       " 50: '[unused49]',\n",
       " 51: '[unused50]',\n",
       " 52: '[unused51]',\n",
       " 53: '[unused52]',\n",
       " 54: '[unused53]',\n",
       " 55: '[unused54]',\n",
       " 56: '[unused55]',\n",
       " 57: '[unused56]',\n",
       " 58: '[unused57]',\n",
       " 59: '[unused58]',\n",
       " 60: '[unused59]',\n",
       " 61: '[unused60]',\n",
       " 62: '[unused61]',\n",
       " 63: '[unused62]',\n",
       " 64: '[unused63]',\n",
       " 65: '[unused64]',\n",
       " 66: '[unused65]',\n",
       " 67: '[unused66]',\n",
       " 68: '[unused67]',\n",
       " 69: '[unused68]',\n",
       " 70: '[unused69]',\n",
       " 71: '[unused70]',\n",
       " 72: '[unused71]',\n",
       " 73: '[unused72]',\n",
       " 74: '[unused73]',\n",
       " 75: '[unused74]',\n",
       " 76: '[unused75]',\n",
       " 77: '[unused76]',\n",
       " 78: '[unused77]',\n",
       " 79: '[unused78]',\n",
       " 80: '[unused79]',\n",
       " 81: '[unused80]',\n",
       " 82: '[unused81]',\n",
       " 83: '[unused82]',\n",
       " 84: '[unused83]',\n",
       " 85: '[unused84]',\n",
       " 86: '[unused85]',\n",
       " 87: '[unused86]',\n",
       " 88: '[unused87]',\n",
       " 89: '[unused88]',\n",
       " 90: '[unused89]',\n",
       " 91: '[unused90]',\n",
       " 92: '[unused91]',\n",
       " 93: '[unused92]',\n",
       " 94: '[unused93]',\n",
       " 95: '[unused94]',\n",
       " 96: '[unused95]',\n",
       " 97: '[unused96]',\n",
       " 98: '[unused97]',\n",
       " 99: '[unused98]',\n",
       " 100: '[UNK]',\n",
       " 101: '[CLS]',\n",
       " 102: '[SEP]',\n",
       " 103: '[MASK]',\n",
       " 104: '[unused99]',\n",
       " 105: '[unused100]',\n",
       " 106: '[unused101]',\n",
       " 107: '[unused102]',\n",
       " 108: '[unused103]',\n",
       " 109: '[unused104]',\n",
       " 110: '[unused105]',\n",
       " 111: '[unused106]',\n",
       " 112: '[unused107]',\n",
       " 113: '[unused108]',\n",
       " 114: '[unused109]',\n",
       " 115: '[unused110]',\n",
       " 116: '[unused111]',\n",
       " 117: '[unused112]',\n",
       " 118: '[unused113]',\n",
       " 119: '[unused114]',\n",
       " 120: '[unused115]',\n",
       " 121: '[unused116]',\n",
       " 122: '[unused117]',\n",
       " 123: '[unused118]',\n",
       " 124: '[unused119]',\n",
       " 125: '[unused120]',\n",
       " 126: '[unused121]',\n",
       " 127: '[unused122]',\n",
       " 128: '[unused123]',\n",
       " 129: '[unused124]',\n",
       " 130: '[unused125]',\n",
       " 131: '[unused126]',\n",
       " 132: '[unused127]',\n",
       " 133: '[unused128]',\n",
       " 134: '[unused129]',\n",
       " 135: '[unused130]',\n",
       " 136: '[unused131]',\n",
       " 137: '[unused132]',\n",
       " 138: '[unused133]',\n",
       " 139: '[unused134]',\n",
       " 140: '[unused135]',\n",
       " 141: '[unused136]',\n",
       " 142: '[unused137]',\n",
       " 143: '[unused138]',\n",
       " 144: '[unused139]',\n",
       " 145: '[unused140]',\n",
       " 146: '[unused141]',\n",
       " 147: '[unused142]',\n",
       " 148: '[unused143]',\n",
       " 149: '[unused144]',\n",
       " 150: '[unused145]',\n",
       " 151: '[unused146]',\n",
       " 152: '[unused147]',\n",
       " 153: '[unused148]',\n",
       " 154: '[unused149]',\n",
       " 155: '[unused150]',\n",
       " 156: '[unused151]',\n",
       " 157: '[unused152]',\n",
       " 158: '[unused153]',\n",
       " 159: '[unused154]',\n",
       " 160: '[unused155]',\n",
       " 161: '[unused156]',\n",
       " 162: '[unused157]',\n",
       " 163: '[unused158]',\n",
       " 164: '[unused159]',\n",
       " 165: '[unused160]',\n",
       " 166: '[unused161]',\n",
       " 167: '[unused162]',\n",
       " 168: '[unused163]',\n",
       " 169: '[unused164]',\n",
       " 170: '[unused165]',\n",
       " 171: '[unused166]',\n",
       " 172: '[unused167]',\n",
       " 173: '[unused168]',\n",
       " 174: '[unused169]',\n",
       " 175: '[unused170]',\n",
       " 176: '[unused171]',\n",
       " 177: '[unused172]',\n",
       " 178: '[unused173]',\n",
       " 179: '[unused174]',\n",
       " 180: '[unused175]',\n",
       " 181: '[unused176]',\n",
       " 182: '[unused177]',\n",
       " 183: '[unused178]',\n",
       " 184: '[unused179]',\n",
       " 185: '[unused180]',\n",
       " 186: '[unused181]',\n",
       " 187: '[unused182]',\n",
       " 188: '[unused183]',\n",
       " 189: '[unused184]',\n",
       " 190: '[unused185]',\n",
       " 191: '[unused186]',\n",
       " 192: '[unused187]',\n",
       " 193: '[unused188]',\n",
       " 194: '[unused189]',\n",
       " 195: '[unused190]',\n",
       " 196: '[unused191]',\n",
       " 197: '[unused192]',\n",
       " 198: '[unused193]',\n",
       " 199: '[unused194]',\n",
       " 200: '[unused195]',\n",
       " 201: '[unused196]',\n",
       " 202: '[unused197]',\n",
       " 203: '[unused198]',\n",
       " 204: '[unused199]',\n",
       " 205: '[unused200]',\n",
       " 206: '[unused201]',\n",
       " 207: '[unused202]',\n",
       " 208: '[unused203]',\n",
       " 209: '[unused204]',\n",
       " 210: '[unused205]',\n",
       " 211: '[unused206]',\n",
       " 212: '[unused207]',\n",
       " 213: '[unused208]',\n",
       " 214: '[unused209]',\n",
       " 215: '[unused210]',\n",
       " 216: '[unused211]',\n",
       " 217: '[unused212]',\n",
       " 218: '[unused213]',\n",
       " 219: '[unused214]',\n",
       " 220: '[unused215]',\n",
       " 221: '[unused216]',\n",
       " 222: '[unused217]',\n",
       " 223: '[unused218]',\n",
       " 224: '[unused219]',\n",
       " 225: '[unused220]',\n",
       " 226: '[unused221]',\n",
       " 227: '[unused222]',\n",
       " 228: '[unused223]',\n",
       " 229: '[unused224]',\n",
       " 230: '[unused225]',\n",
       " 231: '[unused226]',\n",
       " 232: '[unused227]',\n",
       " 233: '[unused228]',\n",
       " 234: '[unused229]',\n",
       " 235: '[unused230]',\n",
       " 236: '[unused231]',\n",
       " 237: '[unused232]',\n",
       " 238: '[unused233]',\n",
       " 239: '[unused234]',\n",
       " 240: '[unused235]',\n",
       " 241: '[unused236]',\n",
       " 242: '[unused237]',\n",
       " 243: '[unused238]',\n",
       " 244: '[unused239]',\n",
       " 245: '[unused240]',\n",
       " 246: '[unused241]',\n",
       " 247: '[unused242]',\n",
       " 248: '[unused243]',\n",
       " 249: '[unused244]',\n",
       " 250: '[unused245]',\n",
       " 251: '[unused246]',\n",
       " 252: '[unused247]',\n",
       " 253: '[unused248]',\n",
       " 254: '[unused249]',\n",
       " 255: '[unused250]',\n",
       " 256: '[unused251]',\n",
       " 257: '[unused252]',\n",
       " 258: '[unused253]',\n",
       " 259: '[unused254]',\n",
       " 260: '[unused255]',\n",
       " 261: '[unused256]',\n",
       " 262: '[unused257]',\n",
       " 263: '[unused258]',\n",
       " 264: '[unused259]',\n",
       " 265: '[unused260]',\n",
       " 266: '[unused261]',\n",
       " 267: '[unused262]',\n",
       " 268: '[unused263]',\n",
       " 269: '[unused264]',\n",
       " 270: '[unused265]',\n",
       " 271: '[unused266]',\n",
       " 272: '[unused267]',\n",
       " 273: '[unused268]',\n",
       " 274: '[unused269]',\n",
       " 275: '[unused270]',\n",
       " 276: '[unused271]',\n",
       " 277: '[unused272]',\n",
       " 278: '[unused273]',\n",
       " 279: '[unused274]',\n",
       " 280: '[unused275]',\n",
       " 281: '[unused276]',\n",
       " 282: '[unused277]',\n",
       " 283: '[unused278]',\n",
       " 284: '[unused279]',\n",
       " 285: '[unused280]',\n",
       " 286: '[unused281]',\n",
       " 287: '[unused282]',\n",
       " 288: '[unused283]',\n",
       " 289: '[unused284]',\n",
       " 290: '[unused285]',\n",
       " 291: '[unused286]',\n",
       " 292: '[unused287]',\n",
       " 293: '[unused288]',\n",
       " 294: '[unused289]',\n",
       " 295: '[unused290]',\n",
       " 296: '[unused291]',\n",
       " 297: '[unused292]',\n",
       " 298: '[unused293]',\n",
       " 299: '[unused294]',\n",
       " 300: '[unused295]',\n",
       " 301: '[unused296]',\n",
       " 302: '[unused297]',\n",
       " 303: '[unused298]',\n",
       " 304: '[unused299]',\n",
       " 305: '[unused300]',\n",
       " 306: '[unused301]',\n",
       " 307: '[unused302]',\n",
       " 308: '[unused303]',\n",
       " 309: '[unused304]',\n",
       " 310: '[unused305]',\n",
       " 311: '[unused306]',\n",
       " 312: '[unused307]',\n",
       " 313: '[unused308]',\n",
       " 314: '[unused309]',\n",
       " 315: '[unused310]',\n",
       " 316: '[unused311]',\n",
       " 317: '[unused312]',\n",
       " 318: '[unused313]',\n",
       " 319: '[unused314]',\n",
       " 320: '[unused315]',\n",
       " 321: '[unused316]',\n",
       " 322: '[unused317]',\n",
       " 323: '[unused318]',\n",
       " 324: '[unused319]',\n",
       " 325: '[unused320]',\n",
       " 326: '[unused321]',\n",
       " 327: '[unused322]',\n",
       " 328: '[unused323]',\n",
       " 329: '[unused324]',\n",
       " 330: '[unused325]',\n",
       " 331: '[unused326]',\n",
       " 332: '[unused327]',\n",
       " 333: '[unused328]',\n",
       " 334: '[unused329]',\n",
       " 335: '[unused330]',\n",
       " 336: '[unused331]',\n",
       " 337: '[unused332]',\n",
       " 338: '[unused333]',\n",
       " 339: '[unused334]',\n",
       " 340: '[unused335]',\n",
       " 341: '[unused336]',\n",
       " 342: '[unused337]',\n",
       " 343: '[unused338]',\n",
       " 344: '[unused339]',\n",
       " 345: '[unused340]',\n",
       " 346: '[unused341]',\n",
       " 347: '[unused342]',\n",
       " 348: '[unused343]',\n",
       " 349: '[unused344]',\n",
       " 350: '[unused345]',\n",
       " 351: '[unused346]',\n",
       " 352: '[unused347]',\n",
       " 353: '[unused348]',\n",
       " 354: '[unused349]',\n",
       " 355: '[unused350]',\n",
       " 356: '[unused351]',\n",
       " 357: '[unused352]',\n",
       " 358: '[unused353]',\n",
       " 359: '[unused354]',\n",
       " 360: '[unused355]',\n",
       " 361: '[unused356]',\n",
       " 362: '[unused357]',\n",
       " 363: '[unused358]',\n",
       " 364: '[unused359]',\n",
       " 365: '[unused360]',\n",
       " 366: '[unused361]',\n",
       " 367: '[unused362]',\n",
       " 368: '[unused363]',\n",
       " 369: '[unused364]',\n",
       " 370: '[unused365]',\n",
       " 371: '[unused366]',\n",
       " 372: '[unused367]',\n",
       " 373: '[unused368]',\n",
       " 374: '[unused369]',\n",
       " 375: '[unused370]',\n",
       " 376: '[unused371]',\n",
       " 377: '[unused372]',\n",
       " 378: '[unused373]',\n",
       " 379: '[unused374]',\n",
       " 380: '[unused375]',\n",
       " 381: '[unused376]',\n",
       " 382: '[unused377]',\n",
       " 383: '[unused378]',\n",
       " 384: '[unused379]',\n",
       " 385: '[unused380]',\n",
       " 386: '[unused381]',\n",
       " 387: '[unused382]',\n",
       " 388: '[unused383]',\n",
       " 389: '[unused384]',\n",
       " 390: '[unused385]',\n",
       " 391: '[unused386]',\n",
       " 392: '[unused387]',\n",
       " 393: '[unused388]',\n",
       " 394: '[unused389]',\n",
       " 395: '[unused390]',\n",
       " 396: '[unused391]',\n",
       " 397: '[unused392]',\n",
       " 398: '[unused393]',\n",
       " 399: '[unused394]',\n",
       " 400: '[unused395]',\n",
       " 401: '[unused396]',\n",
       " 402: '[unused397]',\n",
       " 403: '[unused398]',\n",
       " 404: '[unused399]',\n",
       " 405: '[unused400]',\n",
       " 406: '[unused401]',\n",
       " 407: '[unused402]',\n",
       " 408: '[unused403]',\n",
       " 409: '[unused404]',\n",
       " 410: '[unused405]',\n",
       " 411: '[unused406]',\n",
       " 412: '[unused407]',\n",
       " 413: '[unused408]',\n",
       " 414: '[unused409]',\n",
       " 415: '[unused410]',\n",
       " 416: '[unused411]',\n",
       " 417: '[unused412]',\n",
       " 418: '[unused413]',\n",
       " 419: '[unused414]',\n",
       " 420: '[unused415]',\n",
       " 421: '[unused416]',\n",
       " 422: '[unused417]',\n",
       " 423: '[unused418]',\n",
       " 424: '[unused419]',\n",
       " 425: '[unused420]',\n",
       " 426: '[unused421]',\n",
       " 427: '[unused422]',\n",
       " 428: '[unused423]',\n",
       " 429: '[unused424]',\n",
       " 430: '[unused425]',\n",
       " 431: '[unused426]',\n",
       " 432: '[unused427]',\n",
       " 433: '[unused428]',\n",
       " 434: '[unused429]',\n",
       " 435: '[unused430]',\n",
       " 436: '[unused431]',\n",
       " 437: '[unused432]',\n",
       " 438: '[unused433]',\n",
       " 439: '[unused434]',\n",
       " 440: '[unused435]',\n",
       " 441: '[unused436]',\n",
       " 442: '[unused437]',\n",
       " 443: '[unused438]',\n",
       " 444: '[unused439]',\n",
       " 445: '[unused440]',\n",
       " 446: '[unused441]',\n",
       " 447: '[unused442]',\n",
       " 448: '[unused443]',\n",
       " 449: '[unused444]',\n",
       " 450: '[unused445]',\n",
       " 451: '[unused446]',\n",
       " 452: '[unused447]',\n",
       " 453: '[unused448]',\n",
       " 454: '[unused449]',\n",
       " 455: '[unused450]',\n",
       " 456: '[unused451]',\n",
       " 457: '[unused452]',\n",
       " 458: '[unused453]',\n",
       " 459: '[unused454]',\n",
       " 460: '[unused455]',\n",
       " 461: '[unused456]',\n",
       " 462: '[unused457]',\n",
       " 463: '[unused458]',\n",
       " 464: '[unused459]',\n",
       " 465: '[unused460]',\n",
       " 466: '[unused461]',\n",
       " 467: '[unused462]',\n",
       " 468: '[unused463]',\n",
       " 469: '[unused464]',\n",
       " 470: '[unused465]',\n",
       " 471: '[unused466]',\n",
       " 472: '[unused467]',\n",
       " 473: '[unused468]',\n",
       " 474: '[unused469]',\n",
       " 475: '[unused470]',\n",
       " 476: '[unused471]',\n",
       " 477: '[unused472]',\n",
       " 478: '[unused473]',\n",
       " 479: '[unused474]',\n",
       " 480: '[unused475]',\n",
       " 481: '[unused476]',\n",
       " 482: '[unused477]',\n",
       " 483: '[unused478]',\n",
       " 484: '[unused479]',\n",
       " 485: '[unused480]',\n",
       " 486: '[unused481]',\n",
       " 487: '[unused482]',\n",
       " 488: '[unused483]',\n",
       " 489: '[unused484]',\n",
       " 490: '[unused485]',\n",
       " 491: '[unused486]',\n",
       " 492: '[unused487]',\n",
       " 493: '[unused488]',\n",
       " 494: '[unused489]',\n",
       " 495: '[unused490]',\n",
       " 496: '[unused491]',\n",
       " 497: '[unused492]',\n",
       " 498: '[unused493]',\n",
       " 499: '[unused494]',\n",
       " 500: '[unused495]',\n",
       " 501: '[unused496]',\n",
       " 502: '[unused497]',\n",
       " 503: '[unused498]',\n",
       " 504: '[unused499]',\n",
       " 505: '[unused500]',\n",
       " 506: '[unused501]',\n",
       " 507: '[unused502]',\n",
       " 508: '[unused503]',\n",
       " 509: '[unused504]',\n",
       " 510: '[unused505]',\n",
       " 511: '[unused506]',\n",
       " 512: '[unused507]',\n",
       " 513: '[unused508]',\n",
       " 514: '[unused509]',\n",
       " 515: '[unused510]',\n",
       " 516: '[unused511]',\n",
       " 517: '[unused512]',\n",
       " 518: '[unused513]',\n",
       " 519: '[unused514]',\n",
       " 520: '[unused515]',\n",
       " 521: '[unused516]',\n",
       " 522: '[unused517]',\n",
       " 523: '[unused518]',\n",
       " 524: '[unused519]',\n",
       " 525: '[unused520]',\n",
       " 526: '[unused521]',\n",
       " 527: '[unused522]',\n",
       " 528: '[unused523]',\n",
       " 529: '[unused524]',\n",
       " 530: '[unused525]',\n",
       " 531: '[unused526]',\n",
       " 532: '[unused527]',\n",
       " 533: '[unused528]',\n",
       " 534: '[unused529]',\n",
       " 535: '[unused530]',\n",
       " 536: '[unused531]',\n",
       " 537: '[unused532]',\n",
       " 538: '[unused533]',\n",
       " 539: '[unused534]',\n",
       " 540: '[unused535]',\n",
       " 541: '[unused536]',\n",
       " 542: '[unused537]',\n",
       " 543: '[unused538]',\n",
       " 544: '[unused539]',\n",
       " 545: '[unused540]',\n",
       " 546: '[unused541]',\n",
       " 547: '[unused542]',\n",
       " 548: '[unused543]',\n",
       " 549: '[unused544]',\n",
       " 550: '[unused545]',\n",
       " 551: '[unused546]',\n",
       " 552: '[unused547]',\n",
       " 553: '[unused548]',\n",
       " 554: '[unused549]',\n",
       " 555: '[unused550]',\n",
       " 556: '[unused551]',\n",
       " 557: '[unused552]',\n",
       " 558: '[unused553]',\n",
       " 559: '[unused554]',\n",
       " 560: '[unused555]',\n",
       " 561: '[unused556]',\n",
       " 562: '[unused557]',\n",
       " 563: '[unused558]',\n",
       " 564: '[unused559]',\n",
       " 565: '[unused560]',\n",
       " 566: '[unused561]',\n",
       " 567: '[unused562]',\n",
       " 568: '[unused563]',\n",
       " 569: '[unused564]',\n",
       " 570: '[unused565]',\n",
       " 571: '[unused566]',\n",
       " 572: '[unused567]',\n",
       " 573: '[unused568]',\n",
       " 574: '[unused569]',\n",
       " 575: '[unused570]',\n",
       " 576: '[unused571]',\n",
       " 577: '[unused572]',\n",
       " 578: '[unused573]',\n",
       " 579: '[unused574]',\n",
       " 580: '[unused575]',\n",
       " 581: '[unused576]',\n",
       " 582: '[unused577]',\n",
       " 583: '[unused578]',\n",
       " 584: '[unused579]',\n",
       " 585: '[unused580]',\n",
       " 586: '[unused581]',\n",
       " 587: '[unused582]',\n",
       " 588: '[unused583]',\n",
       " 589: '[unused584]',\n",
       " 590: '[unused585]',\n",
       " 591: '[unused586]',\n",
       " 592: '[unused587]',\n",
       " 593: '[unused588]',\n",
       " 594: '[unused589]',\n",
       " 595: '[unused590]',\n",
       " 596: '[unused591]',\n",
       " 597: '[unused592]',\n",
       " 598: '[unused593]',\n",
       " 599: '[unused594]',\n",
       " 600: '[unused595]',\n",
       " 601: '[unused596]',\n",
       " 602: '[unused597]',\n",
       " 603: '[unused598]',\n",
       " 604: '[unused599]',\n",
       " 605: '[unused600]',\n",
       " 606: '[unused601]',\n",
       " 607: '[unused602]',\n",
       " 608: '[unused603]',\n",
       " 609: '[unused604]',\n",
       " 610: '[unused605]',\n",
       " 611: '[unused606]',\n",
       " 612: '[unused607]',\n",
       " 613: '[unused608]',\n",
       " 614: '[unused609]',\n",
       " 615: '[unused610]',\n",
       " 616: '[unused611]',\n",
       " 617: '[unused612]',\n",
       " 618: '[unused613]',\n",
       " 619: '[unused614]',\n",
       " 620: '[unused615]',\n",
       " 621: '[unused616]',\n",
       " 622: '[unused617]',\n",
       " 623: '[unused618]',\n",
       " 624: '[unused619]',\n",
       " 625: '[unused620]',\n",
       " 626: '[unused621]',\n",
       " 627: '[unused622]',\n",
       " 628: '[unused623]',\n",
       " 629: '[unused624]',\n",
       " 630: '[unused625]',\n",
       " 631: '[unused626]',\n",
       " 632: '[unused627]',\n",
       " 633: '[unused628]',\n",
       " 634: '[unused629]',\n",
       " 635: '[unused630]',\n",
       " 636: '[unused631]',\n",
       " 637: '[unused632]',\n",
       " 638: '[unused633]',\n",
       " 639: '[unused634]',\n",
       " 640: '[unused635]',\n",
       " 641: '[unused636]',\n",
       " 642: '[unused637]',\n",
       " 643: '[unused638]',\n",
       " 644: '[unused639]',\n",
       " 645: '[unused640]',\n",
       " 646: '[unused641]',\n",
       " 647: '[unused642]',\n",
       " 648: '[unused643]',\n",
       " 649: '[unused644]',\n",
       " 650: '[unused645]',\n",
       " 651: '[unused646]',\n",
       " 652: '[unused647]',\n",
       " 653: '[unused648]',\n",
       " 654: '[unused649]',\n",
       " 655: '[unused650]',\n",
       " 656: '[unused651]',\n",
       " 657: '[unused652]',\n",
       " 658: '[unused653]',\n",
       " 659: '[unused654]',\n",
       " 660: '[unused655]',\n",
       " 661: '[unused656]',\n",
       " 662: '[unused657]',\n",
       " 663: '[unused658]',\n",
       " 664: '[unused659]',\n",
       " 665: '[unused660]',\n",
       " 666: '[unused661]',\n",
       " 667: '[unused662]',\n",
       " 668: '[unused663]',\n",
       " 669: '[unused664]',\n",
       " 670: '[unused665]',\n",
       " 671: '[unused666]',\n",
       " 672: '[unused667]',\n",
       " 673: '[unused668]',\n",
       " 674: '[unused669]',\n",
       " 675: '[unused670]',\n",
       " 676: '[unused671]',\n",
       " 677: '[unused672]',\n",
       " 678: '[unused673]',\n",
       " 679: '[unused674]',\n",
       " 680: '[unused675]',\n",
       " 681: '[unused676]',\n",
       " 682: '[unused677]',\n",
       " 683: '[unused678]',\n",
       " 684: '[unused679]',\n",
       " 685: '[unused680]',\n",
       " 686: '[unused681]',\n",
       " 687: '[unused682]',\n",
       " 688: '[unused683]',\n",
       " 689: '[unused684]',\n",
       " 690: '[unused685]',\n",
       " 691: '[unused686]',\n",
       " 692: '[unused687]',\n",
       " 693: '[unused688]',\n",
       " 694: '[unused689]',\n",
       " 695: '[unused690]',\n",
       " 696: '[unused691]',\n",
       " 697: '[unused692]',\n",
       " 698: '[unused693]',\n",
       " 699: '[unused694]',\n",
       " 700: '[unused695]',\n",
       " 701: '[unused696]',\n",
       " 702: '[unused697]',\n",
       " 703: '[unused698]',\n",
       " 704: '[unused699]',\n",
       " 705: '[unused700]',\n",
       " 706: '[unused701]',\n",
       " 707: '[unused702]',\n",
       " 708: '[unused703]',\n",
       " 709: '[unused704]',\n",
       " 710: '[unused705]',\n",
       " 711: '[unused706]',\n",
       " 712: '[unused707]',\n",
       " 713: '[unused708]',\n",
       " 714: '[unused709]',\n",
       " 715: '[unused710]',\n",
       " 716: '[unused711]',\n",
       " 717: '[unused712]',\n",
       " 718: '[unused713]',\n",
       " 719: '[unused714]',\n",
       " 720: '[unused715]',\n",
       " 721: '[unused716]',\n",
       " 722: '[unused717]',\n",
       " 723: '[unused718]',\n",
       " 724: '[unused719]',\n",
       " 725: '[unused720]',\n",
       " 726: '[unused721]',\n",
       " 727: '[unused722]',\n",
       " 728: '[unused723]',\n",
       " 729: '[unused724]',\n",
       " 730: '[unused725]',\n",
       " 731: '[unused726]',\n",
       " 732: '[unused727]',\n",
       " 733: '[unused728]',\n",
       " 734: '[unused729]',\n",
       " 735: '[unused730]',\n",
       " 736: '[unused731]',\n",
       " 737: '[unused732]',\n",
       " 738: '[unused733]',\n",
       " 739: '[unused734]',\n",
       " 740: '[unused735]',\n",
       " 741: '[unused736]',\n",
       " 742: '[unused737]',\n",
       " 743: '[unused738]',\n",
       " 744: '[unused739]',\n",
       " 745: '[unused740]',\n",
       " 746: '[unused741]',\n",
       " 747: '[unused742]',\n",
       " 748: '[unused743]',\n",
       " 749: '[unused744]',\n",
       " 750: '[unused745]',\n",
       " 751: '[unused746]',\n",
       " 752: '[unused747]',\n",
       " 753: '[unused748]',\n",
       " 754: '[unused749]',\n",
       " 755: '[unused750]',\n",
       " 756: '[unused751]',\n",
       " 757: '[unused752]',\n",
       " 758: '[unused753]',\n",
       " 759: '[unused754]',\n",
       " 760: '[unused755]',\n",
       " 761: '[unused756]',\n",
       " 762: '[unused757]',\n",
       " 763: '[unused758]',\n",
       " 764: '[unused759]',\n",
       " 765: '[unused760]',\n",
       " 766: '[unused761]',\n",
       " 767: '[unused762]',\n",
       " 768: '[unused763]',\n",
       " 769: '[unused764]',\n",
       " 770: '[unused765]',\n",
       " 771: '[unused766]',\n",
       " 772: '[unused767]',\n",
       " 773: '[unused768]',\n",
       " 774: '[unused769]',\n",
       " 775: '[unused770]',\n",
       " 776: '[unused771]',\n",
       " 777: '[unused772]',\n",
       " 778: '[unused773]',\n",
       " 779: '[unused774]',\n",
       " 780: '[unused775]',\n",
       " 781: '[unused776]',\n",
       " 782: '[unused777]',\n",
       " 783: '[unused778]',\n",
       " 784: '[unused779]',\n",
       " 785: '[unused780]',\n",
       " 786: '[unused781]',\n",
       " 787: '[unused782]',\n",
       " 788: '[unused783]',\n",
       " 789: '[unused784]',\n",
       " 790: '[unused785]',\n",
       " 791: '[unused786]',\n",
       " 792: '[unused787]',\n",
       " 793: '[unused788]',\n",
       " 794: '[unused789]',\n",
       " 795: '[unused790]',\n",
       " 796: '[unused791]',\n",
       " 797: '[unused792]',\n",
       " 798: '[unused793]',\n",
       " 799: '[unused794]',\n",
       " 800: '[unused795]',\n",
       " 801: '[unused796]',\n",
       " 802: '[unused797]',\n",
       " 803: '[unused798]',\n",
       " 804: '[unused799]',\n",
       " 805: '[unused800]',\n",
       " 806: '[unused801]',\n",
       " 807: '[unused802]',\n",
       " 808: '[unused803]',\n",
       " 809: '[unused804]',\n",
       " 810: '[unused805]',\n",
       " 811: '[unused806]',\n",
       " 812: '[unused807]',\n",
       " 813: '[unused808]',\n",
       " 814: '[unused809]',\n",
       " 815: '[unused810]',\n",
       " 816: '[unused811]',\n",
       " 817: '[unused812]',\n",
       " 818: '[unused813]',\n",
       " 819: '[unused814]',\n",
       " 820: '[unused815]',\n",
       " 821: '[unused816]',\n",
       " 822: '[unused817]',\n",
       " 823: '[unused818]',\n",
       " 824: '[unused819]',\n",
       " 825: '[unused820]',\n",
       " 826: '[unused821]',\n",
       " 827: '[unused822]',\n",
       " 828: '[unused823]',\n",
       " 829: '[unused824]',\n",
       " 830: '[unused825]',\n",
       " 831: '[unused826]',\n",
       " 832: '[unused827]',\n",
       " 833: '[unused828]',\n",
       " 834: '[unused829]',\n",
       " 835: '[unused830]',\n",
       " 836: '[unused831]',\n",
       " 837: '[unused832]',\n",
       " 838: '[unused833]',\n",
       " 839: '[unused834]',\n",
       " 840: '[unused835]',\n",
       " 841: '[unused836]',\n",
       " 842: '[unused837]',\n",
       " 843: '[unused838]',\n",
       " 844: '[unused839]',\n",
       " 845: '[unused840]',\n",
       " 846: '[unused841]',\n",
       " 847: '[unused842]',\n",
       " 848: '[unused843]',\n",
       " 849: '[unused844]',\n",
       " 850: '[unused845]',\n",
       " 851: '[unused846]',\n",
       " 852: '[unused847]',\n",
       " 853: '[unused848]',\n",
       " 854: '[unused849]',\n",
       " 855: '[unused850]',\n",
       " 856: '[unused851]',\n",
       " 857: '[unused852]',\n",
       " 858: '[unused853]',\n",
       " 859: '[unused854]',\n",
       " 860: '[unused855]',\n",
       " 861: '[unused856]',\n",
       " 862: '[unused857]',\n",
       " 863: '[unused858]',\n",
       " 864: '[unused859]',\n",
       " 865: '[unused860]',\n",
       " 866: '[unused861]',\n",
       " 867: '[unused862]',\n",
       " 868: '[unused863]',\n",
       " 869: '[unused864]',\n",
       " 870: '[unused865]',\n",
       " 871: '[unused866]',\n",
       " 872: '[unused867]',\n",
       " 873: '[unused868]',\n",
       " 874: '[unused869]',\n",
       " 875: '[unused870]',\n",
       " 876: '[unused871]',\n",
       " 877: '[unused872]',\n",
       " 878: '[unused873]',\n",
       " 879: '[unused874]',\n",
       " 880: '[unused875]',\n",
       " 881: '[unused876]',\n",
       " 882: '[unused877]',\n",
       " 883: '[unused878]',\n",
       " 884: '[unused879]',\n",
       " 885: '[unused880]',\n",
       " 886: '[unused881]',\n",
       " 887: '[unused882]',\n",
       " 888: '[unused883]',\n",
       " 889: '[unused884]',\n",
       " 890: '[unused885]',\n",
       " 891: '[unused886]',\n",
       " 892: '[unused887]',\n",
       " 893: '[unused888]',\n",
       " 894: '[unused889]',\n",
       " 895: '[unused890]',\n",
       " 896: '[unused891]',\n",
       " 897: '[unused892]',\n",
       " 898: '[unused893]',\n",
       " 899: '[unused894]',\n",
       " 900: '[unused895]',\n",
       " 901: '[unused896]',\n",
       " 902: '[unused897]',\n",
       " 903: '[unused898]',\n",
       " 904: '[unused899]',\n",
       " 905: '[unused900]',\n",
       " 906: '[unused901]',\n",
       " 907: '[unused902]',\n",
       " 908: '[unused903]',\n",
       " 909: '[unused904]',\n",
       " 910: '[unused905]',\n",
       " 911: '[unused906]',\n",
       " 912: '[unused907]',\n",
       " 913: '[unused908]',\n",
       " 914: '[unused909]',\n",
       " 915: '[unused910]',\n",
       " 916: '[unused911]',\n",
       " 917: '[unused912]',\n",
       " 918: '[unused913]',\n",
       " 919: '[unused914]',\n",
       " 920: '[unused915]',\n",
       " 921: '[unused916]',\n",
       " 922: '[unused917]',\n",
       " 923: '[unused918]',\n",
       " 924: '[unused919]',\n",
       " 925: '[unused920]',\n",
       " 926: '[unused921]',\n",
       " 927: '[unused922]',\n",
       " 928: '[unused923]',\n",
       " 929: '[unused924]',\n",
       " 930: '[unused925]',\n",
       " 931: '[unused926]',\n",
       " 932: '[unused927]',\n",
       " 933: '[unused928]',\n",
       " 934: '[unused929]',\n",
       " 935: '[unused930]',\n",
       " 936: '[unused931]',\n",
       " 937: '[unused932]',\n",
       " 938: '[unused933]',\n",
       " 939: '[unused934]',\n",
       " 940: '[unused935]',\n",
       " 941: '[unused936]',\n",
       " 942: '[unused937]',\n",
       " 943: '[unused938]',\n",
       " 944: '[unused939]',\n",
       " 945: '[unused940]',\n",
       " 946: '[unused941]',\n",
       " 947: '[unused942]',\n",
       " 948: '[unused943]',\n",
       " 949: '[unused944]',\n",
       " 950: '[unused945]',\n",
       " 951: '[unused946]',\n",
       " 952: '[unused947]',\n",
       " 953: '[unused948]',\n",
       " 954: '[unused949]',\n",
       " 955: '[unused950]',\n",
       " 956: '[unused951]',\n",
       " 957: '[unused952]',\n",
       " 958: '[unused953]',\n",
       " 959: '[unused954]',\n",
       " 960: '[unused955]',\n",
       " 961: '[unused956]',\n",
       " 962: '[unused957]',\n",
       " 963: '[unused958]',\n",
       " 964: '[unused959]',\n",
       " 965: '[unused960]',\n",
       " 966: '[unused961]',\n",
       " 967: '[unused962]',\n",
       " 968: '[unused963]',\n",
       " 969: '[unused964]',\n",
       " 970: '[unused965]',\n",
       " 971: '[unused966]',\n",
       " 972: '[unused967]',\n",
       " 973: '[unused968]',\n",
       " 974: '[unused969]',\n",
       " 975: '[unused970]',\n",
       " 976: '[unused971]',\n",
       " 977: '[unused972]',\n",
       " 978: '[unused973]',\n",
       " 979: '[unused974]',\n",
       " 980: '[unused975]',\n",
       " 981: '[unused976]',\n",
       " 982: '[unused977]',\n",
       " 983: '[unused978]',\n",
       " 984: '[unused979]',\n",
       " 985: '[unused980]',\n",
       " 986: '[unused981]',\n",
       " 987: '[unused982]',\n",
       " 988: '[unused983]',\n",
       " 989: '[unused984]',\n",
       " 990: '[unused985]',\n",
       " 991: '[unused986]',\n",
       " 992: '[unused987]',\n",
       " 993: '[unused988]',\n",
       " 994: '[unused989]',\n",
       " 995: '[unused990]',\n",
       " 996: '[unused991]',\n",
       " 997: '[unused992]',\n",
       " 998: '[unused993]',\n",
       " 999: '!',\n",
       " ...}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.full_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors ={\n",
    "    word: out_softmax[i, :]\n",
    "    for i, word in processor.full_vocab.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(x, y):\n",
    "    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "from heapq import heappush, heappop\n",
    "def nearest_neighbors(x, n=10):\n",
    "    if isinstance(x, str):\n",
    "        x = word_vectors[x]\n",
    "    heap = []\n",
    "    for w, v in word_vectors.items():\n",
    "        sim = cosine_similarity(x, v)\n",
    "        if len(heap) < n:\n",
    "            heappush(heap, (sim, w))\n",
    "        else:\n",
    "            if heap[0] < (sim, w):\n",
    "                heappop(heap)\n",
    "                heappush(heap, (sim, w))\n",
    "    return sorted(heap, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.99999994, 'hello'),\n",
       " (0.61714774, 'goodbye'),\n",
       " (0.59412915, 'goodnight'),\n",
       " (0.57801807, 'greeting'),\n",
       " (0.5480656, 'farewell'),\n",
       " (0.5474026, 'hey'),\n",
       " (0.5347895, 'hi'),\n",
       " (0.52100366, 'ひ'),\n",
       " (0.50768197, 'congratulations'),\n",
       " (0.50653183, '人')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_neighbors(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 'programmer'),\n",
       " (0.8326575, 'programmers'),\n",
       " (0.6889887, 'keyboardist'),\n",
       " (0.6881369, 'mathematician'),\n",
       " (0.6862794, 'mathematicians'),\n",
       " (0.68078387, '[unused8]'),\n",
       " (0.6806002, '[unused782]'),\n",
       " (0.68054837, 'র'),\n",
       " (0.6805353, '1756'),\n",
       " (0.680412, '[unused59]')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_neighbors(\"programmer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0000001, 'doctor'),\n",
       " (0.7093963, 'doctors'),\n",
       " (0.588878, 'physician'),\n",
       " (0.5472802, 'physicians'),\n",
       " (0.52723026, 'psychiatrist'),\n",
       " (0.5178553, 'dentist'),\n",
       " (0.51108694, 'surgeon'),\n",
       " (0.5001953, 'healer'),\n",
       " (0.4947291, 'medical'),\n",
       " (0.4873745, 'lawyer')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_neighbors(\"doctor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 'queen'),\n",
       " (0.64685124, 'king'),\n",
       " (0.60091084, 'queens'),\n",
       " (0.59095985, 'princess'),\n",
       " (0.5419779, 'empress'),\n",
       " (0.50120705, 'prince'),\n",
       " (0.50038207, 'duchess'),\n",
       " (0.49041915, 'countess'),\n",
       " (0.4852578, 'monarch'),\n",
       " (0.4709278, 'lady')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_neighbors(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.79005355, 'king'),\n",
       " (0.5477464, 'man'),\n",
       " (0.41445062, 'kings'),\n",
       " (0.41256273, 'prince'),\n",
       " (0.35672778, 'queen'),\n",
       " (0.3516159, '##man'),\n",
       " (0.34965998, '336'),\n",
       " (0.34959438, '670'),\n",
       " (0.34730172, '268'),\n",
       " (0.3471633, '263')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_neighbors(word_vectors[\"man\"] - word_vectors[\"woman\"] + word_vectors[\"king\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 'man'),\n",
       " (0.63370425, 'woman'),\n",
       " (0.5670583, 'men'),\n",
       " (0.53630894, '##man'),\n",
       " (0.50406426, 'boy'),\n",
       " (0.49270344, 'girl'),\n",
       " (0.48984453, 'person'),\n",
       " (0.46555227, 'guy'),\n",
       " (0.46152157, '229'),\n",
       " (0.45967817, '228')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_neighbors(word_vectors[\"man\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No [MASK] tokenn tokens [he, is, [, [UNK]] found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-6cc12d79bb10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvec1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_word_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"he is [MASK].\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"[MASK]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mvec2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_word_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"she is [MASK].\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"[MASK]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-91000d448f3a>\u001b[0m in \u001b[0;36mget_word_vector\u001b[1;34m(sentence, word, n_calc)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_word_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_calc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_calc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\programming\\python\\LING78000\\contextual_embedding_bias_measure\\lib\\bert_utils.py\u001b[0m in \u001b[0;36mget_index\u001b[1;34m(self, sentence, word, accept_wordpiece, last)\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"No {word} tokenn tokens {toks} found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_bert_model_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_sentence\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No [MASK] tokenn tokens [he, is, [, [UNK]] found"
     ]
    }
   ],
   "source": [
    "vec1 = get_word_vector(\"he is [MASK].\", \"[MASK]\")\n",
    "vec2 = get_word_vector(\"she is [MASK].\", \"[MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.4538675546646118: '##odies',\n",
       " 0.4861065149307251: 'turing',\n",
       " 0.49317944049835205: 'nipples',\n",
       " 0.5031433701515198: 'brotherhood',\n",
       " 0.516745537519455: '##duced',\n",
       " 0.5172170400619507: '##boys',\n",
       " 0.526495099067688: '##verted',\n",
       " 0.5401962995529175: '##gues',\n",
       " 0.5412156581878662: 'bikini',\n",
       " 0.5476875901222229: 'beaux'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = (-(out_softmax @ (vec1 - vec2)))\n",
    "ordering = diff.argsort()\n",
    "{\n",
    "    diff[i] + 1: processor.index_to_token(i)\n",
    "    for i in ordering[:10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9580987"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec1 = get_word_vector(\"he is [MASK].\", \"is\")\n",
    "vec2 = get_word_vector(\"she is [MASK].\", \"is\")\n",
    "np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No [MASK] tokenn tokens [he, is, [, [UNK]] found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-22ba2cf7974e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvec3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_word_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"he is [MASK].\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"[MASK]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mvec4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_word_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"she is [MASK].\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"[MASK]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvec4\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-91000d448f3a>\u001b[0m in \u001b[0;36mget_word_vector\u001b[1;34m(sentence, word, n_calc)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_word_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_calc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_calc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\programming\\python\\LING78000\\contextual_embedding_bias_measure\\lib\\bert_utils.py\u001b[0m in \u001b[0;36mget_index\u001b[1;34m(self, sentence, word, accept_wordpiece, last)\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"No {word} tokenn tokens {toks} found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_bert_model_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_sentence\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No [MASK] tokenn tokens [he, is, [, [UNK]] found"
     ]
    }
   ],
   "source": [
    "vec3 = get_word_vector(\"he is [MASK].\", \"[MASK]\")\n",
    "vec4 = get_word_vector(\"she is [MASK].\", \"[MASK]\")\n",
    "np.dot(vec3, vec4) / (np.linalg.norm(vec3) * np.linalg.norm(vec4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vec3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-87aa2c1abcb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mvec2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvec3\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mvec4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'vec3' is not defined"
     ]
    }
   ],
   "source": [
    "cosine_similarity((vec1 - vec2), (vec3 - vec4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No [MASK] tokenn tokens [she, is, [, [UNK]] found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-1e9e1525c3c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvec1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_word_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"she is [MASK].\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"[MASK]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0maaaa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout_softmax\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mvec1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mordering\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0maaaa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m {\n\u001b[0;32m      5\u001b[0m     \u001b[0maaaa\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_to_token\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-91000d448f3a>\u001b[0m in \u001b[0;36mget_word_vector\u001b[1;34m(sentence, word, n_calc)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_word_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_calc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_calc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\programming\\python\\LING78000\\contextual_embedding_bias_measure\\lib\\bert_utils.py\u001b[0m in \u001b[0;36mget_index\u001b[1;34m(self, sentence, word, accept_wordpiece, last)\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"No {word} tokenn tokens {toks} found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_bert_model_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_sentence\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No [MASK] tokenn tokens [she, is, [, [UNK]] found"
     ]
    }
   ],
   "source": [
    "vec1 = get_word_vector(\"she is [MASK].\", \"[MASK]\")\n",
    "aaaa = out_softmax @ vec1\n",
    "ordering = (-aaaa).argsort()\n",
    "{\n",
    "    aaaa[i] + 1: processor.index_to_token(i)\n",
    "    for i in ordering[:10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No [MASK] tokenn tokens [he, is, [, [UNK]] found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-76f09c580fac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvec1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_word_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"he is [MASK].\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"[MASK]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mordering\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_softmax\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mvec1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m {\n\u001b[0;32m      4\u001b[0m     \u001b[0mordering\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_to_token\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mordering\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-91000d448f3a>\u001b[0m in \u001b[0;36mget_word_vector\u001b[1;34m(sentence, word, n_calc)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_word_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_calc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_calc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\programming\\python\\LING78000\\contextual_embedding_bias_measure\\lib\\bert_utils.py\u001b[0m in \u001b[0;36mget_index\u001b[1;34m(self, sentence, word, accept_wordpiece, last)\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"No {word} tokenn tokens {toks} found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_bert_model_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_sentence\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No [MASK] tokenn tokens [he, is, [, [UNK]] found"
     ]
    }
   ],
   "source": [
    "vec1 = get_word_vector(\"he is [MASK].\", \"[MASK]\")\n",
    "ordering = (-(out_softmax @ vec1)).argsort()\n",
    "{\n",
    "    ordering[i] + 1: processor.index_to_token(i)\n",
    "    for i in ordering[:10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing words in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1 = get_word_vector(\"he is a programmer.\", \"programmer\")\n",
    "vec2 = get_word_vector(\"she is a programmer.\", \"programmer\")\n",
    "vec3 = get_word_vector(\"the programmer wrote code on the board.\", \"programmer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9778098"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70002395"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vec1, vec3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1 = get_word_vector(\"he is a nurse.\", \"nurse\")\n",
    "vec2 = get_word_vector(\"she is a nurse.\", \"nurse\")\n",
    "vec3 = get_word_vector(\"the nurse wrote code on the board.\", \"nurse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97982275"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.722002"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vec1, vec3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = []\n",
    "vecs.append(get_word_vector(\"he is a programmer.\", \"programmer\"))\n",
    "vecs.append(get_word_vector(\"he is a programmer.\", \"he\"))\n",
    "vecs.append(get_word_vector(\"she is a programmer.\", \"programmer\"))\n",
    "vecs.append(get_word_vector(\"she is a programmer.\", \"she\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sim_matrix(vecs):\n",
    "    sim_matrix = np.zeros((len(vecs), len(vecs)))\n",
    "    for i, v in enumerate(vecs):\n",
    "        for j, w in enumerate(vecs):\n",
    "            sim_matrix[i, j] = cosine_similarity(v, w)\n",
    "    return sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.26702088, 0.97780979, 0.25233993],\n",
       "       [0.26702088, 1.00000012, 0.30498534, 0.78304923],\n",
       "       [0.97780979, 0.30498534, 1.        , 0.29841116],\n",
       "       [0.25233993, 0.78304923, 0.29841116, 1.        ]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_sim_matrix(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = []\n",
    "vecs.append(get_word_vector(\"he is a programmer.\", \"he\"))\n",
    "vecs.append(get_word_vector(\"she is a programmer.\", \"she\"))\n",
    "vecs.append(get_word_vector(\"his profession is a programmer.\", \"his\"))\n",
    "vecs.append(get_word_vector(\"her profession is a programmer.\", \"her\"))\n",
    "vecs.append(get_word_vector(\"please talk to him.\", \"him\"))\n",
    "vecs.append(get_word_vector(\"please talk to her.\", \"her\"))\n",
    "vecs.append(get_word_vector(\"I work as a programmer.\", \"programmer\"))\n",
    "vecs.append(get_word_vector(\"I work as a nurse.\", \"nurse\"))\n",
    "vecs.append(get_word_vector(\"I work as a doctor.\", \"doctor\"))\n",
    "vecs.append(get_word_vector(\"I work as a nurse.\", \"nurse\"))\n",
    "vecs.append(get_word_vector(\"I am your father.\", \"father\"))\n",
    "vecs.append(get_word_vector(\"I am your mother.\", \"mother\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8649077"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vecs[1]- vecs[0], vecs[3] - vecs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.705075"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vecs[1]- vecs[0], vecs[5] - vecs[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6863628"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vecs[3]- vecs[2], vecs[5] - vecs[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15071443"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vecs[3]- vecs[2], vecs[7] - vecs[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22179426"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vecs[3]- vecs[2], vecs[9] - vecs[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there does seem to be a gender subspace...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5433057"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vecs[3]- vecs[2], vecs[11] - vecs[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't find much of a difference..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000012, 0.2496486 , 0.24885978],\n",
       "       [0.2496486 , 1.        , 0.78304923],\n",
       "       [0.24885978, 0.78304923, 1.00000012]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog_vec = get_word_vector(\"[MASK] is a programmer.\", \"programmer\")\n",
    "she_vec = get_word_vector(\"she is a programmer.\", \"she\")\n",
    "he_vec = get_word_vector(\"he is a programmer.\", \"he\")\n",
    "construct_sim_matrix([prog_vec, she_vec, he_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000012, 0.88541687, 0.90504026],\n",
       "       [0.88541687, 1.        , 0.97780979],\n",
       "       [0.90504026, 0.97780979, 1.        ]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog_vec = get_word_vector(\"[MASK] is a programmer.\", \"programmer\")\n",
    "she_vec = get_word_vector(\"she is a programmer.\", \"programmer\")\n",
    "he_vec = get_word_vector(\"he is a programmer.\", \"programmer\")\n",
    "construct_sim_matrix([prog_vec, she_vec, he_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the distance between words in neutral contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programmer is slightly more similar to father than to mother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sim_matrix_df(sentences: List[str],\n",
    "                           words: List[str]):\n",
    "    sim = construct_sim_matrix([get_word_vector(sent, word) for sent, word in zip(sentences, words)])\n",
    "    return pd.DataFrame(data=sim, index=words, columns=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>programmer</th>\n",
       "      <th>mother</th>\n",
       "      <th>father</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>programmer</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420880</td>\n",
       "      <td>0.444445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mother</th>\n",
       "      <td>0.420880</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.733642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>father</th>\n",
       "      <td>0.444445</td>\n",
       "      <td>0.733642</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            programmer    mother    father\n",
       "programmer    1.000000  0.420880  0.444445\n",
       "mother        0.420880  1.000000  0.733642\n",
       "father        0.444445  0.733642  1.000000"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_sim_matrix_df([\"That person is a programmer.\", \"That person is my mother.\", \n",
    "                         \"That person is my father.\"],\n",
    "                       [\"programmer\", \"mother\", \"father\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nurse is closer to mother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nurse</th>\n",
       "      <th>mother</th>\n",
       "      <th>father</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nurse</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.484853</td>\n",
       "      <td>0.392041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mother</th>\n",
       "      <td>0.484853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.733642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>father</th>\n",
       "      <td>0.392041</td>\n",
       "      <td>0.733642</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           nurse    mother    father\n",
       "nurse   1.000000  0.484853  0.392041\n",
       "mother  0.484853  1.000000  0.733642\n",
       "father  0.392041  0.733642  1.000000"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_sim_matrix_df([\"That person is a nurse.\", \"That person is my mother.\", \n",
    "                         \"That person is my father.\"],\n",
    "                       [\"nurse\", \"mother\", \"father\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nurse</th>\n",
       "      <th>mother</th>\n",
       "      <th>father</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nurse</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.488195</td>\n",
       "      <td>0.418121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mother</th>\n",
       "      <td>0.488195</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.733642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>father</th>\n",
       "      <td>0.418121</td>\n",
       "      <td>0.733642</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           nurse    mother    father\n",
       "nurse   1.000000  0.488195  0.418121\n",
       "mother  0.488195  1.000000  0.733642\n",
       "father  0.418121  0.733642  1.000000"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_sim_matrix_df([\"My nurse will not allow that.\", \"That person is my mother.\", \n",
    "                         \"That person is my father.\"],\n",
    "                       [\"nurse\", \"mother\", \"father\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even the same word can have pretty different embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mother</th>\n",
       "      <th>mother</th>\n",
       "      <th>father</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mother</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.765978</td>\n",
       "      <td>0.559372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mother</th>\n",
       "      <td>0.765978</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.733642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>father</th>\n",
       "      <td>0.559372</td>\n",
       "      <td>0.733642</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          mother    mother    father\n",
       "mother  1.000000  0.765978  0.559372\n",
       "mother  0.765978  1.000000  0.733642\n",
       "father  0.559372  0.733642  1.000000"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_sim_matrix_df([\"Please don't let your mother eat that cookie.\", \"That person is my mother.\", \n",
    "                         \"That person is my father.\"],\n",
    "                       [\"mother\", \"mother\", \"father\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different parts of speech lead to vastly different embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mother</th>\n",
       "      <th>mother</th>\n",
       "      <th>father</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mother</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.582067</td>\n",
       "      <td>0.446689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mother</th>\n",
       "      <td>0.582067</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.733642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>father</th>\n",
       "      <td>0.446689</td>\n",
       "      <td>0.733642</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          mother    mother    father\n",
       "mother  1.000000  0.582067  0.446689\n",
       "mother  0.582067  1.000000  0.733642\n",
       "father  0.446689  0.733642  1.000000"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_sim_matrix_df([\"The cat could mother that dog.\", \"That person is my mother.\", \n",
    "                         \"That person is my father.\"],\n",
    "                       [\"mother\", \"mother\", \"father\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparsions between different parts of speech: Still roughly the same pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nurse</th>\n",
       "      <th>she</th>\n",
       "      <th>he</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nurse</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.450963</td>\n",
       "      <td>0.338125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>she</th>\n",
       "      <td>0.450963</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.726877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>0.338125</td>\n",
       "      <td>0.726877</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          nurse       she        he\n",
       "nurse  1.000000  0.450963  0.338125\n",
       "she    0.450963  1.000000  0.726877\n",
       "he     0.338125  0.726877  1.000000"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_sim_matrix_df([\"That person is a nurse.\", \"What is she doing?\", \n",
    "                         \"What is he doing?\"],\n",
    "                       [\"nurse\", \"she\", \"he\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gendered Subspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct gender subspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple test (TODO: Automate construction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_vecs, female_vecs = [], []\n",
    "def add_word_vecs(s: str, male_w: str, female_w: str):\n",
    "    male_vecs.append(get_word_vector(s.replace(\"XXX\", male_w), male_w))\n",
    "    female_vecs.append(get_word_vector(s.replace(\"XXX\", female_w), female_w))\n",
    "\n",
    "for prof in [\"musician\", \"magician\", \"nurse\", \"doctor\", \"teacher\"]:\n",
    "    add_word_vecs(\"XXX is a YYY\".replace(\"YYY\", prof), \"he\", \"she\")\n",
    "    add_word_vecs(\"XXX works as a YYY\".replace(\"YYY\", prof), \"he\", \"she\")\n",
    "\n",
    "for action in [\"talk to\", \"hit\", \"ignore\", \"please\", \"remove\"]:\n",
    "    add_word_vecs(\"please YYY XXX\".replace(\"YYY\", action), \"him\", \"her\")\n",
    "    add_word_vecs(\"don't YYY XXX\".replace(\"YYY\", action), \"him\", \"her\")\n",
    "\n",
    "for thing in [\"food\", \"music\", \"work\", \"running\", \"cooking\"]:\n",
    "    add_word_vecs(\"XXX likes YYY\".replace(\"YYY\", thing), \"he\", \"she\")\n",
    "    add_word_vecs(\"XXX enjoys YYY\".replace(\"YYY\", thing), \"he\", \"she\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_vecs = np.r_[male_vecs]\n",
    "female_vecs = np.r_[female_vecs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 768)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(male_vecs - female_vecs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_vecs = male_vecs - female_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = svd.fit_transform(diff_vecs / (diff_vecs ** 2).sum(1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02218353, 0.33863246, 0.15979412, 0.07528188, 0.06134707],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.657239"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 768)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try eliminating this subspace and checking outputs softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00213644,  0.06697969,  0.01185373, ...,  0.01891251,\n",
       "         0.05929657, -0.05238242],\n",
       "       [-0.03966811,  0.00047857, -0.00887874, ..., -0.01447054,\n",
       "        -0.01458254, -0.0467876 ],\n",
       "       [ 0.0187734 ,  0.02606242, -0.05707034, ..., -0.02682133,\n",
       "         0.00981187,  0.00205974],\n",
       "       [ 0.04818484,  0.01296756, -0.02450339, ..., -0.04148804,\n",
       "         0.01490192,  0.03740549],\n",
       "       [ 0.03615659,  0.04309639,  0.00483598, ...,  0.0155314 ,\n",
       "        -0.04863234, -0.0272552 ]], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = get_word_vector(\"[MASK] is a nurse.\", \"[MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_before = (out_softmax @ vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6940118"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_before[processor.token_to_index(\"she\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2605883"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_before[processor.token_to_index(\"he\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_subspace(v, subspace):\n",
    "    # TODO: Is there a better way?\n",
    "    V = subspace\n",
    "    beta = (np.linalg.inv(V @ V.T) @ V) @ v\n",
    "    res = (v - (V.T @ beta))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_after = eliminate_subspace(vec, svd.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_after = (out_softmax @ vec_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is indeed reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.35022"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_after[processor.token_to_index(\"she\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2542218"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_after[processor.token_to_index(\"he\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not quite working here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit diff before: 0.08666396141052246\n",
      "Logit diff after: 0.17084109783172607\n"
     ]
    }
   ],
   "source": [
    "sentence = \"[MASK] is a programmer.\"\n",
    "vec = get_word_vector(sentence, \"[MASK]\")\n",
    "logits_before = (out_softmax @ vec)\n",
    "vec_after = eliminate_subspace(vec, svd.components_)\n",
    "logits_after = (out_softmax @ vec_after)\n",
    "print(f\"Logit diff before: {logits_before[processor.token_to_index('she')] - logits_before[processor.token_to_index('he')]}\")\n",
    "print(f\"Logit diff after: {logits_after[processor.token_to_index('she')] - logits_after[processor.token_to_index('he')]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit diff before: 0.21036386489868164\n",
      "Logit diff after: -0.13858234882354736\n"
     ]
    }
   ],
   "source": [
    "sentence = \"[MASK] is a housewife.\"\n",
    "vec = get_word_vector(sentence, \"[MASK]\")\n",
    "logits_before = (out_softmax @ vec)\n",
    "vec_after = eliminate_subspace(vec, svd.components_)\n",
    "logits_after = (out_softmax @ vec_after)\n",
    "print(f\"Logit diff before: {logits_before[processor.token_to_index('she')] - logits_before[processor.token_to_index('he')]}\")\n",
    "print(f\"Logit diff after: {logits_after[processor.token_to_index('she')] - logits_after[processor.token_to_index('he')]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit diff before: 0.36542224884033203\n",
      "Logit diff after: 0.06998968124389648\n"
     ]
    }
   ],
   "source": [
    "sentence = \"[MASK] is my mother.\"\n",
    "vec = get_word_vector(sentence, \"[MASK]\")\n",
    "logits_before = (out_softmax @ vec)\n",
    "vec_after = eliminate_subspace(vec, svd.components_)\n",
    "logits_after = (out_softmax @ vec_after)\n",
    "print(f\"Logit diff before: {logits_before[processor.token_to_index('she')] - logits_before[processor.token_to_index('he')]}\")\n",
    "print(f\"Logit diff after: {logits_after[processor.token_to_index('she')] - logits_after[processor.token_to_index('he')]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
