{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_utils import Config, BertPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    model_type=\"bert-base-uncased\",\n",
    "    max_seq_len=128,\n",
    "    subspace_size=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BertPreprocessor(config.model_type, config.max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import BertConfig, BertForMaskedLM\n",
    "model = BertForMaskedLM.from_pretrained(config.model_type)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertForMaskedLM' object has no attribute 'get_output_embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-e423837c4b47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_output_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\biases_proj\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    592\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m--> 594\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m    595\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BertForMaskedLM' object has no attribute 'get_output_embeddings'"
     ]
    }
   ],
   "source": [
    "model.get_output_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ContextWord:\n",
    "    sent: str\n",
    "    word: str\n",
    "    def __post_init__(self):\n",
    "        assert self.word in self.sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(x, y):\n",
    "    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(cword: ContextWord, use_last_mask=False):\n",
    "    sentence, word = cword.sent, cword.word\n",
    "    idx = processor.get_index(sentence, word, last=use_last_mask)\n",
    "    outputs = None\n",
    "    with torch.no_grad():\n",
    "        sequence_output, _ = model.bert(processor.to_bert_model_input(sentence),\n",
    "                                        output_all_encoded_layers=False)\n",
    "        sequence_output.squeeze_(0)\n",
    "        if outputs is None: outputs = torch.zeros_like(sequence_output)\n",
    "        outputs = sequence_output + outputs\n",
    "    return outputs.detach().cpu().numpy()[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sim_matrix(vecs):\n",
    "    sim_matrix = np.zeros((len(vecs), len(vecs)))\n",
    "    for i, v in enumerate(vecs):\n",
    "        for j, w in enumerate(vecs):\n",
    "            sim_matrix[i, j] = cosine_similarity(v, w)\n",
    "    return sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sim_matrix_df(sentences: List[str],\n",
    "                           words: List[str]):\n",
    "    sim = construct_sim_matrix([get_word_vector(ContextWord(sent, word)) for sent, word in zip(sentences, words)])\n",
    "    return pd.DataFrame(data=sim, index=words, columns=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_diff_similarity(cwords1, cwords2):\n",
    "    cword11, cword12 = cwords1\n",
    "    cword21, cword22 = cwords2\n",
    "    return cosine_similarity(get_word_vector(cword11) - get_word_vector(cword12),\n",
    "                             get_word_vector(cword21) - get_word_vector(cword22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_softmax = model.cls.predictions.decoder.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_bias = model.cls.predictions.bias.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_logits(wv: np.ndarray) -> np.ndarray:\n",
    "    return model.cls(torch.FloatTensor(wv).unsqueeze(0)).detach().cpu().numpy()[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>programmer</th>\n",
       "      <th>man</th>\n",
       "      <th>woman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>programmer</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.390309</td>\n",
       "      <td>0.413819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>man</th>\n",
       "      <td>0.390309</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.824558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woman</th>\n",
       "      <td>0.413819</td>\n",
       "      <td>0.824558</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            programmer       man     woman\n",
       "programmer    1.000000  0.390309  0.413819\n",
       "man           0.390309  1.000000  0.824558\n",
       "woman         0.413819  0.824558  1.000000"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_sim_matrix_df([\"That person is a programmer.\", \n",
    "                         \"I am a man.\", \n",
    "                         \"I am a woman.\"],\n",
    "                       [\"programmer\", \"man\", \"woman\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.150812"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"The programmer went to the office.\", \"programmer\"),\n",
    "     ContextWord(\"The nurse went to the office.\", \"nurse\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContextWord(sent='I am a man.', word='man')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ContextWord(\"I am a man.\", \"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23052038"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"The doctor went to the office.\", \"doctor\"),\n",
    "     ContextWord(\"The nurse went to the office.\", \"nurse\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31024477"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff_similarity(\n",
    "    (ContextWord(\"he likes sports.\", \"he\"), ContextWord(\"she likes sports.\", \"she\")),\n",
    "    (ContextWord(\"The doctor went to the office.\", \"doctor\"),\n",
    "     ContextWord(\"The nurse went to the office.\", \"nurse\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find gendered direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original gender terms:\n",
    "- she - he \n",
    "- her - his \n",
    "- woman - man\n",
    "- Mary - John\n",
    "- herself - himself\n",
    "- daughter - son\n",
    "- mother - father\n",
    "- gal - guy\n",
    "- girl - boy\n",
    "- female - male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_vecs, female_vecs = [], []\n",
    "def add_word_vecs(s: str, male_w: str, female_w: str):\n",
    "    male_vecs.append(get_word_vector(ContextWord(s.replace(\"XXX\", male_w), male_w)))\n",
    "    female_vecs.append(get_word_vector(ContextWord(s.replace(\"XXX\", female_w), female_w)))\n",
    "\n",
    "for prof in [\"musician\", \"magician\", \"nurse\", \"doctor\", \"teacher\"]:\n",
    "    add_word_vecs(\"XXX is a YYY\".replace(\"YYY\", prof), \"he\", \"she\")\n",
    "    add_word_vecs(\"XXX works as a YYY\".replace(\"YYY\", prof), \"he\", \"she\")\n",
    "\n",
    "for action in [\"talk to\", \"hit\", \"ignore\", \"please\", \"remove\"]:\n",
    "    add_word_vecs(\"please YYY XXX\".replace(\"YYY\", action), \"him\", \"her\")\n",
    "    add_word_vecs(\"don't YYY XXX\".replace(\"YYY\", action), \"him\", \"her\")\n",
    "\n",
    "for thing in [\"food\", \"music\", \"work\", \"running\", \"cooking\"]:\n",
    "    add_word_vecs(\"XXX dislikes YYY\".replace(\"YYY\", thing), \"man\", \"woman\")\n",
    "    add_word_vecs(\"XXX is thinking about YYY\".replace(\"YYY\", thing), \"man\", \"woman\")\n",
    "    \n",
    "for action in [\"running\", \"thinking\", \"working\", \"watching\", \"reading\"]:\n",
    "    add_word_vecs(\"The XXX is YYY\".replace(\"YYY\", action), \"boy\", \"girl\")\n",
    "    add_word_vecs(\"That XXX likes YYY\".replace(\"YYY\", action), \"boy\", \"girl\")\n",
    "    \n",
    "for adj in [\"fat\", \"cute\", \"attractive\", \"smart\", \"strong\"]:\n",
    "    add_word_vecs(\"My XXX is YYY\".replace(\"YYY\", adj), \"boy\", \"girl\")\n",
    "    add_word_vecs(\"Her XXX is not YYY\".replace(\"YYY\", adj), \"boy\", \"girl\")\n",
    "    \n",
    "for thing in [\"cat\", \"dog\", \"person\", \"word\", \"action\"]:\n",
    "    add_word_vecs(\"XXX is YYY\".replace(\"YYY\", adj), \"male\", \"female\")\n",
    "    add_word_vecs(\"XXX is clearly YYY\".replace(\"YYY\", adj), \"male\", \"female\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_vecs = np.r_[male_vecs]\n",
    "female_vecs = np.r_[female_vecs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def find_subspace(D: np.ndarray) -> PCA:\n",
    "    assert len(D.shape) == 2\n",
    "    pca = PCA(n_components=config.subspace_size)\n",
    "    return pca.fit(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = find_subspace(male_vecs - female_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1e70a273780>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=np.arange(pca.n_components), y=pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what it says in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We denote the projection of a vector $ v $ onto $ B $ by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ v_B = \\sum_{j=1}^{k} (v \\cdot b_j) b_j $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word $ w \\in N $, let $ \\vec{w} $ be re-embedded to\n",
    "$$ \\vec{w} := \\vec{w} - \\vec{w_{B}} / || \\vec{w} - \\vec{w_{B}} || $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mu := \\sum_{w \\in E}w / |E| $$\n",
    "$$ \\nu := \\mu - \\mu_B $$\n",
    "For each $ w \\in E $, \n",
    "$$ \\vec{w} := \\nu + \\sqrt{1 - ||\\nu||^2}\\frac{\\vec{w_B} - \\mu_B}{||\\vec{w_B} - \\mu_B||} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_subspace(X: np.ndarray, subspace: np.ndarray, norm=True) -> np.ndarray:\n",
    "    Xb = ((X @ subspace.T) @ subspace) # projection onto biased subspace\n",
    "    X = (X - Xb) / (np.linalg.norm(X - Xb))\n",
    "    if norm:\n",
    "        mu = X.mean(0)\n",
    "        mub = Xb.mean(0)\n",
    "        nu = mu - mub\n",
    "        return nu + np.sqrt(1 - nu**2) * (Xb - mub) / np.linalg.norm(Xb - mub)\n",
    "    else:\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06290076, -0.09486497, -0.02813753, ..., -0.04342418,\n",
       "        -0.101857  ,  0.06568638],\n",
       "       [-0.06342975, -0.09343772, -0.03166628, ..., -0.04076808,\n",
       "        -0.1011392 ,  0.06436385],\n",
       "       [-0.06442945, -0.09383103, -0.03182642, ..., -0.04132335,\n",
       "        -0.10287205,  0.06333566],\n",
       "       ...,\n",
       "       [-0.06907058, -0.103051  , -0.02295195, ..., -0.05188017,\n",
       "        -0.11271334,  0.0673344 ],\n",
       "       [-0.06983954, -0.10438704, -0.02219155, ..., -0.05312184,\n",
       "        -0.11442162,  0.06829503],\n",
       "       [-0.06907058, -0.103051  , -0.02295195, ..., -0.05188017,\n",
       "        -0.11271334,  0.0673344 ]], dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_subspace(male_vecs, pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newly checking for differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Postprocess\"\"\"\n",
    "    return remove_subspace(np.expand_dims(X, 0), pca.components_, norm=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_new_diff_similarity(cwords1, cwords2):\n",
    "    cword11, cword12 = cwords1\n",
    "    cword21, cword22 = cwords2\n",
    "    return cosine_similarity(pp(get_word_vector(cword11)) - pp(get_word_vector(cword12)),\n",
    "                             pp(get_word_vector(cword21)) - pp(get_word_vector(cword22)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarities are being reduced, so there is a shared gender subspace to a certain extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.150812, 0.12542944)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(compute_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"The programmer went to the office.\", \"programmer\"),\n",
    "     ContextWord(\"The nurse went to the office.\", \"nurse\"))\n",
    "),\n",
    "compute_new_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"The programmer went to the office.\", \"programmer\"),\n",
    "     ContextWord(\"The nurse went to the office.\", \"nurse\"))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.23052038, 0.14701234)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(compute_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"The doctor went to the office.\", \"doctor\"),\n",
    "     ContextWord(\"The nurse went to the office.\", \"nurse\"))\n",
    "),\n",
    "compute_new_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"The doctor went to the office.\", \"doctor\"),\n",
    "     ContextWord(\"The nurse went to the office.\", \"nurse\"))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.31024477, 0.23461922)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(compute_diff_similarity(\n",
    "    (ContextWord(\"he likes sports.\", \"he\"), ContextWord(\"she likes sports.\", \"she\")),\n",
    "    (ContextWord(\"The doctor went to the office.\", \"doctor\"),\n",
    "     ContextWord(\"The nurse went to the office.\", \"nurse\"))\n",
    "),\n",
    "compute_new_diff_similarity(\n",
    "    (ContextWord(\"he likes sports.\", \"he\"), ContextWord(\"she likes sports.\", \"she\")),\n",
    "    (ContextWord(\"The doctor went to the office.\", \"doctor\"),\n",
    "     ContextWord(\"The nurse went to the office.\", \"nurse\"))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for change in bias score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the bias score decreases with this transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_score(sentence: str, gender_words: Iterable[str], \n",
    "               word: str, gender_comes_first=True, \n",
    "               correct_bias=True,\n",
    "               postprocess=False) -> float:    \n",
    "    mw, fw = gender_words\n",
    "    mwi, fwi = processor.token_to_index(mw), processor.token_to_index(fw)\n",
    "    wv = get_word_vector(\n",
    "        ContextWord(sentence.replace(\"XXX\", word).replace(\"GGG\", \"[MASK]\"), \"[MASK]\"),\n",
    "        use_last_mask=not gender_comes_first,        \n",
    "    )\n",
    "    if postprocess: wv = pp(wv)\n",
    "    logits = to_logits(wv)\n",
    "    subject_fill_bias = logits[mwi] - logits[fwi]\n",
    "    if correct_bias:\n",
    "        wv = get_word_vector(\n",
    "            ContextWord(sentence.replace(\"XXX\", \"[MASK]\").replace(\"GGG\", \"[MASK]\"), \"[MASK]\"),\n",
    "            use_last_mask=gender_comes_first,\n",
    "        )\n",
    "        if postprocess: wv = pp(wv)\n",
    "        prior_logits = to_logits(wv)\n",
    "        prior_bias = prior_logits[mwi] - prior_logits[fwi]\n",
    "        subject_fill_bias = subject_fill_bias - prior_bias\n",
    "    return subject_fill_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias is reduced here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6938139"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_score(\"GGG is a XXX.\", [\"he\", \"she\"], \"doctor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.13400638"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_score(\"GGG is a XXX.\", [\"he\", \"she\"], \"doctor\", postprocess=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias is neutralized here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.4922547"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_score(\"GGG is a XXX.\", [\"he\", \"she\"], \"nurse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.85032165"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_score(\"GGG is a XXX.\", [\"he\", \"she\"], \"nurse\", postprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing for adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.395012"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_score(\"GGG is very XXX.\", [\"he\", \"she\"], \"beautiful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.94363105"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_score(\"GGG is very XXX.\", [\"he\", \"she\"], \"beautiful\", postprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60480523"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_score(\"GGG likes XXX.\", [\"he\", \"she\"], \"ESPN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5123712"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_score(\"GGG is very XXX.\", [\"he\", \"she\"], \"dangerous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2682296"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_score(\"GGG is very XXX.\", [\"he\", \"she\"], \"dangerous\", postprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2886461"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_score(\"GGG is very XXX.\", [\"he\", \"she\"], \"cute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.42735302"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_score(\"GGG is very XXX.\", [\"he\", \"she\"], \"cute\", postprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unintended Side Effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any unintended side effects of this transformation? Let's test and see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.009464249"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"The programmer went to the office.\", \"programmer\"),\n",
    "     ContextWord(\"The doctor went to the office.\", \"doctor\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    " def construct_sim_matrix_df(cws: List[ContextWord]):\n",
    "    return pd.DataFrame(data=sim, index=words, columns=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>programmer</th>\n",
       "      <th>doctor</th>\n",
       "      <th>nurse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>programmer</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.683078</td>\n",
       "      <td>0.634901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doctor</th>\n",
       "      <td>0.683078</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.835144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nurse</th>\n",
       "      <td>0.634901</td>\n",
       "      <td>0.835144</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            programmer    doctor     nurse\n",
       "programmer    1.000000  0.683078  0.634901\n",
       "doctor        0.683078  1.000000  0.835144\n",
       "nurse         0.634901  0.835144  1.000000"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cws = [\n",
    "    ContextWord(\"The programmer went to the office.\", \"programmer\"),\n",
    "    ContextWord(\"The doctor went to the office.\", \"doctor\"),\n",
    "    ContextWord(\"The nurse went to the office.\", \"nurse\"),\n",
    "]\n",
    "sim = construct_sim_matrix([get_word_vector(cw) for cw in cws])\n",
    "pd.DataFrame(data=sim, index=[cw.word for cw in cws], columns=[cw.word for cw in cws])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the similarities here seem to be roughly preserved; perhaps because we are neutralizing w.r.t to the gender dimension in the subject space, but not the object space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>programmer</th>\n",
       "      <th>doctor</th>\n",
       "      <th>nurse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>programmer</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.678573</td>\n",
       "      <td>0.630049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doctor</th>\n",
       "      <td>0.678573</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.832840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nurse</th>\n",
       "      <td>0.630049</td>\n",
       "      <td>0.832840</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            programmer    doctor     nurse\n",
       "programmer    1.000000  0.678573  0.630049\n",
       "doctor        0.678573  1.000000  0.832840\n",
       "nurse         0.630049  0.832840  1.000000"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = construct_sim_matrix([pp(get_word_vector(cw)) for cw in cws])\n",
    "pd.DataFrame(data=sim, index=[cw.word for cw in cws], columns=[cw.word for cw in cws])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beautiful</th>\n",
       "      <th>dangerous</th>\n",
       "      <th>normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>beautiful</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600859</td>\n",
       "      <td>0.583575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dangerous</th>\n",
       "      <td>0.600859</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.536445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>normal</th>\n",
       "      <td>0.583575</td>\n",
       "      <td>0.536445</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           beautiful  dangerous    normal\n",
       "beautiful   1.000000   0.600859  0.583575\n",
       "dangerous   0.600859   1.000000  0.536445\n",
       "normal      0.583575   0.536445  1.000000"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cws = [\n",
    "    ContextWord(\"Your colleague is very beautiful.\", \"beautiful\"),\n",
    "    ContextWord(\"Your colleague is very dangerous.\", \"dangerous\"),\n",
    "    ContextWord(\"Your colleague is very normal.\", \"normal\"),\n",
    "]\n",
    "sim = construct_sim_matrix([get_word_vector(cw) for cw in cws])\n",
    "pd.DataFrame(data=sim, index=[cw.word for cw in cws], columns=[cw.word for cw in cws])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, not much reduction in similarities here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beautiful</th>\n",
       "      <th>dangerous</th>\n",
       "      <th>normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>beautiful</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.601322</td>\n",
       "      <td>0.581689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dangerous</th>\n",
       "      <td>0.601322</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.525321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>normal</th>\n",
       "      <td>0.581689</td>\n",
       "      <td>0.525321</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           beautiful  dangerous    normal\n",
       "beautiful   1.000000   0.601322  0.581689\n",
       "dangerous   0.601322   1.000000  0.525321\n",
       "normal      0.581689   0.525321  1.000000"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = construct_sim_matrix([pp(get_word_vector(cw)) for cw in cws])\n",
    "pd.DataFrame(data=sim, index=[cw.word for cw in cws], columns=[cw.word for cw in cws])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linearity of BERT embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT embeddings no longer express the same linear semantics as word2vec/GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714916"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"I am a king.\", \"king\"),\n",
    "     ContextWord(\"I am a queen.\", \"queen\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45550752"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff_similarity(\n",
    "    (ContextWord(\"he is my friend.\", \"he\"), ContextWord(\"she is my friend.\", \"she\")),\n",
    "    (ContextWord(\"they are the king.\", \"king\"),\n",
    "     ContextWord(\"they are the queen.\", \"queen\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in direction does not stay constant as the subject/object status of the word changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35181317"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"The king walked across the road.\", \"king\"),\n",
    "     ContextWord(\"The queen walked across the road.\", \"queen\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24944244"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"king does not do such things.\", \"king\"),\n",
    "     ContextWord(\"queen does not do such things\", \"queen\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40263593"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"I captured the opponent's king.\", \"king\"),\n",
    "     ContextWord(\"I captured the opponent's queen.\", \"queen\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21260609"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"king, please forgive me!\", \"king\"),\n",
    "     ContextWord(\"queen, please forgive me!\", \"queen\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44601053"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff_similarity(\n",
    "    (ContextWord(\"his attitude is irritating.\", \"his\"), \n",
    "     ContextWord(\"her attitude is irritating.\", \"her\")),\n",
    "    (ContextWord(\"they are the king.\", \"king\"),\n",
    "     ContextWord(\"they are the queen.\", \"queen\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-141-31c2f94490ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBertForMaskedLM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(input_ids, masked_lm_labels=input_ids)\n",
    "\n",
    "loss, prediction_scores = outputs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
