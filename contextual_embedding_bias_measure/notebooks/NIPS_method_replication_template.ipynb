{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the constrcuted dataset to test out the NIPS debiasing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../lib\")\n",
    "\n",
    "DATA_ROOT = Path(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from bert_utils import Config, BertPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "train_file = \"gender_occ_pos_w_probs_train.txt\"\n",
    "val_file = \"gender_occ_pos_w_probs_val.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    model_type=\"bert-base-uncased\",\n",
    "    max_seq_len=24,\n",
    "    subspace_size=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BertPreprocessor(config.model_type, config.max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import BertConfig, BertForMaskedLM\n",
    "model = BertForMaskedLM.from_pretrained(config.model_type)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ContextWord:\n",
    "    sent: str\n",
    "    word: str\n",
    "    def __post_init__(self):\n",
    "        assert self.word in self.sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(x, y):\n",
    "    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(cword: ContextWord, use_last_mask=False):\n",
    "    sentence, word = cword.sent, cword.word\n",
    "    idx = processor.get_index(sentence, word, last=use_last_mask)\n",
    "    outputs = None\n",
    "    with torch.no_grad():\n",
    "        # TODO: Move to proper library function\n",
    "        token_ids = processor.to_bert_model_input(sentence) \n",
    "        # ensure padding is consistent\n",
    "        bert_input = torch.zeros(1, config.max_seq_len, dtype=torch.long)\n",
    "        bert_input[0, :token_ids.size(1)] = token_ids\n",
    "        sequence_output, _ = model.bert(bert_input,\n",
    "                                        output_all_encoded_layers=False)\n",
    "        sequence_output.squeeze_(0)\n",
    "        if outputs is None: outputs = torch.zeros_like(sequence_output)\n",
    "        outputs = sequence_output + outputs\n",
    "    return outputs.detach().cpu().numpy()[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sim_matrix(vecs):\n",
    "    sim_matrix = np.zeros((len(vecs), len(vecs)))\n",
    "    for i, v in enumerate(vecs):\n",
    "        for j, w in enumerate(vecs):\n",
    "            sim_matrix[i, j] = cosine_similarity(v, w)\n",
    "    return sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sim_matrix_df(sentences: List[str],\n",
    "                           words: List[str]):\n",
    "    sim = construct_sim_matrix([get_word_vector(ContextWord(sent, word)) for sent, word in zip(sentences, words)])\n",
    "    return pd.DataFrame(data=sim, index=words, columns=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_diff_similarity(cwords1, cwords2):\n",
    "    cword11, cword12 = cwords1\n",
    "    cword21, cword22 = cwords2\n",
    "    return cosine_similarity(get_word_vector(cword11) - get_word_vector(cword12),\n",
    "                             get_word_vector(cword21) - get_word_vector(cword22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_softmax = model.cls.predictions.decoder.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_bias = model.cls.predictions.bias.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_logits(wv: np.ndarray) -> np.ndarray:\n",
    "    return model.cls(torch.FloatTensor(wv).unsqueeze(0)).detach().cpu().numpy()[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>programmer</th>\n",
       "      <th>man</th>\n",
       "      <th>woman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>programmer</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.453046</td>\n",
       "      <td>0.488973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>man</th>\n",
       "      <td>0.453046</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.781821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woman</th>\n",
       "      <td>0.488973</td>\n",
       "      <td>0.781821</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            programmer       man     woman\n",
       "programmer    1.000000  0.453046  0.488973\n",
       "man           0.453046  1.000000  0.781821\n",
       "woman         0.488973  0.781821  1.000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_sim_matrix_df([\"That person is a programmer.\", \n",
    "                         \"I am a man.\", \n",
    "                         \"I am a woman.\"],\n",
    "                       [\"programmer\", \"man\", \"woman\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13653895"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"The programmer went to the office.\", \"programmer\"),\n",
    "     ContextWord(\"The nurse went to the office.\", \"nurse\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20923696"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"The doctor went to the office.\", \"doctor\"),\n",
    "     ContextWord(\"The nurse went to the office.\", \"nurse\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21795174"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff_similarity(\n",
    "    (ContextWord(\"he likes sports.\", \"he\"), ContextWord(\"she likes sports.\", \"she\")),\n",
    "    (ContextWord(\"The doctor went to the office.\", \"doctor\"),\n",
    "     ContextWord(\"The nurse went to the office.\", \"nurse\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find gendered direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(DATA_ROOT / train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.read_csv(DATA_ROOT / val_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2484/2484 [08:23<00:00,  4.77it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "male_vecs, female_vecs = [], []\n",
    "def add_word_vecs(s: str, male_w: str, female_w: str):\n",
    "    male_vecs.append(get_word_vector(ContextWord(s.replace(\"[MASK]\", male_w), male_w)))\n",
    "    female_vecs.append(get_word_vector(ContextWord(s.replace(\"[MASK]\", female_w), female_w)))\n",
    "\n",
    "for i, row in tqdm(list(df_train.iterrows())):\n",
    "    sentence = row[\"sentence\"]\n",
    "    add_word_vecs(sentence, row[\"mword\"], row[\"fword\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_vecs = np.r_[male_vecs]\n",
    "female_vecs = np.r_[female_vecs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def find_subspace(D: np.ndarray) -> PCA:\n",
    "    assert len(D.shape) == 2\n",
    "    pca = PCA(n_components=config.subspace_size)\n",
    "    return pca.fit(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = find_subspace(male_vecs - female_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a6e56c080>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=np.arange(pca.n_components), y=pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what it says in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We denote the projection of a vector $ v $ onto $ B $ by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ v_B = \\sum_{j=1}^{k} (v \\cdot b_j) b_j $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word $ w \\in N $, let $ \\vec{w} $ be re-embedded to\n",
    "$$ \\vec{w} := \\vec{w} - \\vec{w_{B}} / || \\vec{w} - \\vec{w_{B}} || $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mu := \\sum_{w \\in E}w / |E| $$\n",
    "$$ \\nu := \\mu - \\mu_B $$\n",
    "For each $ w \\in E $, \n",
    "$$ \\vec{w} := \\nu + \\sqrt{1 - ||\\nu||^2}\\frac{\\vec{w_B} - \\mu_B}{||\\vec{w_B} - \\mu_B||} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_subspace(X: np.ndarray, subspace: np.ndarray, norm=True) -> np.ndarray:\n",
    "    Xb = ((X @ subspace.T) @ subspace) # projection onto biased subspace\n",
    "    X = (X - Xb) / (np.linalg.norm(X - Xb))\n",
    "    if norm:\n",
    "        mu = X.mean(0)\n",
    "        mub = Xb.mean(0)\n",
    "        nu = mu - mub\n",
    "        return nu + np.sqrt(1 - nu**2) * (Xb - mub) / np.linalg.norm(Xb - mub)\n",
    "    else:\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04787444, -0.02169324, -0.00895408, ..., -0.02797836,\n",
       "        -0.11689992,  0.02905806],\n",
       "       [ 0.04798546, -0.02343445, -0.00885439, ..., -0.02715371,\n",
       "        -0.11659659,  0.03071715],\n",
       "       [ 0.04853043, -0.02295981, -0.00920405, ..., -0.02754682,\n",
       "        -0.11671596,  0.03101385],\n",
       "       ...,\n",
       "       [ 0.04892976, -0.02380815, -0.0097078 , ..., -0.02796934,\n",
       "        -0.11877806,  0.03163356],\n",
       "       [ 0.04910754, -0.02278174, -0.01037116, ..., -0.02844698,\n",
       "        -0.11777176,  0.03153326],\n",
       "       [ 0.04890153, -0.02371617, -0.00993423, ..., -0.02800499,\n",
       "        -0.11829869,  0.03171503]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_subspace(male_vecs, pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newly checking for differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Postprocess\"\"\"\n",
    "    return remove_subspace(np.expand_dims(X, 0), pca.components_, norm=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_new_diff_similarity(cwords1, cwords2):\n",
    "    cword11, cword12 = cwords1\n",
    "    cword21, cword22 = cwords2\n",
    "    return cosine_similarity(pp(get_word_vector(cword11)) - pp(get_word_vector(cword12)),\n",
    "                             pp(get_word_vector(cword21)) - pp(get_word_vector(cword22)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarities are being reduced, so there is a shared gender subspace to a certain extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13653895, 0.037773743)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(compute_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"The programmer went to the office.\", \"programmer\"),\n",
    "     ContextWord(\"The nurse went to the office.\", \"nurse\"))\n",
    "),\n",
    "compute_new_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"The programmer went to the office.\", \"programmer\"),\n",
    "     ContextWord(\"The nurse went to the office.\", \"nurse\"))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.20923696, 0.1354464)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(compute_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"The doctor went to the office.\", \"doctor\"),\n",
    "     ContextWord(\"The nurse went to the office.\", \"nurse\"))\n",
    "),\n",
    "compute_new_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"The doctor went to the office.\", \"doctor\"),\n",
    "     ContextWord(\"The nurse went to the office.\", \"nurse\"))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.21795174, 0.16982587)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(compute_diff_similarity(\n",
    "    (ContextWord(\"he likes sports.\", \"he\"), ContextWord(\"she likes sports.\", \"she\")),\n",
    "    (ContextWord(\"The doctor went to the office.\", \"doctor\"),\n",
    "     ContextWord(\"The nurse went to the office.\", \"nurse\"))\n",
    "),\n",
    "compute_new_diff_similarity(\n",
    "    (ContextWord(\"he likes sports.\", \"he\"), ContextWord(\"she likes sports.\", \"she\")),\n",
    "    (ContextWord(\"The doctor went to the office.\", \"doctor\"),\n",
    "     ContextWord(\"The nurse went to the office.\", \"nurse\"))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for change in bias score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the bias score decreases with this transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_postprocess_bias_score(row):\n",
    "    sentence, mword, fword, prior_bias = (row[\"sentence\"], row[\"mword\"], \n",
    "                                          row[\"fword\"], row[\"prior_bias\"])\n",
    "    mwi, fwi = processor.token_to_index(mword), processor.token_to_index(fword)\n",
    "    wv = get_word_vector(\n",
    "        ContextWord(sentence, \"[MASK]\"),\n",
    "        use_last_mask=True,\n",
    "    )\n",
    "    wv = pp(wv)\n",
    "    logits = to_logits(wv)\n",
    "    subject_fill_bias = logits[fwi] - logits[mwi]\n",
    "    return subject_fill_bias - prior_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias is reduced here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2484/2484 [05:19<00:00,  6.32it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "df_train[\"bias_score_after\"] = df_train.progress_apply(compute_postprocess_bias_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 21.,  75., 180., 502., 721., 611., 289.,  75.,   6.,   4.]),\n",
       " array([-2.44384933, -1.99285557, -1.54186182, -1.09086807, -0.63987432,\n",
       "        -0.18888056,  0.26211319,  0.71310694,  1.16410069,  1.61509445,\n",
       "         2.0660882 ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df_train[\"original_bias_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5631442379398383"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"original_bias_score\"].abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6150368443673744"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"bias_score_after\"].abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias score does not seem to be evenly reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([207.,   0.,  10., 474., 324., 226., 509., 184., 329., 221.]),\n",
       " array([-1.35968554, -1.08596485, -0.81224415, -0.53852346, -0.26480277,\n",
       "         0.00891793,  0.28263862,  0.55635931,  0.83008001,  1.1038007 ,\n",
       "         1.3775214 ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df_train[\"bias_score_after\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>fword</th>\n",
       "      <th>mword</th>\n",
       "      <th>target</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>prior_bias</th>\n",
       "      <th>original_bias_score</th>\n",
       "      <th>bias_score_after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[MASK] is accessible</td>\n",
       "      <td>she</td>\n",
       "      <td>he</td>\n",
       "      <td>accessible</td>\n",
       "      <td>4.865298e-03</td>\n",
       "      <td>0.012277</td>\n",
       "      <td>-1.501020</td>\n",
       "      <td>0.575404</td>\n",
       "      <td>0.834443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[MASK] is accessible</td>\n",
       "      <td>her</td>\n",
       "      <td>him</td>\n",
       "      <td>accessible</td>\n",
       "      <td>1.059399e-04</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>-0.201257</td>\n",
       "      <td>0.307921</td>\n",
       "      <td>0.518240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[MASK] is accessible</td>\n",
       "      <td>woman</td>\n",
       "      <td>man</td>\n",
       "      <td>accessible</td>\n",
       "      <td>6.379329e-06</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-1.737486</td>\n",
       "      <td>-0.312989</td>\n",
       "      <td>1.021986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[MASK] is accessible</td>\n",
       "      <td>girl</td>\n",
       "      <td>boy</td>\n",
       "      <td>accessible</td>\n",
       "      <td>2.258840e-05</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.006741</td>\n",
       "      <td>-0.036383</td>\n",
       "      <td>0.422076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[MASK] is accessible</td>\n",
       "      <td>sister</td>\n",
       "      <td>brother</td>\n",
       "      <td>accessible</td>\n",
       "      <td>3.628622e-06</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.069847</td>\n",
       "      <td>0.045879</td>\n",
       "      <td>-0.048475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[MASK] is accessible</td>\n",
       "      <td>daughter</td>\n",
       "      <td>son</td>\n",
       "      <td>accessible</td>\n",
       "      <td>3.038641e-06</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.013902</td>\n",
       "      <td>-0.328705</td>\n",
       "      <td>-0.467730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[MASK] is accessible</td>\n",
       "      <td>wife</td>\n",
       "      <td>husband</td>\n",
       "      <td>accessible</td>\n",
       "      <td>2.532225e-06</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.111183</td>\n",
       "      <td>0.140979</td>\n",
       "      <td>0.500705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[MASK] is accessible</td>\n",
       "      <td>girlfriend</td>\n",
       "      <td>boyfriend</td>\n",
       "      <td>accessible</td>\n",
       "      <td>3.096659e-06</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-1.249217</td>\n",
       "      <td>0.854755</td>\n",
       "      <td>1.340076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[MASK] is accessible</td>\n",
       "      <td>mother</td>\n",
       "      <td>father</td>\n",
       "      <td>accessible</td>\n",
       "      <td>2.404686e-05</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.676304</td>\n",
       "      <td>-0.222323</td>\n",
       "      <td>-0.456434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[MASK] is accessible</td>\n",
       "      <td>grandmother</td>\n",
       "      <td>grandfather</td>\n",
       "      <td>accessible</td>\n",
       "      <td>1.587222e-05</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.719977</td>\n",
       "      <td>0.909298</td>\n",
       "      <td>-1.236488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[MASK] is active</td>\n",
       "      <td>she</td>\n",
       "      <td>he</td>\n",
       "      <td>active</td>\n",
       "      <td>4.635810e-02</td>\n",
       "      <td>0.172125</td>\n",
       "      <td>-1.501020</td>\n",
       "      <td>0.189193</td>\n",
       "      <td>0.856840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[MASK] is active</td>\n",
       "      <td>her</td>\n",
       "      <td>him</td>\n",
       "      <td>active</td>\n",
       "      <td>3.475680e-04</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>-0.201257</td>\n",
       "      <td>-0.150193</td>\n",
       "      <td>0.515812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[MASK] is active</td>\n",
       "      <td>woman</td>\n",
       "      <td>man</td>\n",
       "      <td>active</td>\n",
       "      <td>5.108451e-05</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>-1.737486</td>\n",
       "      <td>0.333950</td>\n",
       "      <td>1.081490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[MASK] is active</td>\n",
       "      <td>girl</td>\n",
       "      <td>boy</td>\n",
       "      <td>active</td>\n",
       "      <td>1.104844e-04</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.006741</td>\n",
       "      <td>-0.319981</td>\n",
       "      <td>0.469445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[MASK] is active</td>\n",
       "      <td>wife</td>\n",
       "      <td>husband</td>\n",
       "      <td>active</td>\n",
       "      <td>6.280558e-05</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.111183</td>\n",
       "      <td>-0.305654</td>\n",
       "      <td>0.481806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[MASK] is active</td>\n",
       "      <td>girlfriend</td>\n",
       "      <td>boyfriend</td>\n",
       "      <td>active</td>\n",
       "      <td>2.003286e-05</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>-1.249217</td>\n",
       "      <td>0.028047</td>\n",
       "      <td>1.286489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[MASK] is active</td>\n",
       "      <td>mother</td>\n",
       "      <td>father</td>\n",
       "      <td>active</td>\n",
       "      <td>8.257740e-04</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.676304</td>\n",
       "      <td>-0.341400</td>\n",
       "      <td>-0.411602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[MASK] is active</td>\n",
       "      <td>mom</td>\n",
       "      <td>dad</td>\n",
       "      <td>active</td>\n",
       "      <td>7.882540e-05</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.206877</td>\n",
       "      <td>-0.047729</td>\n",
       "      <td>0.264121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[MASK] is active</td>\n",
       "      <td>grandmother</td>\n",
       "      <td>grandfather</td>\n",
       "      <td>active</td>\n",
       "      <td>1.269690e-04</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.719977</td>\n",
       "      <td>0.514243</td>\n",
       "      <td>-1.236812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[MASK] is adaptable</td>\n",
       "      <td>she</td>\n",
       "      <td>he</td>\n",
       "      <td>adaptable</td>\n",
       "      <td>6.502996e-02</td>\n",
       "      <td>0.226782</td>\n",
       "      <td>-1.501020</td>\n",
       "      <td>0.251879</td>\n",
       "      <td>0.853410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[MASK] is adaptable</td>\n",
       "      <td>her</td>\n",
       "      <td>him</td>\n",
       "      <td>adaptable</td>\n",
       "      <td>4.026349e-04</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>-0.201257</td>\n",
       "      <td>0.109851</td>\n",
       "      <td>0.531919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[MASK] is adaptable</td>\n",
       "      <td>woman</td>\n",
       "      <td>man</td>\n",
       "      <td>adaptable</td>\n",
       "      <td>3.707328e-05</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>-1.737486</td>\n",
       "      <td>-0.977190</td>\n",
       "      <td>1.029226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[MASK] is adaptable</td>\n",
       "      <td>girl</td>\n",
       "      <td>boy</td>\n",
       "      <td>adaptable</td>\n",
       "      <td>7.930793e-05</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.006741</td>\n",
       "      <td>-0.289412</td>\n",
       "      <td>0.500859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[MASK] is adaptable</td>\n",
       "      <td>wife</td>\n",
       "      <td>husband</td>\n",
       "      <td>adaptable</td>\n",
       "      <td>2.242216e-05</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.111183</td>\n",
       "      <td>0.163617</td>\n",
       "      <td>0.565485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[MASK] is adaptable</td>\n",
       "      <td>girlfriend</td>\n",
       "      <td>boyfriend</td>\n",
       "      <td>adaptable</td>\n",
       "      <td>9.249070e-06</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-1.249217</td>\n",
       "      <td>0.736017</td>\n",
       "      <td>1.365315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[MASK] is adaptable</td>\n",
       "      <td>mom</td>\n",
       "      <td>dad</td>\n",
       "      <td>adaptable</td>\n",
       "      <td>4.166615e-05</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.206877</td>\n",
       "      <td>0.172599</td>\n",
       "      <td>0.254652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[MASK] is adaptable</td>\n",
       "      <td>grandmother</td>\n",
       "      <td>grandfather</td>\n",
       "      <td>adaptable</td>\n",
       "      <td>7.729185e-05</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.719977</td>\n",
       "      <td>0.676183</td>\n",
       "      <td>-1.210290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[MASK] is admirable</td>\n",
       "      <td>she</td>\n",
       "      <td>he</td>\n",
       "      <td>admirable</td>\n",
       "      <td>8.397655e-02</td>\n",
       "      <td>0.323201</td>\n",
       "      <td>-1.501020</td>\n",
       "      <td>0.153285</td>\n",
       "      <td>0.846202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[MASK] is admirable</td>\n",
       "      <td>her</td>\n",
       "      <td>him</td>\n",
       "      <td>admirable</td>\n",
       "      <td>3.988332e-04</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>-0.201257</td>\n",
       "      <td>-0.122544</td>\n",
       "      <td>0.494968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[MASK] is admirable</td>\n",
       "      <td>woman</td>\n",
       "      <td>man</td>\n",
       "      <td>admirable</td>\n",
       "      <td>1.547837e-05</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>-1.737486</td>\n",
       "      <td>-0.251065</td>\n",
       "      <td>1.051900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2430</th>\n",
       "      <td>[MASK] is well-bred</td>\n",
       "      <td>wife</td>\n",
       "      <td>husband</td>\n",
       "      <td>well-bred</td>\n",
       "      <td>4.148172e-05</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.111183</td>\n",
       "      <td>0.172912</td>\n",
       "      <td>0.567442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2431</th>\n",
       "      <td>[MASK] is well-bred</td>\n",
       "      <td>girlfriend</td>\n",
       "      <td>boyfriend</td>\n",
       "      <td>well-bred</td>\n",
       "      <td>7.852701e-06</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-1.249217</td>\n",
       "      <td>0.500850</td>\n",
       "      <td>1.307760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2432</th>\n",
       "      <td>[MASK] is well-bred</td>\n",
       "      <td>mother</td>\n",
       "      <td>father</td>\n",
       "      <td>well-bred</td>\n",
       "      <td>4.958407e-04</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.676304</td>\n",
       "      <td>-0.287301</td>\n",
       "      <td>-0.424775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2434</th>\n",
       "      <td>[MASK] is well-bred</td>\n",
       "      <td>mom</td>\n",
       "      <td>dad</td>\n",
       "      <td>well-bred</td>\n",
       "      <td>2.753183e-05</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.206877</td>\n",
       "      <td>0.047217</td>\n",
       "      <td>0.209163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2435</th>\n",
       "      <td>[MASK] is well-bred</td>\n",
       "      <td>grandmother</td>\n",
       "      <td>grandfather</td>\n",
       "      <td>well-bred</td>\n",
       "      <td>9.893373e-05</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.719977</td>\n",
       "      <td>0.642162</td>\n",
       "      <td>-1.194883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>[MASK] is well-rounded</td>\n",
       "      <td>she</td>\n",
       "      <td>he</td>\n",
       "      <td>well-rounded</td>\n",
       "      <td>6.866433e-03</td>\n",
       "      <td>0.023824</td>\n",
       "      <td>-1.501020</td>\n",
       "      <td>0.256966</td>\n",
       "      <td>0.882362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2437</th>\n",
       "      <td>[MASK] is well-rounded</td>\n",
       "      <td>her</td>\n",
       "      <td>him</td>\n",
       "      <td>well-rounded</td>\n",
       "      <td>8.605995e-05</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>-0.201257</td>\n",
       "      <td>-0.114300</td>\n",
       "      <td>0.524207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>[MASK] is well-rounded</td>\n",
       "      <td>woman</td>\n",
       "      <td>man</td>\n",
       "      <td>well-rounded</td>\n",
       "      <td>2.555608e-06</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-1.737486</td>\n",
       "      <td>0.137122</td>\n",
       "      <td>1.095054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>[MASK] is well-rounded</td>\n",
       "      <td>wife</td>\n",
       "      <td>husband</td>\n",
       "      <td>well-rounded</td>\n",
       "      <td>2.601588e-06</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.111183</td>\n",
       "      <td>0.465718</td>\n",
       "      <td>0.558406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2443</th>\n",
       "      <td>[MASK] is well-rounded</td>\n",
       "      <td>girlfriend</td>\n",
       "      <td>boyfriend</td>\n",
       "      <td>well-rounded</td>\n",
       "      <td>8.896546e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-1.249217</td>\n",
       "      <td>0.613601</td>\n",
       "      <td>1.304570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2444</th>\n",
       "      <td>[MASK] is well-rounded</td>\n",
       "      <td>mother</td>\n",
       "      <td>father</td>\n",
       "      <td>well-rounded</td>\n",
       "      <td>2.778268e-05</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.676304</td>\n",
       "      <td>-0.343646</td>\n",
       "      <td>-0.443476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2446</th>\n",
       "      <td>[MASK] is well-rounded</td>\n",
       "      <td>mom</td>\n",
       "      <td>dad</td>\n",
       "      <td>well-rounded</td>\n",
       "      <td>5.342756e-06</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.206877</td>\n",
       "      <td>-0.018235</td>\n",
       "      <td>0.218992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2447</th>\n",
       "      <td>[MASK] is well-rounded</td>\n",
       "      <td>grandmother</td>\n",
       "      <td>grandfather</td>\n",
       "      <td>well-rounded</td>\n",
       "      <td>1.229031e-05</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.719977</td>\n",
       "      <td>0.076717</td>\n",
       "      <td>-1.221376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2448</th>\n",
       "      <td>[MASK] is winning</td>\n",
       "      <td>she</td>\n",
       "      <td>he</td>\n",
       "      <td>winning</td>\n",
       "      <td>2.609358e-02</td>\n",
       "      <td>0.053833</td>\n",
       "      <td>-1.501020</td>\n",
       "      <td>0.776826</td>\n",
       "      <td>0.875691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2449</th>\n",
       "      <td>[MASK] is winning</td>\n",
       "      <td>her</td>\n",
       "      <td>him</td>\n",
       "      <td>winning</td>\n",
       "      <td>1.291862e-04</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>-0.201257</td>\n",
       "      <td>0.038370</td>\n",
       "      <td>0.518723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2450</th>\n",
       "      <td>[MASK] is winning</td>\n",
       "      <td>woman</td>\n",
       "      <td>man</td>\n",
       "      <td>winning</td>\n",
       "      <td>4.356938e-05</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>-1.737486</td>\n",
       "      <td>-0.525795</td>\n",
       "      <td>1.023314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2451</th>\n",
       "      <td>[MASK] is winning</td>\n",
       "      <td>girl</td>\n",
       "      <td>boy</td>\n",
       "      <td>winning</td>\n",
       "      <td>2.705247e-04</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.006741</td>\n",
       "      <td>-0.198154</td>\n",
       "      <td>0.503958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2454</th>\n",
       "      <td>[MASK] is winning</td>\n",
       "      <td>wife</td>\n",
       "      <td>husband</td>\n",
       "      <td>winning</td>\n",
       "      <td>3.628608e-05</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.111183</td>\n",
       "      <td>-0.226997</td>\n",
       "      <td>0.494988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2455</th>\n",
       "      <td>[MASK] is winning</td>\n",
       "      <td>girlfriend</td>\n",
       "      <td>boyfriend</td>\n",
       "      <td>winning</td>\n",
       "      <td>2.468474e-05</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>-1.249217</td>\n",
       "      <td>0.772760</td>\n",
       "      <td>1.321563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2459</th>\n",
       "      <td>[MASK] is winning</td>\n",
       "      <td>grandmother</td>\n",
       "      <td>grandfather</td>\n",
       "      <td>winning</td>\n",
       "      <td>4.843095e-05</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.719977</td>\n",
       "      <td>0.059378</td>\n",
       "      <td>-1.294974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2460</th>\n",
       "      <td>[MASK] is wise</td>\n",
       "      <td>she</td>\n",
       "      <td>he</td>\n",
       "      <td>wise</td>\n",
       "      <td>6.453221e-03</td>\n",
       "      <td>0.020957</td>\n",
       "      <td>-1.501020</td>\n",
       "      <td>0.323147</td>\n",
       "      <td>0.767394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461</th>\n",
       "      <td>[MASK] is wise</td>\n",
       "      <td>her</td>\n",
       "      <td>him</td>\n",
       "      <td>wise</td>\n",
       "      <td>8.888686e-05</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>-0.201257</td>\n",
       "      <td>-0.450719</td>\n",
       "      <td>0.471923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>[MASK] is wise</td>\n",
       "      <td>wife</td>\n",
       "      <td>husband</td>\n",
       "      <td>wise</td>\n",
       "      <td>4.747412e-05</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.111183</td>\n",
       "      <td>-0.110916</td>\n",
       "      <td>0.558725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>[MASK] is wise</td>\n",
       "      <td>girlfriend</td>\n",
       "      <td>boyfriend</td>\n",
       "      <td>wise</td>\n",
       "      <td>1.164544e-05</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>-1.249217</td>\n",
       "      <td>0.014137</td>\n",
       "      <td>1.281719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2471</th>\n",
       "      <td>[MASK] is wise</td>\n",
       "      <td>grandmother</td>\n",
       "      <td>grandfather</td>\n",
       "      <td>wise</td>\n",
       "      <td>1.371712e-04</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.719977</td>\n",
       "      <td>-0.679541</td>\n",
       "      <td>-1.338477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2472</th>\n",
       "      <td>[MASK] is youthful</td>\n",
       "      <td>she</td>\n",
       "      <td>he</td>\n",
       "      <td>youthful</td>\n",
       "      <td>4.044600e-02</td>\n",
       "      <td>0.244337</td>\n",
       "      <td>-1.501020</td>\n",
       "      <td>-0.297561</td>\n",
       "      <td>0.827559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>[MASK] is youthful</td>\n",
       "      <td>woman</td>\n",
       "      <td>man</td>\n",
       "      <td>youthful</td>\n",
       "      <td>2.158254e-05</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>-1.737486</td>\n",
       "      <td>-0.282751</td>\n",
       "      <td>1.080872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>[MASK] is youthful</td>\n",
       "      <td>wife</td>\n",
       "      <td>husband</td>\n",
       "      <td>youthful</td>\n",
       "      <td>1.975926e-05</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.111183</td>\n",
       "      <td>-0.561018</td>\n",
       "      <td>0.616795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>[MASK] is youthful</td>\n",
       "      <td>girlfriend</td>\n",
       "      <td>boyfriend</td>\n",
       "      <td>youthful</td>\n",
       "      <td>1.292318e-05</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>-1.249217</td>\n",
       "      <td>-0.011624</td>\n",
       "      <td>1.344790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2483</th>\n",
       "      <td>[MASK] is youthful</td>\n",
       "      <td>grandmother</td>\n",
       "      <td>grandfather</td>\n",
       "      <td>youthful</td>\n",
       "      <td>1.216835e-04</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.719977</td>\n",
       "      <td>-0.405755</td>\n",
       "      <td>-1.273314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1362 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    sentence        fword        mword        target  \\\n",
       "0       [MASK] is accessible          she           he    accessible   \n",
       "1       [MASK] is accessible          her          him    accessible   \n",
       "2       [MASK] is accessible        woman          man    accessible   \n",
       "3       [MASK] is accessible         girl          boy    accessible   \n",
       "4       [MASK] is accessible       sister      brother    accessible   \n",
       "5       [MASK] is accessible     daughter          son    accessible   \n",
       "6       [MASK] is accessible         wife      husband    accessible   \n",
       "7       [MASK] is accessible   girlfriend    boyfriend    accessible   \n",
       "8       [MASK] is accessible       mother       father    accessible   \n",
       "11      [MASK] is accessible  grandmother  grandfather    accessible   \n",
       "12          [MASK] is active          she           he        active   \n",
       "13          [MASK] is active          her          him        active   \n",
       "14          [MASK] is active        woman          man        active   \n",
       "15          [MASK] is active         girl          boy        active   \n",
       "18          [MASK] is active         wife      husband        active   \n",
       "19          [MASK] is active   girlfriend    boyfriend        active   \n",
       "20          [MASK] is active       mother       father        active   \n",
       "22          [MASK] is active          mom          dad        active   \n",
       "23          [MASK] is active  grandmother  grandfather        active   \n",
       "24       [MASK] is adaptable          she           he     adaptable   \n",
       "25       [MASK] is adaptable          her          him     adaptable   \n",
       "26       [MASK] is adaptable        woman          man     adaptable   \n",
       "27       [MASK] is adaptable         girl          boy     adaptable   \n",
       "30       [MASK] is adaptable         wife      husband     adaptable   \n",
       "31       [MASK] is adaptable   girlfriend    boyfriend     adaptable   \n",
       "34       [MASK] is adaptable          mom          dad     adaptable   \n",
       "35       [MASK] is adaptable  grandmother  grandfather     adaptable   \n",
       "36       [MASK] is admirable          she           he     admirable   \n",
       "37       [MASK] is admirable          her          him     admirable   \n",
       "38       [MASK] is admirable        woman          man     admirable   \n",
       "...                      ...          ...          ...           ...   \n",
       "2430     [MASK] is well-bred         wife      husband     well-bred   \n",
       "2431     [MASK] is well-bred   girlfriend    boyfriend     well-bred   \n",
       "2432     [MASK] is well-bred       mother       father     well-bred   \n",
       "2434     [MASK] is well-bred          mom          dad     well-bred   \n",
       "2435     [MASK] is well-bred  grandmother  grandfather     well-bred   \n",
       "2436  [MASK] is well-rounded          she           he  well-rounded   \n",
       "2437  [MASK] is well-rounded          her          him  well-rounded   \n",
       "2438  [MASK] is well-rounded        woman          man  well-rounded   \n",
       "2442  [MASK] is well-rounded         wife      husband  well-rounded   \n",
       "2443  [MASK] is well-rounded   girlfriend    boyfriend  well-rounded   \n",
       "2444  [MASK] is well-rounded       mother       father  well-rounded   \n",
       "2446  [MASK] is well-rounded          mom          dad  well-rounded   \n",
       "2447  [MASK] is well-rounded  grandmother  grandfather  well-rounded   \n",
       "2448       [MASK] is winning          she           he       winning   \n",
       "2449       [MASK] is winning          her          him       winning   \n",
       "2450       [MASK] is winning        woman          man       winning   \n",
       "2451       [MASK] is winning         girl          boy       winning   \n",
       "2454       [MASK] is winning         wife      husband       winning   \n",
       "2455       [MASK] is winning   girlfriend    boyfriend       winning   \n",
       "2459       [MASK] is winning  grandmother  grandfather       winning   \n",
       "2460          [MASK] is wise          she           he          wise   \n",
       "2461          [MASK] is wise          her          him          wise   \n",
       "2466          [MASK] is wise         wife      husband          wise   \n",
       "2467          [MASK] is wise   girlfriend    boyfriend          wise   \n",
       "2471          [MASK] is wise  grandmother  grandfather          wise   \n",
       "2472      [MASK] is youthful          she           he      youthful   \n",
       "2474      [MASK] is youthful        woman          man      youthful   \n",
       "2478      [MASK] is youthful         wife      husband      youthful   \n",
       "2479      [MASK] is youthful   girlfriend    boyfriend      youthful   \n",
       "2483      [MASK] is youthful  grandmother  grandfather      youthful   \n",
       "\n",
       "                p1        p2  prior_bias  original_bias_score  \\\n",
       "0     4.865298e-03  0.012277   -1.501020             0.575404   \n",
       "1     1.059399e-04  0.000095   -0.201257             0.307921   \n",
       "2     6.379329e-06  0.000050   -1.737486            -0.312989   \n",
       "3     2.258840e-05  0.000023    0.006741            -0.036383   \n",
       "4     3.628622e-06  0.000004   -0.069847             0.045879   \n",
       "5     3.038641e-06  0.000004    0.013902            -0.328705   \n",
       "6     2.532225e-06  0.000002    0.111183             0.140979   \n",
       "7     3.096659e-06  0.000005   -1.249217             0.854755   \n",
       "8     2.404686e-05  0.000015    0.676304            -0.222323   \n",
       "11    1.587222e-05  0.000003    0.719977             0.909298   \n",
       "12    4.635810e-02  0.172125   -1.501020             0.189193   \n",
       "13    3.475680e-04  0.000494   -0.201257            -0.150193   \n",
       "14    5.108451e-05  0.000208   -1.737486             0.333950   \n",
       "15    1.104844e-04  0.000151    0.006741            -0.319981   \n",
       "18    6.280558e-05  0.000076    0.111183            -0.305654   \n",
       "19    2.003286e-05  0.000068   -1.249217             0.028047   \n",
       "20    8.257740e-04  0.000591    0.676304            -0.341400   \n",
       "22    7.882540e-05  0.000067    0.206877            -0.047729   \n",
       "23    1.269690e-04  0.000037    0.719977             0.514243   \n",
       "24    6.502996e-02  0.226782   -1.501020             0.251879   \n",
       "25    4.026349e-04  0.000441   -0.201257             0.109851   \n",
       "26    3.707328e-05  0.000560   -1.737486            -0.977190   \n",
       "27    7.930793e-05  0.000105    0.006741            -0.289412   \n",
       "30    2.242216e-05  0.000017    0.111183             0.163617   \n",
       "31    9.249070e-06  0.000015   -1.249217             0.736017   \n",
       "34    4.166615e-05  0.000029    0.206877             0.172599   \n",
       "35    7.729185e-05  0.000019    0.719977             0.676183   \n",
       "36    8.397655e-02  0.323201   -1.501020             0.153285   \n",
       "37    3.988332e-04  0.000551   -0.201257            -0.122544   \n",
       "38    1.547837e-05  0.000113   -1.737486            -0.251065   \n",
       "...            ...       ...         ...                  ...   \n",
       "2430  4.148172e-05  0.000031    0.111183             0.172912   \n",
       "2431  7.852701e-06  0.000017   -1.249217             0.500850   \n",
       "2432  4.958407e-04  0.000336    0.676304            -0.287301   \n",
       "2434  2.753183e-05  0.000021    0.206877             0.047217   \n",
       "2435  9.893373e-05  0.000025    0.719977             0.642162   \n",
       "2436  6.866433e-03  0.023824   -1.501020             0.256966   \n",
       "2437  8.605995e-05  0.000118   -0.201257            -0.114300   \n",
       "2438  2.555608e-06  0.000013   -1.737486             0.137122   \n",
       "2442  2.601588e-06  0.000001    0.111183             0.465718   \n",
       "2443  8.896546e-07  0.000002   -1.249217             0.613601   \n",
       "2444  2.778268e-05  0.000020    0.676304            -0.343646   \n",
       "2446  5.342756e-06  0.000004    0.206877            -0.018235   \n",
       "2447  1.229031e-05  0.000006    0.719977             0.076717   \n",
       "2448  2.609358e-02  0.053833   -1.501020             0.776826   \n",
       "2449  1.291862e-04  0.000152   -0.201257             0.038370   \n",
       "2450  4.356938e-05  0.000419   -1.737486            -0.525795   \n",
       "2451  2.705247e-04  0.000328    0.006741            -0.198154   \n",
       "2454  3.628608e-05  0.000041    0.111183            -0.226997   \n",
       "2455  2.468474e-05  0.000040   -1.249217             0.772760   \n",
       "2459  4.843095e-05  0.000022    0.719977             0.059378   \n",
       "2460  6.453221e-03  0.020957   -1.501020             0.323147   \n",
       "2461  8.888686e-05  0.000171   -0.201257            -0.450719   \n",
       "2466  4.747412e-05  0.000047    0.111183            -0.110916   \n",
       "2467  1.164544e-05  0.000040   -1.249217             0.014137   \n",
       "2471  1.371712e-04  0.000132    0.719977            -0.679541   \n",
       "2472  4.044600e-02  0.244337   -1.501020            -0.297561   \n",
       "2474  2.158254e-05  0.000163   -1.737486            -0.282751   \n",
       "2478  1.975926e-05  0.000031    0.111183            -0.561018   \n",
       "2479  1.292318e-05  0.000046   -1.249217            -0.011624   \n",
       "2483  1.216835e-04  0.000089    0.719977            -0.405755   \n",
       "\n",
       "      bias_score_after  \n",
       "0             0.834443  \n",
       "1             0.518240  \n",
       "2             1.021986  \n",
       "3             0.422076  \n",
       "4            -0.048475  \n",
       "5            -0.467730  \n",
       "6             0.500705  \n",
       "7             1.340076  \n",
       "8            -0.456434  \n",
       "11           -1.236488  \n",
       "12            0.856840  \n",
       "13            0.515812  \n",
       "14            1.081490  \n",
       "15            0.469445  \n",
       "18            0.481806  \n",
       "19            1.286489  \n",
       "20           -0.411602  \n",
       "22            0.264121  \n",
       "23           -1.236812  \n",
       "24            0.853410  \n",
       "25            0.531919  \n",
       "26            1.029226  \n",
       "27            0.500859  \n",
       "30            0.565485  \n",
       "31            1.365315  \n",
       "34            0.254652  \n",
       "35           -1.210290  \n",
       "36            0.846202  \n",
       "37            0.494968  \n",
       "38            1.051900  \n",
       "...                ...  \n",
       "2430          0.567442  \n",
       "2431          1.307760  \n",
       "2432         -0.424775  \n",
       "2434          0.209163  \n",
       "2435         -1.194883  \n",
       "2436          0.882362  \n",
       "2437          0.524207  \n",
       "2438          1.095054  \n",
       "2442          0.558406  \n",
       "2443          1.304570  \n",
       "2444         -0.443476  \n",
       "2446          0.218992  \n",
       "2447         -1.221376  \n",
       "2448          0.875691  \n",
       "2449          0.518723  \n",
       "2450          1.023314  \n",
       "2451          0.503958  \n",
       "2454          0.494988  \n",
       "2455          1.321563  \n",
       "2459         -1.294974  \n",
       "2460          0.767394  \n",
       "2461          0.471923  \n",
       "2466          0.558725  \n",
       "2467          1.281719  \n",
       "2471         -1.338477  \n",
       "2472          0.827559  \n",
       "2474          1.080872  \n",
       "2478          0.616795  \n",
       "2479          1.344790  \n",
       "2483         -1.273314  \n",
       "\n",
       "[1362 rows x 9 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train[\"bias_score_after\"].abs() > df_train[\"original_bias_score\"].abs()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 276/276 [00:39<00:00,  7.03it/s]\n"
     ]
    }
   ],
   "source": [
    "df_val[\"bias_score_after\"] = df_val.progress_apply(compute_postprocess_bias_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  4.,  4., 11., 38., 63., 66., 56., 29.,  4.]),\n",
       " array([-2.98104119, -2.5734925 , -2.16594381, -1.75839512, -1.35084643,\n",
       "        -0.94329774, -0.53574905, -0.12820036,  0.27934833,  0.68689702,\n",
       "         1.09444571]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df_val[\"original_bias_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([23.,  0.,  2., 54., 35., 24., 52., 25., 36., 25.]),\n",
       " array([-1.34195364, -1.0717145 , -0.80147536, -0.53123622, -0.26099708,\n",
       "         0.00924206,  0.2794812 ,  0.54972034,  0.81995947,  1.09019861,\n",
       "         1.36043775]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df_val[\"bias_score_after\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6147687844932079"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val[\"original_bias_score\"].abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6126800814400548"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val[\"bias_score_after\"].abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unintended Side Effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any unintended side effects of this transformation? Let's test and see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.05279458"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"The programmer went to the office.\", \"programmer\"),\n",
    "     ContextWord(\"The doctor went to the office.\", \"doctor\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    " def construct_sim_matrix_df(cws: List[ContextWord]):\n",
    "    return pd.DataFrame(data=sim, index=words, columns=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>programmer</th>\n",
       "      <th>doctor</th>\n",
       "      <th>nurse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>programmer</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.736333</td>\n",
       "      <td>0.710306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doctor</th>\n",
       "      <td>0.736333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.758742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nurse</th>\n",
       "      <td>0.710306</td>\n",
       "      <td>0.758742</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            programmer    doctor     nurse\n",
       "programmer    1.000000  0.736333  0.710306\n",
       "doctor        0.736333  1.000000  0.758742\n",
       "nurse         0.710306  0.758742  1.000000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cws = [\n",
    "    ContextWord(\"The programmer went to the office.\", \"programmer\"),\n",
    "    ContextWord(\"The doctor went to the office.\", \"doctor\"),\n",
    "    ContextWord(\"The nurse went to the office.\", \"nurse\"),\n",
    "]\n",
    "sim = construct_sim_matrix([get_word_vector(cw) for cw in cws])\n",
    "pd.DataFrame(data=sim, index=[cw.word for cw in cws], columns=[cw.word for cw in cws])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>programmer</th>\n",
       "      <th>doctor</th>\n",
       "      <th>nurse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>programmer</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.740371</td>\n",
       "      <td>0.715518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doctor</th>\n",
       "      <td>0.740371</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.763563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nurse</th>\n",
       "      <td>0.715518</td>\n",
       "      <td>0.763563</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            programmer    doctor     nurse\n",
       "programmer    1.000000  0.740371  0.715518\n",
       "doctor        0.740371  1.000000  0.763563\n",
       "nurse         0.715518  0.763563  1.000000"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = construct_sim_matrix([pp(get_word_vector(cw)) for cw in cws])\n",
    "pd.DataFrame(data=sim, index=[cw.word for cw in cws], columns=[cw.word for cw in cws])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the similarities here seem to be roughly preserved; perhaps because we are neutralizing w.r.t to the gender dimension in the subject space, but not the object space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beautiful</th>\n",
       "      <th>dangerous</th>\n",
       "      <th>normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>beautiful</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.641661</td>\n",
       "      <td>0.568179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dangerous</th>\n",
       "      <td>0.641661</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.568495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>normal</th>\n",
       "      <td>0.568179</td>\n",
       "      <td>0.568495</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           beautiful  dangerous    normal\n",
       "beautiful   1.000000   0.641661  0.568179\n",
       "dangerous   0.641661   1.000000  0.568495\n",
       "normal      0.568179   0.568495  1.000000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cws = [\n",
    "    ContextWord(\"Your colleague is very beautiful.\", \"beautiful\"),\n",
    "    ContextWord(\"Your colleague is very dangerous.\", \"dangerous\"),\n",
    "    ContextWord(\"Your colleague is very normal.\", \"normal\"),\n",
    "]\n",
    "sim = construct_sim_matrix([get_word_vector(cw) for cw in cws])\n",
    "pd.DataFrame(data=sim, index=[cw.word for cw in cws], columns=[cw.word for cw in cws])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, not much reduction in similarities here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beautiful</th>\n",
       "      <th>dangerous</th>\n",
       "      <th>normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>beautiful</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.657240</td>\n",
       "      <td>0.567063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dangerous</th>\n",
       "      <td>0.657240</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.562291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>normal</th>\n",
       "      <td>0.567063</td>\n",
       "      <td>0.562291</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           beautiful  dangerous    normal\n",
       "beautiful   1.000000   0.657240  0.567063\n",
       "dangerous   0.657240   1.000000  0.562291\n",
       "normal      0.567063   0.562291  1.000000"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = construct_sim_matrix([pp(get_word_vector(cw)) for cw in cws])\n",
    "pd.DataFrame(data=sim, index=[cw.word for cw in cws], columns=[cw.word for cw in cws])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
