{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_utils import Config, BertPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    model_type=\"bert-base-uncased\",\n",
    "    max_seq_len=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BertPreprocessor(config.model_type, config.max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import BertConfig, BertForMaskedLM\n",
    "model = BertForMaskedLM.from_pretrained(config.model_type)\n",
    "model.eval() # Important! Disable dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(sentence: str) -> np.ndarray:\n",
    "    return model(processor.to_bert_model_input(sentence))[0, :, :].cpu().detach().numpy()\n",
    "\n",
    "def softmax(arr, axis=1):\n",
    "    e = np.exp(arr)\n",
    "    return e / e.sum(axis=axis, keepdims=True)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Parameters: sentence string, list of gender words\n",
    "def get_mask_fill_logits(sentence: str, words: Iterable[str],\n",
    "                         use_last_mask=False, apply_softmax=True) -> Dict[str, float]:\n",
    "    mask_i = processor.get_index(sentence, \"[MASK]\", last=use_last_mask, accept_wordpiece=True)\n",
    "    logits = defaultdict(list)\n",
    "    out_logits = get_logits(sentence)\n",
    "    if apply_softmax: \n",
    "        out_logits = softmax(out_logits)\n",
    "    return {w: out_logits[mask_i, processor.token_to_index(w, accept_wordpiece=True)] for w in words}\n",
    "\n",
    "def bias_score(sentence: str, gender_words: Iterable[Iterable[str]], \n",
    "               word: str, gender_comes_first=True) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Input a sentence of the form \"GGG is XXX\"\n",
    "    XXX is a placeholder for the target word\n",
    "    GGG is a placeholder for the gendered words (the subject)\n",
    "    We will predict the bias when filling in the gendered words and \n",
    "    filling in the target word.\n",
    "    \n",
    "    gender_comes_first: whether GGG comes before XXX (TODO: better way of handling this?)\n",
    "    \"\"\"\n",
    "    # probability of filling [MASK] with \"he\" vs. \"she\" when target is \"programmer\"\n",
    "    mwords, fwords = gender_words\n",
    "    all_words = mwords + fwords\n",
    "    subject_fill_logits = get_mask_fill_logits(\n",
    "        sentence.replace(\"XXX\", word).replace(\"GGG\", \"[MASK]\"), \n",
    "        all_words, use_last_mask=not gender_comes_first,\n",
    "    )\n",
    "    subject_fill_bias = np.log(sum(subject_fill_logits[mw] for mw in mwords)) - \\\n",
    "                        np.log(sum(subject_fill_logits[fw] for fw in fwords))\n",
    "    # male words are simply more likely than female words\n",
    "    # correct for this by masking the target word and measuring the prior probabilities\n",
    "    subject_fill_prior_logits = get_mask_fill_logits(\n",
    "        sentence.replace(\"XXX\", \"[MASK]\").replace(\"GGG\", \"[MASK]\"), \n",
    "        all_words, use_last_mask=gender_comes_first,\n",
    "    )\n",
    "    subject_fill_bias_prior_correction = \\\n",
    "            np.log(sum(subject_fill_prior_logits[mw] for mw in mwords)) - \\\n",
    "            np.log(sum(subject_fill_prior_logits[fw] for fw in fwords))\n",
    "    \n",
    "    return {\n",
    "            \"stimulus\": word,\n",
    "            \"bias\": subject_fill_bias,\n",
    "            \"prior_correction\": subject_fill_bias_prior_correction,\n",
    "            \"bias_prior_corrected\": subject_fill_bias - subject_fill_bias_prior_correction,\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'flower': 0.0007418048, 'bug': 1.0748333e-05}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mask_fill_logits(\"the [MASK] is beautiful\", [\"flower\", \"bug\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(sentence: str, word: str):\n",
    "    idx = processor.get_index(sentence, word, accept_wordpiece=True)\n",
    "    outputs = None\n",
    "    with torch.no_grad():\n",
    "        sequence_output, _ = model.bert(processor.to_bert_model_input(sentence),\n",
    "                                        output_all_encoded_layers=False)\n",
    "        sequence_output.squeeze_(0)\n",
    "    return sequence_output.detach().cpu().numpy()[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(x, y):\n",
    "    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_effect_size(df1, df2, k=\"bias_prior_corrected\"):\n",
    "    diff = (df1[k].mean() - df2[k].mean())\n",
    "    std_ = pd.concat([df1, df2], axis=0)[k].std() + 1e-8\n",
    "    return diff / std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_mc_perm_test(xs, ys, nmc=100000):\n",
    "    n, k = len(xs), 0\n",
    "    diff = np.abs(np.mean(xs) - np.mean(ys))\n",
    "    zs = np.concatenate([xs, ys])\n",
    "    for j in range(nmc):\n",
    "        np.random.shuffle(zs)\n",
    "        k += diff < np.abs(np.mean(zs[:n]) - np.mean(zs[n:]))\n",
    "    return k / nmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.06369518e-01,  2.22109765e-01, -7.18901008e-02,  1.70864940e-01,\n",
       "        3.62271309e-01, -1.20856151e-01, -6.16566688e-02,  7.87993133e-01,\n",
       "        3.73470753e-01, -2.52068713e-02,  2.59382874e-02, -1.49484551e+00,\n",
       "       -1.80714473e-01,  1.11428094e+00, -1.03053010e+00,  8.63718510e-01,\n",
       "        1.34956911e-01,  1.07128906e+00,  1.33132502e-01,  8.86740446e-01,\n",
       "       -1.61311820e-01, -5.93276024e-01,  7.23845139e-02, -5.57699025e-01,\n",
       "        1.10267961e+00, -5.62288880e-01, -2.21851707e-01,  8.34756911e-01,\n",
       "        4.88106251e-01, -9.58714262e-03,  2.16243923e-01,  1.58447057e-01,\n",
       "        5.18487453e-01,  2.14277059e-01,  1.41341984e-01, -2.22598538e-01,\n",
       "       -1.90466657e-01,  3.14576507e-01, -4.24953938e-01, -3.25795531e-01,\n",
       "       -7.41146803e-01, -1.04409897e+00,  6.84070289e-01,  4.49090868e-01,\n",
       "        2.56366879e-01, -7.32405782e-01, -9.85546529e-01,  3.55274409e-01,\n",
       "        9.63768840e-01,  3.46497953e-01,  5.14259525e-02,  5.61742187e-01,\n",
       "        2.53276885e-01, -1.06839433e-01,  4.62810725e-01,  5.47232807e-01,\n",
       "       -8.03264678e-01, -7.77305663e-01, -2.00483233e-01, -5.28368711e-01,\n",
       "       -4.77690250e-01,  8.90959650e-02,  3.79050970e-01, -1.00972667e-01,\n",
       "        9.57354531e-02, -8.97196382e-02, -1.48199931e-01,  1.25380859e-01,\n",
       "       -9.09168124e-01, -4.43255037e-01,  1.24072999e-01,  4.07962769e-01,\n",
       "        3.49193603e-01, -5.10468483e-02, -4.53874648e-01,  2.38604665e-01,\n",
       "       -3.83497596e-01,  2.26237416e-01, -2.38299310e-01, -4.38013732e-01,\n",
       "       -2.28098884e-01, -1.11407094e-01,  4.13440675e-01,  3.87610048e-01,\n",
       "       -9.92386416e-02,  8.58975649e-02, -9.76567626e-01,  4.05914634e-02,\n",
       "       -8.99850503e-02,  2.05641791e-01,  1.58097193e-01, -1.85252249e-01,\n",
       "       -5.00990570e-01,  2.39046440e-01, -1.34954855e-01, -4.15777266e-01,\n",
       "        7.87932426e-02,  4.71143544e-01, -7.26344362e-02,  2.40169138e-01,\n",
       "        8.60921502e-01, -8.16041604e-02, -2.44584411e-01,  4.08138633e-01,\n",
       "       -6.02750778e-01, -4.35080618e-01, -6.49739131e-02, -1.62537664e-01,\n",
       "        2.65623093e-01,  7.45046198e-01, -7.62214780e-01,  4.02592182e-01,\n",
       "       -4.47386235e-01,  1.19652815e-01, -1.14981580e+00,  1.24807334e+00,\n",
       "        5.31868756e-01, -4.88355666e-01,  1.46929264e-01, -2.39980385e-01,\n",
       "        6.72569633e-01, -3.89886677e-01,  3.02861094e-01,  4.00121808e-01,\n",
       "       -1.69996589e-01, -5.11888385e-01,  1.92454547e-01,  1.86521038e-01,\n",
       "       -8.37388575e-01,  1.05222240e-01, -4.64141965e-01,  4.67706263e-01,\n",
       "       -1.85250908e-01, -5.01711130e-01,  2.23009974e-01, -9.75936174e-01,\n",
       "       -1.94810122e-01, -2.00362608e-01, -7.91729391e-02,  4.77513000e-02,\n",
       "       -3.62029582e-01,  5.16069531e-01, -1.61559805e-01,  1.06315064e+00,\n",
       "       -8.03826302e-02,  6.59516811e-01, -7.46768713e-03,  8.29806849e-02,\n",
       "       -2.24850640e-01, -7.57497907e-01,  4.00907844e-01,  2.02943161e-01,\n",
       "       -6.59538954e-02, -1.73705071e-03, -2.10254133e-01,  3.24883103e-01,\n",
       "       -6.15544558e-01,  6.77369773e-01,  1.87589675e-01,  3.29706892e-02,\n",
       "       -3.59585941e-01, -7.53547549e-01,  1.65893540e-01,  4.97108281e-01,\n",
       "       -3.97464573e-01, -4.43160743e-01,  3.82503033e-01,  2.65457958e-01,\n",
       "       -7.97319114e-02,  7.20816493e-01, -8.93282712e-01, -5.99095449e-02,\n",
       "        2.71350324e-01,  6.12361990e-02,  9.22667027e-01, -2.92799294e-01,\n",
       "        2.65346438e-01, -8.09285223e-01, -3.07702005e-01,  6.68435872e-01,\n",
       "       -2.71839440e-01,  6.16511285e-01,  4.73261029e-01,  7.08366871e-01,\n",
       "       -8.79255906e-02, -4.17531192e-01,  1.00453699e+00,  5.55702388e-01,\n",
       "       -2.85637379e-01, -1.49904251e-01, -1.97960362e-01, -6.10363066e-01,\n",
       "       -2.67986596e-01,  2.12840974e-01, -1.73632234e-01, -9.85309184e-02,\n",
       "       -3.27348746e-02, -3.27062845e-01, -9.93386865e-01, -3.90170932e-01,\n",
       "        6.61132514e-01,  1.64619207e-01, -2.00865895e-01, -4.29663748e-01,\n",
       "       -5.69214761e-01, -1.22738987e-01,  5.86378932e-01, -4.87890422e-01,\n",
       "        5.74026227e-01,  3.13331425e-01, -2.59083867e-01, -9.16624442e-03,\n",
       "        9.99531269e-01, -3.05078804e-01,  4.39469904e-01,  4.63630743e-02,\n",
       "       -6.72846735e-02,  1.23664290e-02, -6.13555312e-03,  1.25280889e-02,\n",
       "        3.91279519e-01,  2.25989610e-01, -5.14249146e-01,  7.67559767e-01,\n",
       "       -5.49520433e-01,  4.78257179e-01,  6.96447790e-01, -4.86617714e-01,\n",
       "        2.84597576e-01, -7.60636330e-02,  2.09488004e-01,  3.72304559e-01,\n",
       "        8.20942223e-01, -2.82124847e-01, -2.64740586e-01,  3.12003314e-01,\n",
       "       -1.26166776e-01,  1.17298141e-02, -1.29803941e-01,  7.37189054e-01,\n",
       "        7.90811926e-02,  5.44379234e-01, -1.34595215e-01,  8.24179649e-02,\n",
       "        3.50830287e-01, -3.43899339e-01,  3.32313001e-01, -2.02917427e-01,\n",
       "        5.53840280e-01, -6.55833423e-01, -2.59503275e-01,  4.76769000e-01,\n",
       "        3.65739167e-02, -5.60237765e-01, -1.65224820e-01, -7.70691559e-02,\n",
       "        8.48130882e-02,  6.74858630e-01,  1.67155713e-01, -5.50797656e-02,\n",
       "        2.92654395e-01,  9.48140994e-02,  3.49618852e-01, -1.01741917e-01,\n",
       "       -5.75546801e-01, -9.16646868e-02, -4.70333636e-01,  1.39525473e-01,\n",
       "        1.03665516e-01, -8.79752859e-02, -3.27949286e-01,  1.46400273e-01,\n",
       "       -1.55137390e-01,  2.56914496e-01, -4.41621423e-01, -6.69949174e-01,\n",
       "       -5.78222036e-01, -2.89916277e-01, -4.90753427e-02, -5.57360798e-02,\n",
       "        3.50906134e-01,  2.79748768e-01, -7.24173903e-01, -1.26637161e-01,\n",
       "        4.46231216e-01, -4.43808228e-01,  2.34892920e-01,  6.79355025e-01,\n",
       "       -3.44263196e-01, -1.74710751e-01, -9.37994778e-01,  5.34851789e-01,\n",
       "       -4.74429935e-01,  9.28932279e-02,  2.20538303e-01, -2.23297596e-01,\n",
       "        4.19267237e-01,  4.41834703e-02,  9.44654644e-01, -2.93301828e-02,\n",
       "        5.16590834e-01,  3.71194452e-01,  6.15360677e-01, -5.15531898e-01,\n",
       "       -4.88995641e-01,  7.38495290e-01, -1.74698308e-01,  2.31962964e-01,\n",
       "       -4.53382158e+00,  1.38129786e-01, -3.99393812e-02,  2.39824858e-02,\n",
       "        4.55935359e-01,  1.36820689e-01, -1.29579142e-01, -1.26011461e-01,\n",
       "        1.06470615e-01,  1.37062132e-01,  7.21079528e-01, -1.68523148e-01,\n",
       "        9.27704811e-01,  1.15799412e-01,  3.16249192e-01, -4.17039871e-01,\n",
       "        3.86143118e-01, -1.19816065e+00, -3.30826253e-01,  1.68343291e-01,\n",
       "       -6.97976947e-01,  7.14322254e-02,  1.21875301e-01,  4.71082747e-01,\n",
       "        1.15364641e-01,  2.85469480e-02,  1.08732253e-01, -4.02185053e-01,\n",
       "       -7.08010316e-01,  7.19707131e-01, -6.40040815e-01, -3.56172621e-01,\n",
       "        6.06680334e-01,  7.74816573e-01, -7.36118436e-01, -4.47126180e-01,\n",
       "        4.02376801e-02, -6.86364055e-01,  2.88285434e-01,  2.14421272e-01,\n",
       "       -4.81394798e-01, -9.51110423e-01,  7.40530550e-01,  9.28563297e-01,\n",
       "        1.59917593e-01, -3.86909366e-01, -5.57639122e-01, -1.57299742e-01,\n",
       "       -2.53450394e-01,  7.66298115e-01,  7.25700915e-01, -5.26722558e-02,\n",
       "        4.22823429e-01, -4.83141750e-01, -7.36654103e-02, -2.71598518e-01,\n",
       "        2.12322325e-01,  5.62419713e-01, -3.76054704e-01, -1.32401466e-01,\n",
       "       -9.67185438e-01, -3.04849446e-01, -1.35320246e-01, -3.68716359e-01,\n",
       "       -5.14563560e-01, -6.20892107e-01, -5.71697831e-01, -3.99815530e-01,\n",
       "        1.85978636e-01,  5.12269378e-01,  4.00952011e-01, -3.20577532e-01,\n",
       "       -1.97420537e-01, -1.55722547e+00, -5.24125993e-01, -4.06745523e-01,\n",
       "       -4.88031268e-01,  2.44067222e-01,  1.13590561e-01, -3.55475061e-02,\n",
       "        4.02468652e-01, -6.37131929e-01,  8.35756660e-01,  4.04143274e-01,\n",
       "        6.41270638e-01, -7.00438917e-01, -1.38532147e-01, -4.34188008e-01,\n",
       "        8.99958014e-02,  8.98114443e-02,  1.78909153e-01,  4.16032150e-02,\n",
       "        1.30684882e-01,  5.43633938e-01,  1.61817476e-01, -3.23454440e-01,\n",
       "        4.83743489e-01,  3.85014206e-01,  7.64078200e-01, -8.34070265e-01,\n",
       "       -7.31380805e-02, -1.72004014e-01,  3.93362015e-01,  2.44497165e-01,\n",
       "       -8.41716826e-02, -4.31857347e-01, -1.02624607e+00,  1.33562103e-01,\n",
       "        3.45307350e-01,  1.86023563e-01,  1.26864433e+00, -3.96883458e-01,\n",
       "        7.20792174e-01, -5.74663222e-01,  7.64546022e-02,  7.88795650e-02,\n",
       "        4.09597963e-01,  2.75424123e-01,  8.58077705e-01, -2.31392756e-01,\n",
       "       -7.54590392e-01,  6.75302625e-01, -9.43760946e-02, -1.94428608e-01,\n",
       "        2.03639761e-01,  2.28710681e-01, -7.69647419e-01, -6.05415665e-02,\n",
       "       -9.27394629e-01,  2.02782303e-01,  7.57930726e-02, -4.03024465e-01,\n",
       "       -2.50724465e-01,  6.90565884e-01, -3.04469526e-01,  4.84250516e-01,\n",
       "       -1.05760860e+00, -1.22488654e+00, -2.68402249e-02,  4.93184805e-01,\n",
       "       -2.82254070e-02,  5.37834130e-02, -7.81420052e-01,  5.19675493e-01,\n",
       "        1.17235088e+00,  4.66050245e-02,  1.40038401e-01, -3.94704133e-01,\n",
       "       -4.01081860e-01, -4.00583684e-01, -3.29865098e-01, -3.89830768e-01,\n",
       "        8.50434154e-02, -1.00752473e+00, -6.19486213e-01, -1.16372514e+00,\n",
       "        2.22055420e-01,  2.31030881e-01, -5.49963355e-01, -7.19015181e-01,\n",
       "        2.97052830e-01,  2.51613021e-01, -1.85336620e-01, -1.48723334e-01,\n",
       "        5.02508938e-01,  1.38924524e-01,  3.66462201e-01, -2.06580862e-01,\n",
       "       -2.47316465e-01,  1.05458939e+00, -1.24157131e-01,  2.89026678e-01,\n",
       "       -6.68336272e-01, -2.57178426e-01, -1.54251158e-01,  4.66682196e-01,\n",
       "       -6.68329000e-01,  2.04154164e-01,  1.36733666e-01,  5.20241261e-01,\n",
       "       -1.22909002e-01, -2.04426169e-01, -3.31187725e-01,  2.44014516e-01,\n",
       "        6.06478274e-01, -2.26640433e-01,  1.73310012e-01, -1.15606976e+00,\n",
       "       -5.63660741e-01,  6.05199277e-01, -9.50557947e-01,  5.69976032e-01,\n",
       "       -8.86081636e-01, -5.27899384e-01, -7.94336438e-01, -6.86111927e-01,\n",
       "       -2.13280603e-01,  4.57363665e-01,  2.95508832e-01, -1.24294150e+00,\n",
       "        6.75271034e-01, -4.16885883e-01, -1.39369637e-01, -1.25983372e-01,\n",
       "       -1.08480237e-01, -4.27219748e-01,  6.90889597e-01,  3.16936076e-02,\n",
       "       -5.68816960e-01, -5.08291051e-02,  7.81225339e-02, -5.47761381e-01,\n",
       "       -1.06468332e+00, -8.24098587e-01, -4.01882939e-02,  4.05541211e-02,\n",
       "       -2.56478280e-01,  3.28599781e-01, -8.22212338e-01,  2.60284930e-01,\n",
       "        2.77458608e-01,  9.54266340e-02,  6.41827285e-03, -4.98912513e-01,\n",
       "       -2.92941034e-01, -7.07516909e-01, -1.52387798e-01,  4.57335353e-01,\n",
       "       -6.91832185e-01,  4.01380628e-01,  1.05152398e-01, -2.18253881e-01,\n",
       "        6.76671207e-01, -1.97770745e-01, -1.67895630e-02,  1.05948783e-01,\n",
       "       -7.11154699e-01, -3.23545158e-01,  3.90837789e-01,  4.15822804e-01,\n",
       "       -5.71506381e-01,  1.74523577e-01, -3.49796981e-01, -4.12660614e-02,\n",
       "       -5.92122018e-01,  9.35914099e-01,  1.17991328e-01,  5.62198877e-01,\n",
       "       -3.72251421e-01,  7.48511434e-01, -5.85672438e-01,  5.99284172e-01,\n",
       "       -4.19356227e-01,  8.60666484e-02,  7.13525772e-01, -8.62761676e-01,\n",
       "        2.57613420e-01,  4.63201553e-01, -3.24389279e-01, -3.87123764e-01,\n",
       "        1.26571029e-01, -4.87369031e-01, -1.44880638e-01,  3.96622986e-01,\n",
       "        9.33790952e-02, -7.66997695e-01,  1.52846172e-01,  2.18596354e-01,\n",
       "        1.03887403e+00, -4.50378895e-01, -4.82612282e-01,  5.47776401e-01,\n",
       "       -7.64465928e-02, -2.75664926e-02, -2.11762100e-01,  2.41730139e-01,\n",
       "       -2.00064063e-01, -1.00314021e+00,  5.30778289e-01, -8.17351460e-01,\n",
       "        2.32116021e-02,  1.94842666e-01,  2.77002752e-01, -1.59762949e-01,\n",
       "       -2.09863901e-01,  4.31712478e-01, -8.37902486e-01,  2.25760117e-02,\n",
       "       -1.50682747e-01, -1.69860974e-01, -4.66837734e-02, -4.68170047e-01,\n",
       "        2.99406558e-01,  2.24027243e-02,  1.15545079e-01,  4.00306344e-01,\n",
       "        9.41042542e-01,  3.78793418e-01, -2.34508932e-01, -3.30593348e-01,\n",
       "       -4.55935597e-01,  3.79623383e-01,  5.43193579e-01,  6.02502674e-02,\n",
       "        3.68198514e-01,  2.76881337e-01, -1.56597123e-01, -6.57819629e-01,\n",
       "        1.58714242e-02,  1.67995289e-01,  7.90993720e-02,  1.54479280e-01,\n",
       "        4.90975201e-01,  2.75600135e-01, -8.77219141e-01,  1.77820429e-01,\n",
       "        6.94640130e-02, -5.61373711e-01,  5.18324494e-01, -3.10646236e-01,\n",
       "        1.85657933e-01,  8.25930238e-02,  8.61015141e-01,  2.04245076e-01,\n",
       "        4.75537330e-01,  6.87890410e-01, -6.20347023e-01, -1.23703673e-01,\n",
       "        8.43648255e-01, -2.63560772e-01,  3.93254906e-01,  1.75094157e-01,\n",
       "       -1.26994765e+00,  1.00738490e+00,  3.38513888e-02, -3.47852141e-01,\n",
       "       -2.11264610e-01, -1.11965146e-02,  1.88761860e-01, -1.66786704e-02,\n",
       "        5.68395019e-01, -4.58167613e-01,  1.30343688e+00,  3.29140186e-01,\n",
       "        5.46640933e-01, -3.21959145e-02,  2.89696664e-01,  1.48958042e-01,\n",
       "        1.99962169e-01,  2.41495803e-01,  1.33437908e+00,  2.12382600e-01,\n",
       "       -1.36261851e-01, -4.51153129e-01,  7.00960457e-01, -2.55881101e-01,\n",
       "       -2.59274065e-01,  5.78921661e-02, -1.41588241e-01,  1.10462368e+00,\n",
       "        7.51242936e-02,  2.35104859e-01, -2.87890524e-01,  2.58328855e-01,\n",
       "       -1.03069174e+00,  1.06716323e+00, -4.04172629e-01,  4.79334109e-02,\n",
       "       -9.84776676e-01, -8.76864940e-02,  3.04778785e-01, -7.15065539e-01,\n",
       "        5.34995317e-01,  9.05357003e-01,  9.52498138e-01, -5.50198928e-02,\n",
       "       -1.20324075e-01, -4.17085588e-01,  1.07436335e+00,  5.77311218e-01,\n",
       "        7.36602366e-01,  4.23544079e-01, -2.73052782e-01,  3.25897098e-01,\n",
       "       -3.88508916e-01, -3.46491575e-01, -5.67286760e-02, -4.33420330e-01,\n",
       "       -5.11849448e-02, -4.95140135e-01,  2.81704932e-01,  7.30852544e-01,\n",
       "        2.33443588e-01, -2.26417929e-01,  1.03887282e-01,  9.29215550e-03,\n",
       "       -2.92512000e-01,  3.44494671e-01,  2.16403976e-01, -4.23743064e-03,\n",
       "        4.13833290e-01, -2.13304728e-01, -3.54139596e-01,  5.84301725e-03,\n",
       "        1.50868017e-02,  2.84027755e-01,  3.78560305e-01,  1.98405027e-01,\n",
       "       -7.75684237e-01,  1.83305398e-01, -4.69022781e-01,  1.06922388e+00,\n",
       "       -5.05056143e-01,  3.14204365e-01,  1.98299080e-01, -7.17022493e-02,\n",
       "        1.46759853e-01,  3.48679066e-01, -1.89368680e-01, -3.63281697e-01,\n",
       "       -3.87411833e-01, -3.29566956e-01, -5.20225525e-01,  1.04127240e+00,\n",
       "        6.45576119e-01, -1.19615388e+00, -3.86944525e-02,  8.02033186e-01,\n",
       "        3.74547184e-01, -2.84893364e-01, -2.37130344e-01,  1.45652071e-02,\n",
       "        6.64204806e-02,  4.17400926e-01,  1.79776847e-01,  1.18220806e-01,\n",
       "       -4.44052100e-01,  4.48320895e-01, -9.31756437e-01, -5.55737793e-01,\n",
       "        1.20350808e-01,  5.34013867e-01, -2.35694110e-01,  2.27902293e-01,\n",
       "       -8.65195990e-02, -3.15869749e-01, -1.51770145e-01, -6.60384774e-01,\n",
       "       -1.54838085e-01, -1.32897401e+00,  3.83683965e-02, -1.97420388e-01,\n",
       "        6.28133178e-01,  4.46448833e-01, -1.30691573e-01, -2.68271536e-01,\n",
       "        8.98969024e-02, -4.23212558e-01,  1.44195005e-01, -1.03101182e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_vector(\"the flower is beautiful\", \"flower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_vocab = {v:k for k, v in processor.full_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, ranksums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import permutation_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flowers vs. Insects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All borrowed from WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_words(wlist, filter_oov=True):\n",
    "    return [w.strip() for w in wlist.lower().replace(\"\\n\", \" \").split(\", \") if w.strip() in rev_vocab or not filter_oov]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words not in vocab are removed and target words are converted to adjectives when applicable and removed otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flower_words = to_words(\"\"\"aster, clover, hyacinth, marigold, poppy, azalea, crocus, iris, orchid, rose, bluebell, daffodil, lilac, pansy, tulip, buttercup, daisy, lily, peony, violet, carnation, gladiola,\n",
    "# magnolia, petunia, zinnia\"\"\")\n",
    "# insect_words = to_words(\"\"\"ant, caterpillar, flea, locust, spider, bedbug, centipede, fly, maggot, tarantula,\n",
    "# bee, cockroach, gnat, mosquito, termite, beetle, cricket, hornet, moth, wasp, blackfly,\n",
    "# dragonfly, horsefly, roach, weevil\"\"\")\n",
    "flower_single_words = [\"flower\"]\n",
    "flower_words = [\"flowers\"]\n",
    "insect_single_words = [\"bug\"]\n",
    "insect_words = [\"bugs\"]\n",
    "pleasant_words = to_words(\"\"\"caress, freedom, health, love, peace, cheer, friend, heaven, loyal, pleasure, diamond, gentle, honest, lucky, rainbow, diploma, gift, honor, miracle, sunrise, family,\n",
    "happy, laughter, paradise, vacation\"\"\", filter_oov=False)\n",
    "unpleasant_words = to_words(\"\"\"abuse, crash, filth, murder, sickness, accident, death, grief, poison, stink,\n",
    "assault, disaster, hatred, pollute, tragedy, divorce, jail, poverty, ugly, cancer, kill, rotten,\n",
    "vomit, agony, prison\"\"\", filter_oov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stimulus': 'beautiful',\n",
       " 'bias': 5.321797708296733,\n",
       " 'prior_correction': 1.11839522706574,\n",
       " 'bias_prior_corrected': 4.203402481230993}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_score(\"GGG are XXX.\", [flower_words, insect_words], \"beautiful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stimulus': 'pleasant',\n",
       " 'bias': 4.155168578779855,\n",
       " 'prior_correction': 1.11839522706574,\n",
       " 'bias_prior_corrected': 3.036773351714115}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_score(\"GGG are XXX.\", [flower_words, insect_words], \"pleasant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stimulus</th>\n",
       "      <th>bias</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caress</td>\n",
       "      <td>4.534071</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>1.698317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>freedom</td>\n",
       "      <td>1.931569</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-0.904185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>health</td>\n",
       "      <td>0.935261</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-1.900493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>love</td>\n",
       "      <td>2.969307</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>0.133553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>peace</td>\n",
       "      <td>2.585584</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-0.250170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cheer</td>\n",
       "      <td>1.589395</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-1.246359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>friend</td>\n",
       "      <td>2.620294</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-0.215460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>heaven</td>\n",
       "      <td>2.981361</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>0.145607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>loyal</td>\n",
       "      <td>-0.540773</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-3.376527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pleasure</td>\n",
       "      <td>2.312524</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-0.523230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>diamond</td>\n",
       "      <td>3.466475</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>0.630721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gentle</td>\n",
       "      <td>1.188142</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-1.647612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>honest</td>\n",
       "      <td>2.053278</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-0.782476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lucky</td>\n",
       "      <td>-1.306458</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-4.142212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rainbow</td>\n",
       "      <td>7.155796</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>4.320042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>diploma</td>\n",
       "      <td>3.613333</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>0.777579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gift</td>\n",
       "      <td>5.169295</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>2.333540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>honor</td>\n",
       "      <td>4.403491</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>1.567737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>miracle</td>\n",
       "      <td>3.007301</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>0.171547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sunrise</td>\n",
       "      <td>2.032387</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-0.803367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>family</td>\n",
       "      <td>2.124522</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-0.711232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>happy</td>\n",
       "      <td>-0.289853</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-3.125607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>laughter</td>\n",
       "      <td>0.341103</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-2.494651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>paradise</td>\n",
       "      <td>2.364365</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-0.471389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>vacation</td>\n",
       "      <td>1.336957</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-1.498797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caress</td>\n",
       "      <td>3.197661</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>2.537834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>freedom</td>\n",
       "      <td>1.191874</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>0.532047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>health</td>\n",
       "      <td>0.413345</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>-0.246482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>love</td>\n",
       "      <td>3.916658</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>3.256831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>peace</td>\n",
       "      <td>2.864187</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>2.204360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cheer</td>\n",
       "      <td>1.455994</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>0.796167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>friend</td>\n",
       "      <td>0.278083</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>-0.381744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>heaven</td>\n",
       "      <td>3.241119</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>2.581292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>loyal</td>\n",
       "      <td>0.096922</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>-0.562905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pleasure</td>\n",
       "      <td>3.010143</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>2.350316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>diamond</td>\n",
       "      <td>5.192859</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>4.533032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gentle</td>\n",
       "      <td>1.448969</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>0.789142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>honest</td>\n",
       "      <td>1.060471</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>0.400644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lucky</td>\n",
       "      <td>0.506522</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>-0.153305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rainbow</td>\n",
       "      <td>6.737400</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>6.077573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>diploma</td>\n",
       "      <td>1.498156</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>0.838329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gift</td>\n",
       "      <td>4.540969</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>3.881142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>honor</td>\n",
       "      <td>4.504522</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>3.844695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>miracle</td>\n",
       "      <td>2.502134</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>1.842307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sunrise</td>\n",
       "      <td>1.639029</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>0.979202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>family</td>\n",
       "      <td>0.361932</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>-0.297895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>happy</td>\n",
       "      <td>1.273915</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>0.614088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>laughter</td>\n",
       "      <td>1.051203</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>0.391376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>paradise</td>\n",
       "      <td>3.119540</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>2.459713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>vacation</td>\n",
       "      <td>-1.242822</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>-1.902649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    stimulus      bias  prior_correction  bias_prior_corrected\n",
       "0     caress  4.534071          2.835754              1.698317\n",
       "1    freedom  1.931569          2.835754             -0.904185\n",
       "2     health  0.935261          2.835754             -1.900493\n",
       "3       love  2.969307          2.835754              0.133553\n",
       "4      peace  2.585584          2.835754             -0.250170\n",
       "5      cheer  1.589395          2.835754             -1.246359\n",
       "6     friend  2.620294          2.835754             -0.215460\n",
       "7     heaven  2.981361          2.835754              0.145607\n",
       "8      loyal -0.540773          2.835754             -3.376527\n",
       "9   pleasure  2.312524          2.835754             -0.523230\n",
       "10   diamond  3.466475          2.835754              0.630721\n",
       "11    gentle  1.188142          2.835754             -1.647612\n",
       "12    honest  2.053278          2.835754             -0.782476\n",
       "13     lucky -1.306458          2.835754             -4.142212\n",
       "14   rainbow  7.155796          2.835754              4.320042\n",
       "15   diploma  3.613333          2.835754              0.777579\n",
       "16      gift  5.169295          2.835754              2.333540\n",
       "17     honor  4.403491          2.835754              1.567737\n",
       "18   miracle  3.007301          2.835754              0.171547\n",
       "19   sunrise  2.032387          2.835754             -0.803367\n",
       "20    family  2.124522          2.835754             -0.711232\n",
       "21     happy -0.289853          2.835754             -3.125607\n",
       "22  laughter  0.341103          2.835754             -2.494651\n",
       "23  paradise  2.364365          2.835754             -0.471389\n",
       "24  vacation  1.336957          2.835754             -1.498797\n",
       "0     caress  3.197661          0.659827              2.537834\n",
       "1    freedom  1.191874          0.659827              0.532047\n",
       "2     health  0.413345          0.659827             -0.246482\n",
       "3       love  3.916658          0.659827              3.256831\n",
       "4      peace  2.864187          0.659827              2.204360\n",
       "5      cheer  1.455994          0.659827              0.796167\n",
       "6     friend  0.278083          0.659827             -0.381744\n",
       "7     heaven  3.241119          0.659827              2.581292\n",
       "8      loyal  0.096922          0.659827             -0.562905\n",
       "9   pleasure  3.010143          0.659827              2.350316\n",
       "10   diamond  5.192859          0.659827              4.533032\n",
       "11    gentle  1.448969          0.659827              0.789142\n",
       "12    honest  1.060471          0.659827              0.400644\n",
       "13     lucky  0.506522          0.659827             -0.153305\n",
       "14   rainbow  6.737400          0.659827              6.077573\n",
       "15   diploma  1.498156          0.659827              0.838329\n",
       "16      gift  4.540969          0.659827              3.881142\n",
       "17     honor  4.504522          0.659827              3.844695\n",
       "18   miracle  2.502134          0.659827              1.842307\n",
       "19   sunrise  1.639029          0.659827              0.979202\n",
       "20    family  0.361932          0.659827             -0.297895\n",
       "21     happy  1.273915          0.659827              0.614088\n",
       "22  laughter  1.051203          0.659827              0.391376\n",
       "23  paradise  3.119540          0.659827              2.459713\n",
       "24  vacation -1.242822          0.659827             -1.902649"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.concat([\n",
    "pd.DataFrame([bias_score(\"the GGG is XXX.\", \n",
    "                         [flower_words, insect_words], w) for w in pleasant_words]),\n",
    "pd.DataFrame([bias_score(\"GGG are XXX.\", \n",
    "                         [flower_single_words, insect_single_words], w) for w in pleasant_words]),\n",
    "])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5009996429862796"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"bias_prior_corrected\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stimulus</th>\n",
       "      <th>bias</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abuse</td>\n",
       "      <td>0.314224</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-2.521530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crash</td>\n",
       "      <td>0.594513</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-2.241241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filth</td>\n",
       "      <td>0.027893</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-2.807861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>murder</td>\n",
       "      <td>-0.106989</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-2.942743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sickness</td>\n",
       "      <td>-0.446693</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-3.282447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>accident</td>\n",
       "      <td>0.296810</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-2.538944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>death</td>\n",
       "      <td>0.649061</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-2.186693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>grief</td>\n",
       "      <td>1.743163</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-1.092591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>poison</td>\n",
       "      <td>0.930887</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-1.904867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stink</td>\n",
       "      <td>-0.206610</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-3.042364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>assault</td>\n",
       "      <td>0.033612</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-2.802142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>disaster</td>\n",
       "      <td>0.044311</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-2.791443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hatred</td>\n",
       "      <td>0.324913</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-2.510841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pollute</td>\n",
       "      <td>5.991964</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>3.156210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tragedy</td>\n",
       "      <td>1.680378</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-1.155376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>divorce</td>\n",
       "      <td>2.047873</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-0.787881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>jail</td>\n",
       "      <td>-0.699906</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-3.535660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>poverty</td>\n",
       "      <td>-1.945633</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-4.781387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ugly</td>\n",
       "      <td>1.830495</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-1.005259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cancer</td>\n",
       "      <td>0.102904</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-2.732850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>kill</td>\n",
       "      <td>0.330104</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-2.505650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rotten</td>\n",
       "      <td>2.289231</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-0.546523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>vomit</td>\n",
       "      <td>0.527845</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-2.307909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>agony</td>\n",
       "      <td>1.575409</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-1.260345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>prison</td>\n",
       "      <td>-0.538115</td>\n",
       "      <td>2.835754</td>\n",
       "      <td>-3.373870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abuse</td>\n",
       "      <td>0.447292</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>-0.212535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crash</td>\n",
       "      <td>-1.197243</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>-1.857070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filth</td>\n",
       "      <td>1.587515</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>0.927688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>murder</td>\n",
       "      <td>0.329298</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>-0.330529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sickness</td>\n",
       "      <td>-0.587581</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>-1.247408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>accident</td>\n",
       "      <td>-1.119427</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>-1.779254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>death</td>\n",
       "      <td>2.103615</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>1.443788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>grief</td>\n",
       "      <td>1.998107</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>1.338280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>poison</td>\n",
       "      <td>1.807838</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>1.148011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stink</td>\n",
       "      <td>-0.696866</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>-1.356693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>assault</td>\n",
       "      <td>-1.669645</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>-2.329472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>disaster</td>\n",
       "      <td>1.049758</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>0.389931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hatred</td>\n",
       "      <td>1.717679</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>1.057852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pollute</td>\n",
       "      <td>4.139421</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>3.479594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tragedy</td>\n",
       "      <td>1.008454</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>0.348627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>divorce</td>\n",
       "      <td>0.278176</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>-0.381651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>jail</td>\n",
       "      <td>-1.504254</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>-2.164081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>poverty</td>\n",
       "      <td>-0.889814</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>-1.549641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ugly</td>\n",
       "      <td>1.439071</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>0.779244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cancer</td>\n",
       "      <td>-0.082583</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>-0.742410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>kill</td>\n",
       "      <td>0.425656</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>-0.234171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rotten</td>\n",
       "      <td>1.582448</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>0.922621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>vomit</td>\n",
       "      <td>0.905868</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>0.246041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>agony</td>\n",
       "      <td>1.881909</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>1.222082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>prison</td>\n",
       "      <td>0.003126</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>-0.656701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    stimulus      bias  prior_correction  bias_prior_corrected\n",
       "0      abuse  0.314224          2.835754             -2.521530\n",
       "1      crash  0.594513          2.835754             -2.241241\n",
       "2      filth  0.027893          2.835754             -2.807861\n",
       "3     murder -0.106989          2.835754             -2.942743\n",
       "4   sickness -0.446693          2.835754             -3.282447\n",
       "5   accident  0.296810          2.835754             -2.538944\n",
       "6      death  0.649061          2.835754             -2.186693\n",
       "7      grief  1.743163          2.835754             -1.092591\n",
       "8     poison  0.930887          2.835754             -1.904867\n",
       "9      stink -0.206610          2.835754             -3.042364\n",
       "10   assault  0.033612          2.835754             -2.802142\n",
       "11  disaster  0.044311          2.835754             -2.791443\n",
       "12    hatred  0.324913          2.835754             -2.510841\n",
       "13   pollute  5.991964          2.835754              3.156210\n",
       "14   tragedy  1.680378          2.835754             -1.155376\n",
       "15   divorce  2.047873          2.835754             -0.787881\n",
       "16      jail -0.699906          2.835754             -3.535660\n",
       "17   poverty -1.945633          2.835754             -4.781387\n",
       "18      ugly  1.830495          2.835754             -1.005259\n",
       "19    cancer  0.102904          2.835754             -2.732850\n",
       "20      kill  0.330104          2.835754             -2.505650\n",
       "21    rotten  2.289231          2.835754             -0.546523\n",
       "22     vomit  0.527845          2.835754             -2.307909\n",
       "23     agony  1.575409          2.835754             -1.260345\n",
       "24    prison -0.538115          2.835754             -3.373870\n",
       "0      abuse  0.447292          0.659827             -0.212535\n",
       "1      crash -1.197243          0.659827             -1.857070\n",
       "2      filth  1.587515          0.659827              0.927688\n",
       "3     murder  0.329298          0.659827             -0.330529\n",
       "4   sickness -0.587581          0.659827             -1.247408\n",
       "5   accident -1.119427          0.659827             -1.779254\n",
       "6      death  2.103615          0.659827              1.443788\n",
       "7      grief  1.998107          0.659827              1.338280\n",
       "8     poison  1.807838          0.659827              1.148011\n",
       "9      stink -0.696866          0.659827             -1.356693\n",
       "10   assault -1.669645          0.659827             -2.329472\n",
       "11  disaster  1.049758          0.659827              0.389931\n",
       "12    hatred  1.717679          0.659827              1.057852\n",
       "13   pollute  4.139421          0.659827              3.479594\n",
       "14   tragedy  1.008454          0.659827              0.348627\n",
       "15   divorce  0.278176          0.659827             -0.381651\n",
       "16      jail -1.504254          0.659827             -2.164081\n",
       "17   poverty -0.889814          0.659827             -1.549641\n",
       "18      ugly  1.439071          0.659827              0.779244\n",
       "19    cancer -0.082583          0.659827             -0.742410\n",
       "20      kill  0.425656          0.659827             -0.234171\n",
       "21    rotten  1.582448          0.659827              0.922621\n",
       "22     vomit  0.905868          0.659827              0.246041\n",
       "23     agony  1.881909          0.659827              1.222082\n",
       "24    prison  0.003126          0.659827             -0.656701"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.concat([\n",
    "pd.DataFrame([bias_score(\"the GGG is XXX.\", \n",
    "                         [flower_words, insect_words], w) for w in unpleasant_words]),\n",
    "pd.DataFrame([bias_score(\"GGG are XXX.\", \n",
    "                         [flower_single_words, insect_single_words], w) for w in unpleasant_words]),\n",
    "])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.1008013812648163"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[\"bias_prior_corrected\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical test (is the t-test appropriate here?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7673179818177706"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_effect_size(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=4.137087135658693, pvalue=7.439289810174356e-05)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_ind(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RanksumsResult(statistic=3.846751490505069, pvalue=0.00011969426356751705)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranksums(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7e-05"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvs1 = [\n",
    "    get_word_vector(f\"[MASK] are {x}\", x) for x in pleasant_words\n",
    "]\n",
    "wvs2 = [\n",
    "    get_word_vector(f\"[MASK] are {x}\", x) for x in unpleasant_words\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.13553414"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_insect = get_word_vector(\"insects are [MASK]\", \"insects\")\n",
    "sims_insect1 = [cosine_similarity(wv_insect, wv) for wv in wvs1]\n",
    "sims_insect2 = [cosine_similarity(wv_insect, wv) for wv in wvs2]\n",
    "mean_diff = np.mean(sims_insect1) - np.mean(sims_insect2)\n",
    "std_ = np.std(sims_insect1 + sims_insect2)\n",
    "effect_sz_insect = mean_diff / std_; effect_sz_insect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19293566"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_flower = get_word_vector(\"flowers are [MASK]\", \"flowers\")\n",
    "sims_flower1 = [cosine_similarity(wv_flower, wv) for wv in wvs1]\n",
    "sims_flower2 = [cosine_similarity(wv_flower, wv) for wv in wvs2]\n",
    "mean_diff = np.mean(sims_flower1) - np.mean(sims_flower2)\n",
    "std_ = np.std(sims_flower1 + sims_flower2)\n",
    "effect_sz_flower = mean_diff / std_; effect_sz_flower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(sims_insect1, sims_flower1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3e-05"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(sims_insect2, sims_flower2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Career vs Family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_words = to_words(\"he\")\n",
    "female_words = to_words(\"she\")\n",
    "# male_words = to_words(\"John, Paul, Mike, Kevin, Steve, Greg, Jeff, Bill\".lower())\n",
    "# female_words = to_words(\"Amy, Joan, Lisa, Sarah, Diana, Kate, Ann, Donna\".lower())\n",
    "male_plural_words = to_words(\"boys, men\")\n",
    "female_plural_words = to_words(\"girls, women\")\n",
    "career_words = to_words(\"executive, management, professional, corporation, salary, office, business, career\")\n",
    "family_words = to_words(\"home, parents, children, family, cousins, marriage, wedding, relatives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stimulus</th>\n",
       "      <th>bias</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>executive</td>\n",
       "      <td>0.599076</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>0.195088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>management</td>\n",
       "      <td>0.705400</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>0.301412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>professional</td>\n",
       "      <td>0.655196</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>0.251208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>corporation</td>\n",
       "      <td>1.832331</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>1.428343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>salary</td>\n",
       "      <td>1.705612</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>1.301624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>office</td>\n",
       "      <td>0.620151</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>0.216163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>business</td>\n",
       "      <td>0.630227</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>0.226239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>career</td>\n",
       "      <td>1.301030</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>0.897042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>executive</td>\n",
       "      <td>0.205277</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.571826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>management</td>\n",
       "      <td>0.220116</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.586665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>professional</td>\n",
       "      <td>-0.496713</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>-0.130164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>corporation</td>\n",
       "      <td>1.265400</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>1.631948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>salary</td>\n",
       "      <td>0.609615</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.976164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>office</td>\n",
       "      <td>0.070766</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.437315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>business</td>\n",
       "      <td>-0.002690</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.363859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>career</td>\n",
       "      <td>0.296032</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.662581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>executive</td>\n",
       "      <td>0.859385</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>0.376974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>management</td>\n",
       "      <td>0.788204</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>0.305794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>professional</td>\n",
       "      <td>0.503229</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>0.020819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>corporation</td>\n",
       "      <td>1.359951</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>0.877540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>salary</td>\n",
       "      <td>0.654332</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>0.171921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>office</td>\n",
       "      <td>0.623053</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>0.140642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>business</td>\n",
       "      <td>0.746951</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>0.264540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>career</td>\n",
       "      <td>0.908055</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>0.425644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       stimulus      bias  prior_correction  bias_prior_corrected\n",
       "0     executive  0.599076          0.403988              0.195088\n",
       "1    management  0.705400          0.403988              0.301412\n",
       "2  professional  0.655196          0.403988              0.251208\n",
       "3   corporation  1.832331          0.403988              1.428343\n",
       "4        salary  1.705612          0.403988              1.301624\n",
       "5        office  0.620151          0.403988              0.216163\n",
       "6      business  0.630227          0.403988              0.226239\n",
       "7        career  1.301030          0.403988              0.897042\n",
       "0     executive  0.205277         -0.366549              0.571826\n",
       "1    management  0.220116         -0.366549              0.586665\n",
       "2  professional -0.496713         -0.366549             -0.130164\n",
       "3   corporation  1.265400         -0.366549              1.631948\n",
       "4        salary  0.609615         -0.366549              0.976164\n",
       "5        office  0.070766         -0.366549              0.437315\n",
       "6      business -0.002690         -0.366549              0.363859\n",
       "7        career  0.296032         -0.366549              0.662581\n",
       "0     executive  0.859385          0.482411              0.376974\n",
       "1    management  0.788204          0.482411              0.305794\n",
       "2  professional  0.503229          0.482411              0.020819\n",
       "3   corporation  1.359951          0.482411              0.877540\n",
       "4        salary  0.654332          0.482411              0.171921\n",
       "5        office  0.623053          0.482411              0.140642\n",
       "6      business  0.746951          0.482411              0.264540\n",
       "7        career  0.908055          0.482411              0.425644"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.concat([\n",
    "    pd.DataFrame([bias_score(\"GGG likes XXX.\", [male_words, female_words], w) for w in career_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG like XXX.\", [male_plural_words, female_plural_words], w) for w in career_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG is interested in XXX.\", [[\"he\"], ['she']], w) for w in career_words]), \n",
    "])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5208827496935904"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"bias_prior_corrected\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stimulus</th>\n",
       "      <th>bias</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>home</td>\n",
       "      <td>-0.262433</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>-0.666421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>parents</td>\n",
       "      <td>-0.137539</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>-0.541527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>children</td>\n",
       "      <td>-0.000793</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>-0.404781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>family</td>\n",
       "      <td>0.536997</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>0.133009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cousins</td>\n",
       "      <td>0.335159</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>-0.068829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>marriage</td>\n",
       "      <td>0.090117</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>-0.313871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wedding</td>\n",
       "      <td>0.150402</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>-0.253586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>relatives</td>\n",
       "      <td>0.260327</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>-0.143661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>home</td>\n",
       "      <td>-0.204501</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.162048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>parents</td>\n",
       "      <td>-0.628229</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>-0.261680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>children</td>\n",
       "      <td>-0.209043</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.157506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>family</td>\n",
       "      <td>-0.523117</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>-0.156568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cousins</td>\n",
       "      <td>-0.269898</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.096650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>marriage</td>\n",
       "      <td>-0.118060</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.248488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wedding</td>\n",
       "      <td>-1.448022</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>-1.081473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>relatives</td>\n",
       "      <td>-0.256575</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.109974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>home</td>\n",
       "      <td>0.271714</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>-0.210696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>parents</td>\n",
       "      <td>0.084234</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>-0.398176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>children</td>\n",
       "      <td>-0.201239</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>-0.683650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>family</td>\n",
       "      <td>0.408174</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>-0.074236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cousins</td>\n",
       "      <td>0.335865</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>-0.146546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>marriage</td>\n",
       "      <td>-0.130144</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>-0.612555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wedding</td>\n",
       "      <td>-0.095080</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>-0.577491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>relatives</td>\n",
       "      <td>0.063974</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>-0.418436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    stimulus      bias  prior_correction  bias_prior_corrected\n",
       "0       home -0.262433          0.403988             -0.666421\n",
       "1    parents -0.137539          0.403988             -0.541527\n",
       "2   children -0.000793          0.403988             -0.404781\n",
       "3     family  0.536997          0.403988              0.133009\n",
       "4    cousins  0.335159          0.403988             -0.068829\n",
       "5   marriage  0.090117          0.403988             -0.313871\n",
       "6    wedding  0.150402          0.403988             -0.253586\n",
       "7  relatives  0.260327          0.403988             -0.143661\n",
       "0       home -0.204501         -0.366549              0.162048\n",
       "1    parents -0.628229         -0.366549             -0.261680\n",
       "2   children -0.209043         -0.366549              0.157506\n",
       "3     family -0.523117         -0.366549             -0.156568\n",
       "4    cousins -0.269898         -0.366549              0.096650\n",
       "5   marriage -0.118060         -0.366549              0.248488\n",
       "6    wedding -1.448022         -0.366549             -1.081473\n",
       "7  relatives -0.256575         -0.366549              0.109974\n",
       "0       home  0.271714          0.482411             -0.210696\n",
       "1    parents  0.084234          0.482411             -0.398176\n",
       "2   children -0.201239          0.482411             -0.683650\n",
       "3     family  0.408174          0.482411             -0.074236\n",
       "4    cousins  0.335865          0.482411             -0.146546\n",
       "5   marriage -0.130144          0.482411             -0.612555\n",
       "6    wedding -0.095080          0.482411             -0.577491\n",
       "7  relatives  0.063974          0.482411             -0.418436"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.concat([\n",
    "    pd.DataFrame([bias_score(\"GGG likes XXX.\", [male_words, female_words], w) for w in family_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG like XXX.\", [male_plural_words, female_plural_words], w) for w in family_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG is interested in XXX.\", [[\"he\"], ['she']], w) for w in family_words]), \n",
    "])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2544378493797377"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[\"bias_prior_corrected\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for statistical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3989820106873352"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_effect_size(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=6.778235512032725, pvalue=1.9612859665338685e-08)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_ind(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RanksumsResult(statistic=5.505447209772503, pvalue=3.6823229420196424e-08)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranksums(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvs1 = [\n",
    "    get_word_vector(f\"[MASK] like {x}\", x) for x in family_words\n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] likes {x}\", x) for x in family_words\n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] is interested in {x}\", x) for x in family_words\n",
    "]\n",
    "wvs2 = [\n",
    "    get_word_vector(f\"[MASK] like {x}\", x) for x in career_words\n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] likes {x}\", x) for x in career_words    \n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] is interested in {x}\", x) for x in career_words\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5870323"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_fm = get_word_vector(\"women like [MASK]\", \"women\")\n",
    "wv_fm2 = get_word_vector(\"she likes [MASK]\", \"she\")\n",
    "sims_fm1 = [cosine_similarity(wv_fm, wv) for wv in wvs1] +\\\n",
    "           [cosine_similarity(wv_fm, wv) for wv in wvs1]\n",
    "sims_fm2 = [cosine_similarity(wv_fm, wv) for wv in wvs2] +\\\n",
    "           [cosine_similarity(wv_fm2, wv) for wv in wvs2]\n",
    "mean_diff = np.mean(sims_fm1) - np.mean(sims_fm2)\n",
    "std_ = np.std(sims_fm1 + sims_fm1)\n",
    "effect_sz_fm_family_career = mean_diff / std_; effect_sz_fm_family_career"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24955627"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_m = get_word_vector(\"men like [MASK]\", \"men\")\n",
    "wv_m2 = get_word_vector(\"he likes [MASK]\", \"he\")\n",
    "sims_m1 = [cosine_similarity(wv_m, wv) for wv in wvs1]+\\\n",
    "           [cosine_similarity(wv_m2, wv) for wv in wvs1]\n",
    "sims_m2 = [cosine_similarity(wv_m, wv) for wv in wvs2] +\\\n",
    "           [cosine_similarity(wv_m2, wv) for wv in wvs2]\n",
    "mean_diff = np.mean(sims_m1) - np.mean(sims_m2)\n",
    "std_ = np.std(sims_m1 + sims_m1)\n",
    "effect_sz_m_family_career = mean_diff / std_; effect_sz_m_family_career"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(sims_fm1, sims_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92718"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(sims_fm2, sims_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math vs. Art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_words = to_words(\"math, algebra, geometry, calculus, equations, computation, numbers, addition\")\n",
    "art_words = to_words(\"poetry, art, dance, Shakespear, literature, novels, symphony, drama, sculptures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stimulus</th>\n",
       "      <th>bias</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>math</td>\n",
       "      <td>0.272837</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>-0.131151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>algebra</td>\n",
       "      <td>0.495616</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>0.091628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>geometry</td>\n",
       "      <td>0.376719</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>-0.027269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>calculus</td>\n",
       "      <td>0.359230</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>-0.044758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>equations</td>\n",
       "      <td>1.007015</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>0.603027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>computation</td>\n",
       "      <td>1.119367</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>0.715379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>numbers</td>\n",
       "      <td>0.606420</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>0.202432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>addition</td>\n",
       "      <td>0.726641</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>0.322653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>math</td>\n",
       "      <td>-0.253876</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.112673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>algebra</td>\n",
       "      <td>-0.523834</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>-0.157285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>geometry</td>\n",
       "      <td>-0.390305</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>-0.023756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>calculus</td>\n",
       "      <td>-0.975975</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>-0.609426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>equations</td>\n",
       "      <td>0.257565</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.624114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>computation</td>\n",
       "      <td>0.318910</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.685459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>numbers</td>\n",
       "      <td>0.302348</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.668897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>addition</td>\n",
       "      <td>-0.230889</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.135660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>math</td>\n",
       "      <td>0.498302</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>0.015891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>algebra</td>\n",
       "      <td>1.411588</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>0.929177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>geometry</td>\n",
       "      <td>1.123701</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>0.641290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>calculus</td>\n",
       "      <td>1.074119</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>0.591708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>equations</td>\n",
       "      <td>1.690855</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>1.208444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>computation</td>\n",
       "      <td>1.465523</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>0.983112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>numbers</td>\n",
       "      <td>1.123957</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>0.641546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>addition</td>\n",
       "      <td>0.787366</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>0.304955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      stimulus      bias  prior_correction  bias_prior_corrected\n",
       "0         math  0.272837          0.403988             -0.131151\n",
       "1      algebra  0.495616          0.403988              0.091628\n",
       "2     geometry  0.376719          0.403988             -0.027269\n",
       "3     calculus  0.359230          0.403988             -0.044758\n",
       "4    equations  1.007015          0.403988              0.603027\n",
       "5  computation  1.119367          0.403988              0.715379\n",
       "6      numbers  0.606420          0.403988              0.202432\n",
       "7     addition  0.726641          0.403988              0.322653\n",
       "0         math -0.253876         -0.366549              0.112673\n",
       "1      algebra -0.523834         -0.366549             -0.157285\n",
       "2     geometry -0.390305         -0.366549             -0.023756\n",
       "3     calculus -0.975975         -0.366549             -0.609426\n",
       "4    equations  0.257565         -0.366549              0.624114\n",
       "5  computation  0.318910         -0.366549              0.685459\n",
       "6      numbers  0.302348         -0.366549              0.668897\n",
       "7     addition -0.230889         -0.366549              0.135660\n",
       "0         math  0.498302          0.482411              0.015891\n",
       "1      algebra  1.411588          0.482411              0.929177\n",
       "2     geometry  1.123701          0.482411              0.641290\n",
       "3     calculus  1.074119          0.482411              0.591708\n",
       "4    equations  1.690855          0.482411              1.208444\n",
       "5  computation  1.465523          0.482411              0.983112\n",
       "6      numbers  1.123957          0.482411              0.641546\n",
       "7     addition  0.787366          0.482411              0.304955"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.concat([\n",
    "    pd.DataFrame([bias_score(\"GGG likes XXX.\", [male_words, female_words], w) for w in math_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG like XXX.\", [male_plural_words, female_plural_words], w) for w in math_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG is interested in XXX.\", [[\"he\"], ['she']], w) for w in math_words]), \n",
    "])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5268000033417001"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"bias\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stimulus</th>\n",
       "      <th>bias</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>poetry</td>\n",
       "      <td>0.354761</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>-0.049227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>art</td>\n",
       "      <td>-0.062774</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>-0.466762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dance</td>\n",
       "      <td>0.088699</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>-0.315289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>literature</td>\n",
       "      <td>0.626059</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>0.222071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>novels</td>\n",
       "      <td>0.306324</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>-0.097664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>symphony</td>\n",
       "      <td>0.970467</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>0.566479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>drama</td>\n",
       "      <td>0.195011</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>-0.208977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sculptures</td>\n",
       "      <td>0.375476</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>-0.028512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>poetry</td>\n",
       "      <td>-0.322518</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.044031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>art</td>\n",
       "      <td>-0.480286</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>-0.113737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dance</td>\n",
       "      <td>-1.050046</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>-0.683497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>literature</td>\n",
       "      <td>-0.247653</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.118896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>novels</td>\n",
       "      <td>-0.250754</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.115794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>symphony</td>\n",
       "      <td>-0.487094</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>-0.120545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>drama</td>\n",
       "      <td>-1.304311</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>-0.937762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sculptures</td>\n",
       "      <td>-0.013457</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.353092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>poetry</td>\n",
       "      <td>0.098519</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>-0.383891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>art</td>\n",
       "      <td>0.172856</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>-0.309554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dance</td>\n",
       "      <td>-0.046586</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>-0.528997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>literature</td>\n",
       "      <td>0.363983</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>-0.118428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>novels</td>\n",
       "      <td>0.089295</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>-0.393115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>symphony</td>\n",
       "      <td>1.059014</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>0.576604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>drama</td>\n",
       "      <td>-0.165875</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>-0.648286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sculptures</td>\n",
       "      <td>0.132909</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>-0.349502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     stimulus      bias  prior_correction  bias_prior_corrected\n",
       "0      poetry  0.354761          0.403988             -0.049227\n",
       "1         art -0.062774          0.403988             -0.466762\n",
       "2       dance  0.088699          0.403988             -0.315289\n",
       "3  literature  0.626059          0.403988              0.222071\n",
       "4      novels  0.306324          0.403988             -0.097664\n",
       "5    symphony  0.970467          0.403988              0.566479\n",
       "6       drama  0.195011          0.403988             -0.208977\n",
       "7  sculptures  0.375476          0.403988             -0.028512\n",
       "0      poetry -0.322518         -0.366549              0.044031\n",
       "1         art -0.480286         -0.366549             -0.113737\n",
       "2       dance -1.050046         -0.366549             -0.683497\n",
       "3  literature -0.247653         -0.366549              0.118896\n",
       "4      novels -0.250754         -0.366549              0.115794\n",
       "5    symphony -0.487094         -0.366549             -0.120545\n",
       "6       drama -1.304311         -0.366549             -0.937762\n",
       "7  sculptures -0.013457         -0.366549              0.353092\n",
       "0      poetry  0.098519          0.482411             -0.383891\n",
       "1         art  0.172856          0.482411             -0.309554\n",
       "2       dance -0.046586          0.482411             -0.528997\n",
       "3  literature  0.363983          0.482411             -0.118428\n",
       "4      novels  0.089295          0.482411             -0.393115\n",
       "5    symphony  1.059014          0.482411              0.576604\n",
       "6       drama -0.165875          0.482411             -0.648286\n",
       "7  sculptures  0.132909          0.482411             -0.349502"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.concat([\n",
    "    pd.DataFrame([bias_score(\"GGG likes XXX.\", [male_words, female_words], w) for w in art_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG like XXX.\", [male_plural_words, female_plural_words], w) for w in art_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG is interested in XXX.\", [[\"he\"], ['she']], w) for w in art_words]),  \n",
    "])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016750827809628565"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[\"bias\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0683379030179307"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_effect_size(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=4.349416231070443, pvalue=7.503390606457258e-05)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_ind(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RanksumsResult(statistic=3.8352553596168, pvalue=0.00012543390384381966)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranksums(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8e-05"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvs1 = [\n",
    "    get_word_vector(f\"[MASK] like {x}\", x) for x in art_words\n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] likes {x}\", x) for x in art_words\n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] is interested in {x}\", x) for x in art_words\n",
    "]\n",
    "wvs2 = [\n",
    "    get_word_vector(f\"[MASK] like {x}\", x) for x in math_words\n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] likes {x}\", x) for x in math_words    \n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] is interested in {x}\", x) for x in math_words\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0761006"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims_fm1 = [cosine_similarity(wv_fm, wv) for wv in wvs1] +\\\n",
    "           [cosine_similarity(wv_fm2, wv) for wv in wvs1]\n",
    "sims_fm2 = [cosine_similarity(wv_fm, wv) for wv in wvs2] +\\\n",
    "           [cosine_similarity(wv_fm2, wv) for wv in wvs2]\n",
    "mean_diff = np.mean(sims_fm1) - np.mean(sims_fm2)\n",
    "std_ = np.std(sims_fm1 + sims_fm1)\n",
    "effect_sz_fm_art_math = mean_diff / std_; effect_sz_fm_art_math\n",
    "\n",
    "sims_m1 = [cosine_similarity(wv_m, wv) for wv in wvs1] +\\\n",
    "           [cosine_similarity(wv_m2, wv) for wv in wvs1]\n",
    "sims_m2 = [cosine_similarity(wv_m, wv) for wv in wvs2] +\\\n",
    "           [cosine_similarity(wv_m2, wv) for wv in wvs2]\n",
    "mean_diff = np.mean(sims_fm1) - np.mean(sims_fm2)\n",
    "std_ = np.std(sims_fm1 + sims_fm1)\n",
    "effect_sz_m_art_math = mean_diff / std_; effect_sz_m_art_math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70493"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(sims_fm1, sims_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96269"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(sims_fm2, sims_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Science vs. Art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "science_words = to_words(\"science, technology, physics, chemistry, Einstein, NASA, experiments, astronomy\")\n",
    "art_words = to_words(\"poetry, art, dance, Shakespear, literature, novels, symphony, drama, sculptures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stimulus</th>\n",
       "      <th>bias</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>science</td>\n",
       "      <td>0.631547</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>0.227559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>technology</td>\n",
       "      <td>0.876078</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>0.472090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>physics</td>\n",
       "      <td>0.546069</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>0.142081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chemistry</td>\n",
       "      <td>0.127345</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>-0.276643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>einstein</td>\n",
       "      <td>0.277876</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>-0.126112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nasa</td>\n",
       "      <td>1.020048</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>0.616060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>experiments</td>\n",
       "      <td>0.982220</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>0.578232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>astronomy</td>\n",
       "      <td>0.386327</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>-0.017661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>science</td>\n",
       "      <td>-0.023502</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.343047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>technology</td>\n",
       "      <td>0.271393</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.637942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>physics</td>\n",
       "      <td>-0.186839</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.179710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chemistry</td>\n",
       "      <td>-0.654191</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>-0.287642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>einstein</td>\n",
       "      <td>1.031780</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>1.398329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nasa</td>\n",
       "      <td>0.540943</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.907492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>experiments</td>\n",
       "      <td>0.173073</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.539622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>astronomy</td>\n",
       "      <td>-0.642304</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>-0.275755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>science</td>\n",
       "      <td>0.998535</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>0.516124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>technology</td>\n",
       "      <td>0.996838</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>0.514427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>physics</td>\n",
       "      <td>1.260713</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>0.778302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chemistry</td>\n",
       "      <td>1.120332</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>0.637921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>einstein</td>\n",
       "      <td>0.716209</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>0.233798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nasa</td>\n",
       "      <td>1.124194</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>0.641784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>experiments</td>\n",
       "      <td>1.086175</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>0.603764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>astronomy</td>\n",
       "      <td>1.415930</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>0.933519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      stimulus      bias  prior_correction  bias_prior_corrected\n",
       "0      science  0.631547          0.403988              0.227559\n",
       "1   technology  0.876078          0.403988              0.472090\n",
       "2      physics  0.546069          0.403988              0.142081\n",
       "3    chemistry  0.127345          0.403988             -0.276643\n",
       "4     einstein  0.277876          0.403988             -0.126112\n",
       "5         nasa  1.020048          0.403988              0.616060\n",
       "6  experiments  0.982220          0.403988              0.578232\n",
       "7    astronomy  0.386327          0.403988             -0.017661\n",
       "0      science -0.023502         -0.366549              0.343047\n",
       "1   technology  0.271393         -0.366549              0.637942\n",
       "2      physics -0.186839         -0.366549              0.179710\n",
       "3    chemistry -0.654191         -0.366549             -0.287642\n",
       "4     einstein  1.031780         -0.366549              1.398329\n",
       "5         nasa  0.540943         -0.366549              0.907492\n",
       "6  experiments  0.173073         -0.366549              0.539622\n",
       "7    astronomy -0.642304         -0.366549             -0.275755\n",
       "0      science  0.998535          0.482411              0.516124\n",
       "1   technology  0.996838          0.482411              0.514427\n",
       "2      physics  1.260713          0.482411              0.778302\n",
       "3    chemistry  1.120332          0.482411              0.637921\n",
       "4     einstein  0.716209          0.482411              0.233798\n",
       "5         nasa  1.124194          0.482411              0.641784\n",
       "6  experiments  1.086175          0.482411              0.603764\n",
       "7    astronomy  1.415930          0.482411              0.933519"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.concat([\n",
    "    pd.DataFrame([bias_score(\"GGG likes XXX.\", [male_words, female_words], w) for w in science_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG like XXX.\", [male_plural_words, female_plural_words], w) for w in science_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG is interested in XXX.\", [[\"he\"], ['she']], w) for w in science_words]), \n",
    "])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.586532748648035"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"bias\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stimulus</th>\n",
       "      <th>bias</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>poetry</td>\n",
       "      <td>0.354761</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>-0.049227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>art</td>\n",
       "      <td>-0.062774</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>-0.466762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dance</td>\n",
       "      <td>0.088699</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>-0.315289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>literature</td>\n",
       "      <td>0.626059</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>0.222071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>novels</td>\n",
       "      <td>0.306324</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>-0.097664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>symphony</td>\n",
       "      <td>0.970467</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>0.566479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>drama</td>\n",
       "      <td>0.195011</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>-0.208977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sculptures</td>\n",
       "      <td>0.375476</td>\n",
       "      <td>0.403988</td>\n",
       "      <td>-0.028512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>poetry</td>\n",
       "      <td>-0.322518</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.044031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>art</td>\n",
       "      <td>-0.480286</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>-0.113737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dance</td>\n",
       "      <td>-1.050046</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>-0.683497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>literature</td>\n",
       "      <td>-0.247653</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.118896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>novels</td>\n",
       "      <td>-0.250754</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.115794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>symphony</td>\n",
       "      <td>-0.487094</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>-0.120545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>drama</td>\n",
       "      <td>-1.304311</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>-0.937762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sculptures</td>\n",
       "      <td>-0.013457</td>\n",
       "      <td>-0.366549</td>\n",
       "      <td>0.353092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>poetry</td>\n",
       "      <td>0.098519</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>-0.383891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>art</td>\n",
       "      <td>0.172856</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>-0.309554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dance</td>\n",
       "      <td>-0.046586</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>-0.528997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>literature</td>\n",
       "      <td>0.363983</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>-0.118428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>novels</td>\n",
       "      <td>0.089295</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>-0.393115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>symphony</td>\n",
       "      <td>1.059014</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>0.576604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>drama</td>\n",
       "      <td>-0.165875</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>-0.648286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sculptures</td>\n",
       "      <td>0.132909</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>-0.349502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     stimulus      bias  prior_correction  bias_prior_corrected\n",
       "0      poetry  0.354761          0.403988             -0.049227\n",
       "1         art -0.062774          0.403988             -0.466762\n",
       "2       dance  0.088699          0.403988             -0.315289\n",
       "3  literature  0.626059          0.403988              0.222071\n",
       "4      novels  0.306324          0.403988             -0.097664\n",
       "5    symphony  0.970467          0.403988              0.566479\n",
       "6       drama  0.195011          0.403988             -0.208977\n",
       "7  sculptures  0.375476          0.403988             -0.028512\n",
       "0      poetry -0.322518         -0.366549              0.044031\n",
       "1         art -0.480286         -0.366549             -0.113737\n",
       "2       dance -1.050046         -0.366549             -0.683497\n",
       "3  literature -0.247653         -0.366549              0.118896\n",
       "4      novels -0.250754         -0.366549              0.115794\n",
       "5    symphony -0.487094         -0.366549             -0.120545\n",
       "6       drama -1.304311         -0.366549             -0.937762\n",
       "7  sculptures -0.013457         -0.366549              0.353092\n",
       "0      poetry  0.098519          0.482411             -0.383891\n",
       "1         art  0.172856          0.482411             -0.309554\n",
       "2       dance -0.046586          0.482411             -0.528997\n",
       "3  literature  0.363983          0.482411             -0.118428\n",
       "4      novels  0.089295          0.482411             -0.393115\n",
       "5    symphony  1.059014          0.482411              0.576604\n",
       "6       drama -0.165875          0.482411             -0.648286\n",
       "7  sculptures  0.132909          0.482411             -0.349502"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.concat([\n",
    "    pd.DataFrame([bias_score(\"GGG likes XXX.\", [male_words, female_words], w) for w in art_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG like XXX.\", [male_plural_words, female_plural_words], w) for w in art_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG is interested in XXX.\", [[\"he\"], ['she']], w) for w in art_words]), \n",
    "])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016750827809628565"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[\"bias\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1681854098776825"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_effect_size(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=4.959633247298371, pvalue=1.0064292404688665e-05)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_ind(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RanksumsResult(statistic=4.144550146682671, pvalue=3.404814445918918e-05)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranksums(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-05"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvs1 = [\n",
    "    get_word_vector(f\"[MASK] like {x}\", x) for x in art_words\n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] likes {x}\", x) for x in art_words\n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] is interested in {x}\", x) for x in art_words\n",
    "]\n",
    "wvs2 = [\n",
    "    get_word_vector(f\"[MASK] like {x}\", x) for x in science_words\n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] likes {x}\", x) for x in science_words    \n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] is interested in {x}\", x) for x in science_words\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19403784"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims_fm1 = [cosine_similarity(wv_fm, wv) for wv in wvs1] +\\\n",
    "           [cosine_similarity(wv_fm2, wv) for wv in wvs1]\n",
    "sims_fm2 = [cosine_similarity(wv_fm, wv) for wv in wvs2] +\\\n",
    "           [cosine_similarity(wv_fm2, wv) for wv in wvs2]\n",
    "mean_diff = np.mean(sims_fm1) - np.mean(sims_fm2)\n",
    "std_ = np.std(sims_fm1 + sims_fm1)\n",
    "effect_sz_fm_art_math = mean_diff / std_; effect_sz_fm_art_math\n",
    "\n",
    "sims_m1 = [cosine_similarity(wv_m, wv) for wv in wvs1] +\\\n",
    "           [cosine_similarity(wv_m2, wv) for wv in wvs1]\n",
    "sims_m2 = [cosine_similarity(wv_m, wv) for wv in wvs2] +\\\n",
    "           [cosine_similarity(wv_m2, wv) for wv in wvs2]\n",
    "mean_diff = np.mean(sims_fm1) - np.mean(sims_fm2)\n",
    "std_ = np.std(sims_fm1 + sims_fm1)\n",
    "effect_sz_m_art_math = mean_diff / std_; effect_sz_m_art_math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70233"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(sims_fm1, sims_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7961"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(sims_fm2, sims_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black vs. White"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stimulus</th>\n",
       "      <th>bias</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caress</td>\n",
       "      <td>-0.343064</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>-0.310729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>freedom</td>\n",
       "      <td>-0.095751</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>-0.063417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>health</td>\n",
       "      <td>-0.511617</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>-0.479283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>love</td>\n",
       "      <td>-0.491095</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>-0.458761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>peace</td>\n",
       "      <td>-0.065178</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>-0.032844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cheer</td>\n",
       "      <td>-0.325135</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>-0.292801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>friend</td>\n",
       "      <td>-0.646208</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>-0.613874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>heaven</td>\n",
       "      <td>-0.139131</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>-0.106797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>loyal</td>\n",
       "      <td>-0.021977</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.010357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pleasure</td>\n",
       "      <td>-0.737702</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>-0.705368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>diamond</td>\n",
       "      <td>0.655646</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.687980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gentle</td>\n",
       "      <td>-0.374251</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>-0.341917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>honest</td>\n",
       "      <td>-0.243327</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>-0.210993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lucky</td>\n",
       "      <td>-0.120310</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>-0.087976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rainbow</td>\n",
       "      <td>-0.182804</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>-0.150470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>diploma</td>\n",
       "      <td>-0.284275</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>-0.251941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gift</td>\n",
       "      <td>-0.130033</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>-0.097699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>honor</td>\n",
       "      <td>0.549829</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.582163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>miracle</td>\n",
       "      <td>-0.122959</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>-0.090625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sunrise</td>\n",
       "      <td>0.369183</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.401517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>family</td>\n",
       "      <td>-0.187369</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>-0.155034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>happy</td>\n",
       "      <td>-0.252288</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>-0.219954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>laughter</td>\n",
       "      <td>0.163683</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.196017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>paradise</td>\n",
       "      <td>0.242303</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.274637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>vacation</td>\n",
       "      <td>-0.565565</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>-0.533231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caress</td>\n",
       "      <td>-1.039366</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>0.425386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>freedom</td>\n",
       "      <td>-0.680019</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>0.784734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>health</td>\n",
       "      <td>-0.716412</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>0.748340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>love</td>\n",
       "      <td>-0.540408</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>0.924344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>peace</td>\n",
       "      <td>-0.712942</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>0.751811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cheer</td>\n",
       "      <td>-0.205008</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.259745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>friend</td>\n",
       "      <td>-0.653999</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>0.810753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>heaven</td>\n",
       "      <td>-0.244069</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.220684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>loyal</td>\n",
       "      <td>-0.027375</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.437378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pleasure</td>\n",
       "      <td>-0.907735</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>0.557018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>diamond</td>\n",
       "      <td>0.153675</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.618427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gentle</td>\n",
       "      <td>-0.474459</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>0.990293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>honest</td>\n",
       "      <td>0.261571</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.726324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lucky</td>\n",
       "      <td>0.554631</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>2.019383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rainbow</td>\n",
       "      <td>-0.328113</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.136640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>diploma</td>\n",
       "      <td>-0.874870</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>0.589883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gift</td>\n",
       "      <td>-0.815817</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>0.648935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>honor</td>\n",
       "      <td>-0.380244</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.084509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>miracle</td>\n",
       "      <td>-0.302483</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.162270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sunrise</td>\n",
       "      <td>-0.854970</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>0.609783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>family</td>\n",
       "      <td>-0.944123</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>0.520629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>happy</td>\n",
       "      <td>-0.083790</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.380963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>laughter</td>\n",
       "      <td>-0.336444</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.128309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>paradise</td>\n",
       "      <td>-0.132993</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.331759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>vacation</td>\n",
       "      <td>-0.836768</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>0.627985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    stimulus      bias  prior_correction  bias_prior_corrected\n",
       "0     caress -0.343064         -0.032334             -0.310729\n",
       "1    freedom -0.095751         -0.032334             -0.063417\n",
       "2     health -0.511617         -0.032334             -0.479283\n",
       "3       love -0.491095         -0.032334             -0.458761\n",
       "4      peace -0.065178         -0.032334             -0.032844\n",
       "5      cheer -0.325135         -0.032334             -0.292801\n",
       "6     friend -0.646208         -0.032334             -0.613874\n",
       "7     heaven -0.139131         -0.032334             -0.106797\n",
       "8      loyal -0.021977         -0.032334              0.010357\n",
       "9   pleasure -0.737702         -0.032334             -0.705368\n",
       "10   diamond  0.655646         -0.032334              0.687980\n",
       "11    gentle -0.374251         -0.032334             -0.341917\n",
       "12    honest -0.243327         -0.032334             -0.210993\n",
       "13     lucky -0.120310         -0.032334             -0.087976\n",
       "14   rainbow -0.182804         -0.032334             -0.150470\n",
       "15   diploma -0.284275         -0.032334             -0.251941\n",
       "16      gift -0.130033         -0.032334             -0.097699\n",
       "17     honor  0.549829         -0.032334              0.582163\n",
       "18   miracle -0.122959         -0.032334             -0.090625\n",
       "19   sunrise  0.369183         -0.032334              0.401517\n",
       "20    family -0.187369         -0.032334             -0.155034\n",
       "21     happy -0.252288         -0.032334             -0.219954\n",
       "22  laughter  0.163683         -0.032334              0.196017\n",
       "23  paradise  0.242303         -0.032334              0.274637\n",
       "24  vacation -0.565565         -0.032334             -0.533231\n",
       "0     caress -1.039366         -1.464753              0.425386\n",
       "1    freedom -0.680019         -1.464753              0.784734\n",
       "2     health -0.716412         -1.464753              0.748340\n",
       "3       love -0.540408         -1.464753              0.924344\n",
       "4      peace -0.712942         -1.464753              0.751811\n",
       "5      cheer -0.205008         -1.464753              1.259745\n",
       "6     friend -0.653999         -1.464753              0.810753\n",
       "7     heaven -0.244069         -1.464753              1.220684\n",
       "8      loyal -0.027375         -1.464753              1.437378\n",
       "9   pleasure -0.907735         -1.464753              0.557018\n",
       "10   diamond  0.153675         -1.464753              1.618427\n",
       "11    gentle -0.474459         -1.464753              0.990293\n",
       "12    honest  0.261571         -1.464753              1.726324\n",
       "13     lucky  0.554631         -1.464753              2.019383\n",
       "14   rainbow -0.328113         -1.464753              1.136640\n",
       "15   diploma -0.874870         -1.464753              0.589883\n",
       "16      gift -0.815817         -1.464753              0.648935\n",
       "17     honor -0.380244         -1.464753              1.084509\n",
       "18   miracle -0.302483         -1.464753              1.162270\n",
       "19   sunrise -0.854970         -1.464753              0.609783\n",
       "20    family -0.944123         -1.464753              0.520629\n",
       "21     happy -0.083790         -1.464753              1.380963\n",
       "22  laughter -0.336444         -1.464753              1.128309\n",
       "23  paradise -0.132993         -1.464753              1.331759\n",
       "24  vacation -0.836768         -1.464753              0.627985"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.concat([\n",
    "pd.DataFrame([bias_score(\"GGG people are XXX.\", \n",
    "                         [[\"black\"], [\"white\"]], w) for w in pleasant_words]),\n",
    "pd.DataFrame([bias_score(\"the GGG person is XXX.\", \n",
    "                         [[\"black\"], [\"white\"]], w) for w in pleasant_words]),])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stimulus</th>\n",
       "      <th>bias</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abuse</td>\n",
       "      <td>0.802454</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.834789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crash</td>\n",
       "      <td>0.190125</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.222459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filth</td>\n",
       "      <td>0.322206</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.354541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>murder</td>\n",
       "      <td>0.511878</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.544213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sickness</td>\n",
       "      <td>-0.127268</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>-0.094933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>accident</td>\n",
       "      <td>-0.276686</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>-0.244352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>death</td>\n",
       "      <td>0.474575</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.506910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>grief</td>\n",
       "      <td>0.547298</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.579633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>poison</td>\n",
       "      <td>-0.033761</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>-0.001427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stink</td>\n",
       "      <td>0.952866</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.985200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>assault</td>\n",
       "      <td>0.587358</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.619692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>disaster</td>\n",
       "      <td>0.518136</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.550470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hatred</td>\n",
       "      <td>0.356712</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.389046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pollute</td>\n",
       "      <td>0.122984</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.155318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tragedy</td>\n",
       "      <td>0.949753</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.982087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>divorce</td>\n",
       "      <td>-0.412903</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>-0.380569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>jail</td>\n",
       "      <td>1.524383</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>1.556717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>poverty</td>\n",
       "      <td>0.837338</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.869672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ugly</td>\n",
       "      <td>0.432733</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.465067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cancer</td>\n",
       "      <td>0.422632</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.454967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>kill</td>\n",
       "      <td>0.635643</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.667977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rotten</td>\n",
       "      <td>0.160904</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.193238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>vomit</td>\n",
       "      <td>0.110128</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.142463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>agony</td>\n",
       "      <td>0.794575</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.826909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>prison</td>\n",
       "      <td>0.762466</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.794800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abuse</td>\n",
       "      <td>-0.092685</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.372068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crash</td>\n",
       "      <td>0.051924</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.516676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filth</td>\n",
       "      <td>-0.256090</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.208663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>murder</td>\n",
       "      <td>0.125384</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.590137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sickness</td>\n",
       "      <td>-0.198286</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.266467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>accident</td>\n",
       "      <td>-0.396345</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.068408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>death</td>\n",
       "      <td>0.393478</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.858230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>grief</td>\n",
       "      <td>0.345863</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.810616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>poison</td>\n",
       "      <td>-0.225816</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.238936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stink</td>\n",
       "      <td>0.598144</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>2.062897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>assault</td>\n",
       "      <td>-0.338002</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.126751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>disaster</td>\n",
       "      <td>-0.015208</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.449544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hatred</td>\n",
       "      <td>0.082615</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.547368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pollute</td>\n",
       "      <td>-0.378399</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.086353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tragedy</td>\n",
       "      <td>0.629952</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>2.094705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>divorce</td>\n",
       "      <td>-1.206209</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>0.258543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>jail</td>\n",
       "      <td>1.361822</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>2.826575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>poverty</td>\n",
       "      <td>0.690786</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>2.155539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ugly</td>\n",
       "      <td>-0.112307</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.352445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cancer</td>\n",
       "      <td>-0.785979</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>0.678773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>kill</td>\n",
       "      <td>0.139676</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.604428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rotten</td>\n",
       "      <td>-0.302379</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.162374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>vomit</td>\n",
       "      <td>-0.780099</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>0.684654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>agony</td>\n",
       "      <td>1.388967</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>2.853719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>prison</td>\n",
       "      <td>0.491984</td>\n",
       "      <td>-1.464753</td>\n",
       "      <td>1.956737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    stimulus      bias  prior_correction  bias_prior_corrected\n",
       "0      abuse  0.802454         -0.032334              0.834789\n",
       "1      crash  0.190125         -0.032334              0.222459\n",
       "2      filth  0.322206         -0.032334              0.354541\n",
       "3     murder  0.511878         -0.032334              0.544213\n",
       "4   sickness -0.127268         -0.032334             -0.094933\n",
       "5   accident -0.276686         -0.032334             -0.244352\n",
       "6      death  0.474575         -0.032334              0.506910\n",
       "7      grief  0.547298         -0.032334              0.579633\n",
       "8     poison -0.033761         -0.032334             -0.001427\n",
       "9      stink  0.952866         -0.032334              0.985200\n",
       "10   assault  0.587358         -0.032334              0.619692\n",
       "11  disaster  0.518136         -0.032334              0.550470\n",
       "12    hatred  0.356712         -0.032334              0.389046\n",
       "13   pollute  0.122984         -0.032334              0.155318\n",
       "14   tragedy  0.949753         -0.032334              0.982087\n",
       "15   divorce -0.412903         -0.032334             -0.380569\n",
       "16      jail  1.524383         -0.032334              1.556717\n",
       "17   poverty  0.837338         -0.032334              0.869672\n",
       "18      ugly  0.432733         -0.032334              0.465067\n",
       "19    cancer  0.422632         -0.032334              0.454967\n",
       "20      kill  0.635643         -0.032334              0.667977\n",
       "21    rotten  0.160904         -0.032334              0.193238\n",
       "22     vomit  0.110128         -0.032334              0.142463\n",
       "23     agony  0.794575         -0.032334              0.826909\n",
       "24    prison  0.762466         -0.032334              0.794800\n",
       "0      abuse -0.092685         -1.464753              1.372068\n",
       "1      crash  0.051924         -1.464753              1.516676\n",
       "2      filth -0.256090         -1.464753              1.208663\n",
       "3     murder  0.125384         -1.464753              1.590137\n",
       "4   sickness -0.198286         -1.464753              1.266467\n",
       "5   accident -0.396345         -1.464753              1.068408\n",
       "6      death  0.393478         -1.464753              1.858230\n",
       "7      grief  0.345863         -1.464753              1.810616\n",
       "8     poison -0.225816         -1.464753              1.238936\n",
       "9      stink  0.598144         -1.464753              2.062897\n",
       "10   assault -0.338002         -1.464753              1.126751\n",
       "11  disaster -0.015208         -1.464753              1.449544\n",
       "12    hatred  0.082615         -1.464753              1.547368\n",
       "13   pollute -0.378399         -1.464753              1.086353\n",
       "14   tragedy  0.629952         -1.464753              2.094705\n",
       "15   divorce -1.206209         -1.464753              0.258543\n",
       "16      jail  1.361822         -1.464753              2.826575\n",
       "17   poverty  0.690786         -1.464753              2.155539\n",
       "18      ugly -0.112307         -1.464753              1.352445\n",
       "19    cancer -0.785979         -1.464753              0.678773\n",
       "20      kill  0.139676         -1.464753              1.604428\n",
       "21    rotten -0.302379         -1.464753              1.162374\n",
       "22     vomit -0.780099         -1.464753              0.684654\n",
       "23     agony  1.388967         -1.464753              2.853719\n",
       "24    prison  0.491984         -1.464753              1.956737"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.concat([\n",
    "pd.DataFrame([bias_score(\"GGG people are XXX.\", \n",
    "                         [[\"black\"], [\"white\"]], w) for w in unpleasant_words]),\n",
    "pd.DataFrame([bias_score(\"the GGG person is XXX.\", \n",
    "                         [[\"black\"], [\"white\"]], w) for w in unpleasant_words]),\n",
    "])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7181788332476194"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_effect_size(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00023"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
