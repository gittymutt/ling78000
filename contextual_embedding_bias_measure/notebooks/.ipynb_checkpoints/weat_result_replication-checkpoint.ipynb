{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_utils import Config, BertPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    model_type=\"bert-base-uncased\",\n",
    "    max_seq_len=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BertPreprocessor(config.model_type, config.max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import BertConfig, BertForMaskedLM\n",
    "model = BertForMaskedLM.from_pretrained(config.model_type)\n",
    "model.eval() # Important! Disable dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(sentence: str) -> np.ndarray:\n",
    "    return model(processor.to_bert_model_input(sentence))[0, :, :].cpu().detach().numpy()\n",
    "\n",
    "def softmax(arr, axis=1):\n",
    "    e = np.exp(arr)\n",
    "    return e / e.sum(axis=axis, keepdims=True)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_mask_fill_logits(sentence: str, words: Iterable[str],\n",
    "                         use_last_mask=False, apply_softmax=True) -> Dict[str, float]:\n",
    "    mask_i = processor.get_index(sentence, \"[MASK]\", last=use_last_mask, accept_wordpiece=True)\n",
    "    logits = defaultdict(list)\n",
    "    out_logits = get_logits(sentence)\n",
    "    if apply_softmax: \n",
    "        out_logits = softmax(out_logits)\n",
    "    return {w: out_logits[mask_i, processor.token_to_index(w, accept_wordpiece=True)] for w in words}\n",
    "\n",
    "def bias_score(sentence: str, gender_words: Iterable[Iterable[str]], \n",
    "               word: str, gender_comes_first=True) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Input a sentence of the form \"GGG is XXX\"\n",
    "    XXX is a placeholder for the target word\n",
    "    GGG is a placeholder for the gendered words (the subject)\n",
    "    We will predict the bias when filling in the gendered words and \n",
    "    filling in the target word.\n",
    "    \n",
    "    gender_comes_first: whether GGG comes before XXX (TODO: better way of handling this?)\n",
    "    \"\"\"\n",
    "    # probability of filling [MASK] with \"he\" vs. \"she\" when target is \"programmer\"\n",
    "    mwords, fwords = gender_words\n",
    "    all_words = mwords + fwords\n",
    "    subject_fill_logits = get_mask_fill_logits(\n",
    "        sentence.replace(\"XXX\", word).replace(\"GGG\", \"[MASK]\"), \n",
    "        all_words, use_last_mask=not gender_comes_first,\n",
    "    )\n",
    "    subject_fill_bias = np.log(sum(subject_fill_logits[mw] for mw in mwords)) - \\\n",
    "                        np.log(sum(subject_fill_logits[fw] for fw in fwords))\n",
    "    # male words are simply more likely than female words\n",
    "    # correct for this by masking the target word and measuring the prior probabilities\n",
    "    subject_fill_prior_logits = get_mask_fill_logits(\n",
    "        sentence.replace(\"XXX\", \"[MASK]\").replace(\"GGG\", \"[MASK]\"), \n",
    "        all_words, use_last_mask=gender_comes_first,\n",
    "    )\n",
    "    subject_fill_bias_prior_correction = \\\n",
    "            np.log(sum(subject_fill_prior_logits[mw] for mw in mwords)) - \\\n",
    "            np.log(sum(subject_fill_prior_logits[fw] for fw in fwords))\n",
    "    \n",
    "    return {\n",
    "            \"stimulus\": word,\n",
    "            \"bias\": subject_fill_bias,\n",
    "            \"prior_correction\": subject_fill_bias_prior_correction,\n",
    "            \"bias_prior_corrected\": subject_fill_bias - subject_fill_bias_prior_correction,\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'flower': 0.0007418048, 'bug': 1.0748333e-05}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mask_fill_logits(\"the [MASK] is beautiful\", [\"flower\", \"bug\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(sentence: str, word: str):\n",
    "    idx = processor.get_index(sentence, word, accept_wordpiece=True)\n",
    "    outputs = None\n",
    "    with torch.no_grad():\n",
    "        sequence_output, _ = model.bert(processor.to_bert_model_input(sentence),\n",
    "                                        output_all_encoded_layers=False)\n",
    "        sequence_output.squeeze_(0)\n",
    "    return sequence_output.detach().cpu().numpy()[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(x, y):\n",
    "    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_effect_size(df1, df2, k=\"bias_prior_corrected\"):\n",
    "    diff = (df1[k].mean() - df2[k].mean())\n",
    "    std_ = pd.concat([df1, df2], axis=0)[k].std() + 1e-8\n",
    "    return diff / std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_mc_perm_test(xs, ys, nmc=100000):\n",
    "    n, k = len(xs), 0\n",
    "    diff = np.abs(np.mean(xs) - np.mean(ys))\n",
    "    zs = np.concatenate([xs, ys])\n",
    "    for j in range(nmc):\n",
    "        np.random.shuffle(zs)\n",
    "        k += diff < np.abs(np.mean(zs[:n]) - np.mean(zs[n:]))\n",
    "    return k / nmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.06369518e-01,  2.22109765e-01, -7.18901008e-02,  1.70864940e-01,\n",
       "        3.62271309e-01, -1.20856151e-01, -6.16566688e-02,  7.87993133e-01,\n",
       "        3.73470753e-01, -2.52068713e-02,  2.59382874e-02, -1.49484551e+00,\n",
       "       -1.80714473e-01,  1.11428094e+00, -1.03053010e+00,  8.63718510e-01,\n",
       "        1.34956911e-01,  1.07128906e+00,  1.33132502e-01,  8.86740446e-01,\n",
       "       -1.61311820e-01, -5.93276024e-01,  7.23845139e-02, -5.57699025e-01,\n",
       "        1.10267961e+00, -5.62288880e-01, -2.21851707e-01,  8.34756911e-01,\n",
       "        4.88106251e-01, -9.58714262e-03,  2.16243923e-01,  1.58447057e-01,\n",
       "        5.18487453e-01,  2.14277059e-01,  1.41341984e-01, -2.22598538e-01,\n",
       "       -1.90466657e-01,  3.14576507e-01, -4.24953938e-01, -3.25795531e-01,\n",
       "       -7.41146803e-01, -1.04409897e+00,  6.84070289e-01,  4.49090868e-01,\n",
       "        2.56366879e-01, -7.32405782e-01, -9.85546529e-01,  3.55274409e-01,\n",
       "        9.63768840e-01,  3.46497953e-01,  5.14259525e-02,  5.61742187e-01,\n",
       "        2.53276885e-01, -1.06839433e-01,  4.62810725e-01,  5.47232807e-01,\n",
       "       -8.03264678e-01, -7.77305663e-01, -2.00483233e-01, -5.28368711e-01,\n",
       "       -4.77690250e-01,  8.90959650e-02,  3.79050970e-01, -1.00972667e-01,\n",
       "        9.57354531e-02, -8.97196382e-02, -1.48199931e-01,  1.25380859e-01,\n",
       "       -9.09168124e-01, -4.43255037e-01,  1.24072999e-01,  4.07962769e-01,\n",
       "        3.49193603e-01, -5.10468483e-02, -4.53874648e-01,  2.38604665e-01,\n",
       "       -3.83497596e-01,  2.26237416e-01, -2.38299310e-01, -4.38013732e-01,\n",
       "       -2.28098884e-01, -1.11407094e-01,  4.13440675e-01,  3.87610048e-01,\n",
       "       -9.92386416e-02,  8.58975649e-02, -9.76567626e-01,  4.05914634e-02,\n",
       "       -8.99850503e-02,  2.05641791e-01,  1.58097193e-01, -1.85252249e-01,\n",
       "       -5.00990570e-01,  2.39046440e-01, -1.34954855e-01, -4.15777266e-01,\n",
       "        7.87932426e-02,  4.71143544e-01, -7.26344362e-02,  2.40169138e-01,\n",
       "        8.60921502e-01, -8.16041604e-02, -2.44584411e-01,  4.08138633e-01,\n",
       "       -6.02750778e-01, -4.35080618e-01, -6.49739131e-02, -1.62537664e-01,\n",
       "        2.65623093e-01,  7.45046198e-01, -7.62214780e-01,  4.02592182e-01,\n",
       "       -4.47386235e-01,  1.19652815e-01, -1.14981580e+00,  1.24807334e+00,\n",
       "        5.31868756e-01, -4.88355666e-01,  1.46929264e-01, -2.39980385e-01,\n",
       "        6.72569633e-01, -3.89886677e-01,  3.02861094e-01,  4.00121808e-01,\n",
       "       -1.69996589e-01, -5.11888385e-01,  1.92454547e-01,  1.86521038e-01,\n",
       "       -8.37388575e-01,  1.05222240e-01, -4.64141965e-01,  4.67706263e-01,\n",
       "       -1.85250908e-01, -5.01711130e-01,  2.23009974e-01, -9.75936174e-01,\n",
       "       -1.94810122e-01, -2.00362608e-01, -7.91729391e-02,  4.77513000e-02,\n",
       "       -3.62029582e-01,  5.16069531e-01, -1.61559805e-01,  1.06315064e+00,\n",
       "       -8.03826302e-02,  6.59516811e-01, -7.46768713e-03,  8.29806849e-02,\n",
       "       -2.24850640e-01, -7.57497907e-01,  4.00907844e-01,  2.02943161e-01,\n",
       "       -6.59538954e-02, -1.73705071e-03, -2.10254133e-01,  3.24883103e-01,\n",
       "       -6.15544558e-01,  6.77369773e-01,  1.87589675e-01,  3.29706892e-02,\n",
       "       -3.59585941e-01, -7.53547549e-01,  1.65893540e-01,  4.97108281e-01,\n",
       "       -3.97464573e-01, -4.43160743e-01,  3.82503033e-01,  2.65457958e-01,\n",
       "       -7.97319114e-02,  7.20816493e-01, -8.93282712e-01, -5.99095449e-02,\n",
       "        2.71350324e-01,  6.12361990e-02,  9.22667027e-01, -2.92799294e-01,\n",
       "        2.65346438e-01, -8.09285223e-01, -3.07702005e-01,  6.68435872e-01,\n",
       "       -2.71839440e-01,  6.16511285e-01,  4.73261029e-01,  7.08366871e-01,\n",
       "       -8.79255906e-02, -4.17531192e-01,  1.00453699e+00,  5.55702388e-01,\n",
       "       -2.85637379e-01, -1.49904251e-01, -1.97960362e-01, -6.10363066e-01,\n",
       "       -2.67986596e-01,  2.12840974e-01, -1.73632234e-01, -9.85309184e-02,\n",
       "       -3.27348746e-02, -3.27062845e-01, -9.93386865e-01, -3.90170932e-01,\n",
       "        6.61132514e-01,  1.64619207e-01, -2.00865895e-01, -4.29663748e-01,\n",
       "       -5.69214761e-01, -1.22738987e-01,  5.86378932e-01, -4.87890422e-01,\n",
       "        5.74026227e-01,  3.13331425e-01, -2.59083867e-01, -9.16624442e-03,\n",
       "        9.99531269e-01, -3.05078804e-01,  4.39469904e-01,  4.63630743e-02,\n",
       "       -6.72846735e-02,  1.23664290e-02, -6.13555312e-03,  1.25280889e-02,\n",
       "        3.91279519e-01,  2.25989610e-01, -5.14249146e-01,  7.67559767e-01,\n",
       "       -5.49520433e-01,  4.78257179e-01,  6.96447790e-01, -4.86617714e-01,\n",
       "        2.84597576e-01, -7.60636330e-02,  2.09488004e-01,  3.72304559e-01,\n",
       "        8.20942223e-01, -2.82124847e-01, -2.64740586e-01,  3.12003314e-01,\n",
       "       -1.26166776e-01,  1.17298141e-02, -1.29803941e-01,  7.37189054e-01,\n",
       "        7.90811926e-02,  5.44379234e-01, -1.34595215e-01,  8.24179649e-02,\n",
       "        3.50830287e-01, -3.43899339e-01,  3.32313001e-01, -2.02917427e-01,\n",
       "        5.53840280e-01, -6.55833423e-01, -2.59503275e-01,  4.76769000e-01,\n",
       "        3.65739167e-02, -5.60237765e-01, -1.65224820e-01, -7.70691559e-02,\n",
       "        8.48130882e-02,  6.74858630e-01,  1.67155713e-01, -5.50797656e-02,\n",
       "        2.92654395e-01,  9.48140994e-02,  3.49618852e-01, -1.01741917e-01,\n",
       "       -5.75546801e-01, -9.16646868e-02, -4.70333636e-01,  1.39525473e-01,\n",
       "        1.03665516e-01, -8.79752859e-02, -3.27949286e-01,  1.46400273e-01,\n",
       "       -1.55137390e-01,  2.56914496e-01, -4.41621423e-01, -6.69949174e-01,\n",
       "       -5.78222036e-01, -2.89916277e-01, -4.90753427e-02, -5.57360798e-02,\n",
       "        3.50906134e-01,  2.79748768e-01, -7.24173903e-01, -1.26637161e-01,\n",
       "        4.46231216e-01, -4.43808228e-01,  2.34892920e-01,  6.79355025e-01,\n",
       "       -3.44263196e-01, -1.74710751e-01, -9.37994778e-01,  5.34851789e-01,\n",
       "       -4.74429935e-01,  9.28932279e-02,  2.20538303e-01, -2.23297596e-01,\n",
       "        4.19267237e-01,  4.41834703e-02,  9.44654644e-01, -2.93301828e-02,\n",
       "        5.16590834e-01,  3.71194452e-01,  6.15360677e-01, -5.15531898e-01,\n",
       "       -4.88995641e-01,  7.38495290e-01, -1.74698308e-01,  2.31962964e-01,\n",
       "       -4.53382158e+00,  1.38129786e-01, -3.99393812e-02,  2.39824858e-02,\n",
       "        4.55935359e-01,  1.36820689e-01, -1.29579142e-01, -1.26011461e-01,\n",
       "        1.06470615e-01,  1.37062132e-01,  7.21079528e-01, -1.68523148e-01,\n",
       "        9.27704811e-01,  1.15799412e-01,  3.16249192e-01, -4.17039871e-01,\n",
       "        3.86143118e-01, -1.19816065e+00, -3.30826253e-01,  1.68343291e-01,\n",
       "       -6.97976947e-01,  7.14322254e-02,  1.21875301e-01,  4.71082747e-01,\n",
       "        1.15364641e-01,  2.85469480e-02,  1.08732253e-01, -4.02185053e-01,\n",
       "       -7.08010316e-01,  7.19707131e-01, -6.40040815e-01, -3.56172621e-01,\n",
       "        6.06680334e-01,  7.74816573e-01, -7.36118436e-01, -4.47126180e-01,\n",
       "        4.02376801e-02, -6.86364055e-01,  2.88285434e-01,  2.14421272e-01,\n",
       "       -4.81394798e-01, -9.51110423e-01,  7.40530550e-01,  9.28563297e-01,\n",
       "        1.59917593e-01, -3.86909366e-01, -5.57639122e-01, -1.57299742e-01,\n",
       "       -2.53450394e-01,  7.66298115e-01,  7.25700915e-01, -5.26722558e-02,\n",
       "        4.22823429e-01, -4.83141750e-01, -7.36654103e-02, -2.71598518e-01,\n",
       "        2.12322325e-01,  5.62419713e-01, -3.76054704e-01, -1.32401466e-01,\n",
       "       -9.67185438e-01, -3.04849446e-01, -1.35320246e-01, -3.68716359e-01,\n",
       "       -5.14563560e-01, -6.20892107e-01, -5.71697831e-01, -3.99815530e-01,\n",
       "        1.85978636e-01,  5.12269378e-01,  4.00952011e-01, -3.20577532e-01,\n",
       "       -1.97420537e-01, -1.55722547e+00, -5.24125993e-01, -4.06745523e-01,\n",
       "       -4.88031268e-01,  2.44067222e-01,  1.13590561e-01, -3.55475061e-02,\n",
       "        4.02468652e-01, -6.37131929e-01,  8.35756660e-01,  4.04143274e-01,\n",
       "        6.41270638e-01, -7.00438917e-01, -1.38532147e-01, -4.34188008e-01,\n",
       "        8.99958014e-02,  8.98114443e-02,  1.78909153e-01,  4.16032150e-02,\n",
       "        1.30684882e-01,  5.43633938e-01,  1.61817476e-01, -3.23454440e-01,\n",
       "        4.83743489e-01,  3.85014206e-01,  7.64078200e-01, -8.34070265e-01,\n",
       "       -7.31380805e-02, -1.72004014e-01,  3.93362015e-01,  2.44497165e-01,\n",
       "       -8.41716826e-02, -4.31857347e-01, -1.02624607e+00,  1.33562103e-01,\n",
       "        3.45307350e-01,  1.86023563e-01,  1.26864433e+00, -3.96883458e-01,\n",
       "        7.20792174e-01, -5.74663222e-01,  7.64546022e-02,  7.88795650e-02,\n",
       "        4.09597963e-01,  2.75424123e-01,  8.58077705e-01, -2.31392756e-01,\n",
       "       -7.54590392e-01,  6.75302625e-01, -9.43760946e-02, -1.94428608e-01,\n",
       "        2.03639761e-01,  2.28710681e-01, -7.69647419e-01, -6.05415665e-02,\n",
       "       -9.27394629e-01,  2.02782303e-01,  7.57930726e-02, -4.03024465e-01,\n",
       "       -2.50724465e-01,  6.90565884e-01, -3.04469526e-01,  4.84250516e-01,\n",
       "       -1.05760860e+00, -1.22488654e+00, -2.68402249e-02,  4.93184805e-01,\n",
       "       -2.82254070e-02,  5.37834130e-02, -7.81420052e-01,  5.19675493e-01,\n",
       "        1.17235088e+00,  4.66050245e-02,  1.40038401e-01, -3.94704133e-01,\n",
       "       -4.01081860e-01, -4.00583684e-01, -3.29865098e-01, -3.89830768e-01,\n",
       "        8.50434154e-02, -1.00752473e+00, -6.19486213e-01, -1.16372514e+00,\n",
       "        2.22055420e-01,  2.31030881e-01, -5.49963355e-01, -7.19015181e-01,\n",
       "        2.97052830e-01,  2.51613021e-01, -1.85336620e-01, -1.48723334e-01,\n",
       "        5.02508938e-01,  1.38924524e-01,  3.66462201e-01, -2.06580862e-01,\n",
       "       -2.47316465e-01,  1.05458939e+00, -1.24157131e-01,  2.89026678e-01,\n",
       "       -6.68336272e-01, -2.57178426e-01, -1.54251158e-01,  4.66682196e-01,\n",
       "       -6.68329000e-01,  2.04154164e-01,  1.36733666e-01,  5.20241261e-01,\n",
       "       -1.22909002e-01, -2.04426169e-01, -3.31187725e-01,  2.44014516e-01,\n",
       "        6.06478274e-01, -2.26640433e-01,  1.73310012e-01, -1.15606976e+00,\n",
       "       -5.63660741e-01,  6.05199277e-01, -9.50557947e-01,  5.69976032e-01,\n",
       "       -8.86081636e-01, -5.27899384e-01, -7.94336438e-01, -6.86111927e-01,\n",
       "       -2.13280603e-01,  4.57363665e-01,  2.95508832e-01, -1.24294150e+00,\n",
       "        6.75271034e-01, -4.16885883e-01, -1.39369637e-01, -1.25983372e-01,\n",
       "       -1.08480237e-01, -4.27219748e-01,  6.90889597e-01,  3.16936076e-02,\n",
       "       -5.68816960e-01, -5.08291051e-02,  7.81225339e-02, -5.47761381e-01,\n",
       "       -1.06468332e+00, -8.24098587e-01, -4.01882939e-02,  4.05541211e-02,\n",
       "       -2.56478280e-01,  3.28599781e-01, -8.22212338e-01,  2.60284930e-01,\n",
       "        2.77458608e-01,  9.54266340e-02,  6.41827285e-03, -4.98912513e-01,\n",
       "       -2.92941034e-01, -7.07516909e-01, -1.52387798e-01,  4.57335353e-01,\n",
       "       -6.91832185e-01,  4.01380628e-01,  1.05152398e-01, -2.18253881e-01,\n",
       "        6.76671207e-01, -1.97770745e-01, -1.67895630e-02,  1.05948783e-01,\n",
       "       -7.11154699e-01, -3.23545158e-01,  3.90837789e-01,  4.15822804e-01,\n",
       "       -5.71506381e-01,  1.74523577e-01, -3.49796981e-01, -4.12660614e-02,\n",
       "       -5.92122018e-01,  9.35914099e-01,  1.17991328e-01,  5.62198877e-01,\n",
       "       -3.72251421e-01,  7.48511434e-01, -5.85672438e-01,  5.99284172e-01,\n",
       "       -4.19356227e-01,  8.60666484e-02,  7.13525772e-01, -8.62761676e-01,\n",
       "        2.57613420e-01,  4.63201553e-01, -3.24389279e-01, -3.87123764e-01,\n",
       "        1.26571029e-01, -4.87369031e-01, -1.44880638e-01,  3.96622986e-01,\n",
       "        9.33790952e-02, -7.66997695e-01,  1.52846172e-01,  2.18596354e-01,\n",
       "        1.03887403e+00, -4.50378895e-01, -4.82612282e-01,  5.47776401e-01,\n",
       "       -7.64465928e-02, -2.75664926e-02, -2.11762100e-01,  2.41730139e-01,\n",
       "       -2.00064063e-01, -1.00314021e+00,  5.30778289e-01, -8.17351460e-01,\n",
       "        2.32116021e-02,  1.94842666e-01,  2.77002752e-01, -1.59762949e-01,\n",
       "       -2.09863901e-01,  4.31712478e-01, -8.37902486e-01,  2.25760117e-02,\n",
       "       -1.50682747e-01, -1.69860974e-01, -4.66837734e-02, -4.68170047e-01,\n",
       "        2.99406558e-01,  2.24027243e-02,  1.15545079e-01,  4.00306344e-01,\n",
       "        9.41042542e-01,  3.78793418e-01, -2.34508932e-01, -3.30593348e-01,\n",
       "       -4.55935597e-01,  3.79623383e-01,  5.43193579e-01,  6.02502674e-02,\n",
       "        3.68198514e-01,  2.76881337e-01, -1.56597123e-01, -6.57819629e-01,\n",
       "        1.58714242e-02,  1.67995289e-01,  7.90993720e-02,  1.54479280e-01,\n",
       "        4.90975201e-01,  2.75600135e-01, -8.77219141e-01,  1.77820429e-01,\n",
       "        6.94640130e-02, -5.61373711e-01,  5.18324494e-01, -3.10646236e-01,\n",
       "        1.85657933e-01,  8.25930238e-02,  8.61015141e-01,  2.04245076e-01,\n",
       "        4.75537330e-01,  6.87890410e-01, -6.20347023e-01, -1.23703673e-01,\n",
       "        8.43648255e-01, -2.63560772e-01,  3.93254906e-01,  1.75094157e-01,\n",
       "       -1.26994765e+00,  1.00738490e+00,  3.38513888e-02, -3.47852141e-01,\n",
       "       -2.11264610e-01, -1.11965146e-02,  1.88761860e-01, -1.66786704e-02,\n",
       "        5.68395019e-01, -4.58167613e-01,  1.30343688e+00,  3.29140186e-01,\n",
       "        5.46640933e-01, -3.21959145e-02,  2.89696664e-01,  1.48958042e-01,\n",
       "        1.99962169e-01,  2.41495803e-01,  1.33437908e+00,  2.12382600e-01,\n",
       "       -1.36261851e-01, -4.51153129e-01,  7.00960457e-01, -2.55881101e-01,\n",
       "       -2.59274065e-01,  5.78921661e-02, -1.41588241e-01,  1.10462368e+00,\n",
       "        7.51242936e-02,  2.35104859e-01, -2.87890524e-01,  2.58328855e-01,\n",
       "       -1.03069174e+00,  1.06716323e+00, -4.04172629e-01,  4.79334109e-02,\n",
       "       -9.84776676e-01, -8.76864940e-02,  3.04778785e-01, -7.15065539e-01,\n",
       "        5.34995317e-01,  9.05357003e-01,  9.52498138e-01, -5.50198928e-02,\n",
       "       -1.20324075e-01, -4.17085588e-01,  1.07436335e+00,  5.77311218e-01,\n",
       "        7.36602366e-01,  4.23544079e-01, -2.73052782e-01,  3.25897098e-01,\n",
       "       -3.88508916e-01, -3.46491575e-01, -5.67286760e-02, -4.33420330e-01,\n",
       "       -5.11849448e-02, -4.95140135e-01,  2.81704932e-01,  7.30852544e-01,\n",
       "        2.33443588e-01, -2.26417929e-01,  1.03887282e-01,  9.29215550e-03,\n",
       "       -2.92512000e-01,  3.44494671e-01,  2.16403976e-01, -4.23743064e-03,\n",
       "        4.13833290e-01, -2.13304728e-01, -3.54139596e-01,  5.84301725e-03,\n",
       "        1.50868017e-02,  2.84027755e-01,  3.78560305e-01,  1.98405027e-01,\n",
       "       -7.75684237e-01,  1.83305398e-01, -4.69022781e-01,  1.06922388e+00,\n",
       "       -5.05056143e-01,  3.14204365e-01,  1.98299080e-01, -7.17022493e-02,\n",
       "        1.46759853e-01,  3.48679066e-01, -1.89368680e-01, -3.63281697e-01,\n",
       "       -3.87411833e-01, -3.29566956e-01, -5.20225525e-01,  1.04127240e+00,\n",
       "        6.45576119e-01, -1.19615388e+00, -3.86944525e-02,  8.02033186e-01,\n",
       "        3.74547184e-01, -2.84893364e-01, -2.37130344e-01,  1.45652071e-02,\n",
       "        6.64204806e-02,  4.17400926e-01,  1.79776847e-01,  1.18220806e-01,\n",
       "       -4.44052100e-01,  4.48320895e-01, -9.31756437e-01, -5.55737793e-01,\n",
       "        1.20350808e-01,  5.34013867e-01, -2.35694110e-01,  2.27902293e-01,\n",
       "       -8.65195990e-02, -3.15869749e-01, -1.51770145e-01, -6.60384774e-01,\n",
       "       -1.54838085e-01, -1.32897401e+00,  3.83683965e-02, -1.97420388e-01,\n",
       "        6.28133178e-01,  4.46448833e-01, -1.30691573e-01, -2.68271536e-01,\n",
       "        8.98969024e-02, -4.23212558e-01,  1.44195005e-01, -1.03101182e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_vector(\"the flower is beautiful\", \"flower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_vocab = {v:k for k, v in processor.full_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, ranksums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlxtend'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-2b0d16e38d50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmlxtend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpermutation_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mlxtend'"
     ]
    }
   ],
   "source": [
    "from mlxtend.evaluate import permutation_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flowers vs. Insects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All borrowed from WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_words(wlist, filter_oov=True):\n",
    "    return [w.strip() for w in wlist.lower().replace(\"\\n\", \" \").split(\", \") if w.strip() in rev_vocab or not filter_oov]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words not in vocab are removed and target words are converted to adjectives when applicable and removed otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flower_words = to_words(\"\"\"aster, clover, hyacinth, marigold, poppy, azalea, crocus, iris, orchid, rose, bluebell, daffodil, lilac, pansy, tulip, buttercup, daisy, lily, peony, violet, carnation, gladiola,\n",
    "# magnolia, petunia, zinnia\"\"\")\n",
    "# insect_words = to_words(\"\"\"ant, caterpillar, flea, locust, spider, bedbug, centipede, fly, maggot, tarantula,\n",
    "# bee, cockroach, gnat, mosquito, termite, beetle, cricket, hornet, moth, wasp, blackfly,\n",
    "# dragonfly, horsefly, roach, weevil\"\"\")\n",
    "flower_single_words = [\"flower\"]\n",
    "flower_words = [\"flowers\"]\n",
    "insect_single_words = [\"bug\"]\n",
    "insect_words = [\"bugs\"]\n",
    "pleasant_words = to_words(\"\"\"caress, freedom, health, love, peace, cheer, friend, heaven, loyal, pleasure, diamond, gentle, honest, lucky, rainbow, diploma, gift, honor, miracle, sunrise, family,\n",
    "happy, laughter, paradise, vacation\"\"\", filter_oov=False)\n",
    "unpleasant_words = to_words(\"\"\"abuse, crash, filth, murder, sickness, accident, death, grief, poison, stink,\n",
    "assault, disaster, hatred, pollute, tragedy, divorce, jail, poverty, ugly, cancer, kill, rotten,\n",
    "vomit, agony, prison\"\"\", filter_oov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stimulus': 'beautiful',\n",
       " 'bias': 5.321797706793324,\n",
       " 'prior_correction': 3.5636120069576354,\n",
       " 'bias_prior_corrected': 1.758185699835689}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_score(\"GGG are XXX.\", [flower_words, insect_words], \"beautiful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stimulus': 'pleasant',\n",
       " 'bias': 4.155169996205565,\n",
       " 'prior_correction': 3.5636120069576354,\n",
       " 'bias_prior_corrected': 0.5915579892479297}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_score(\"GGG are XXX.\", [flower_words, insect_words], \"pleasant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>stimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.534074</td>\n",
       "      <td>-5.327127</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.931570</td>\n",
       "      <td>-7.929632</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>freedom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.935260</td>\n",
       "      <td>-8.925941</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.969309</td>\n",
       "      <td>-6.891893</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.585581</td>\n",
       "      <td>-7.275620</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>peace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.589396</td>\n",
       "      <td>-8.271805</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>cheer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.620295</td>\n",
       "      <td>-7.240907</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.981358</td>\n",
       "      <td>-6.879843</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>heaven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.540774</td>\n",
       "      <td>-10.401975</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>loyal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.312522</td>\n",
       "      <td>-7.548679</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>pleasure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.466475</td>\n",
       "      <td>-6.394727</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>diamond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.188142</td>\n",
       "      <td>-8.673060</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>gentle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.053281</td>\n",
       "      <td>-7.807920</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>honest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.306459</td>\n",
       "      <td>-11.167660</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>lucky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7.155793</td>\n",
       "      <td>-2.705408</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>rainbow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.613336</td>\n",
       "      <td>-6.247866</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>diploma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.169296</td>\n",
       "      <td>-4.691905</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>gift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.403495</td>\n",
       "      <td>-5.457706</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>honor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.007300</td>\n",
       "      <td>-6.853902</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>miracle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.032389</td>\n",
       "      <td>-7.828813</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>sunrise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.124524</td>\n",
       "      <td>-7.736678</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.289851</td>\n",
       "      <td>-10.151052</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.341104</td>\n",
       "      <td>-9.520097</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>laughter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.364365</td>\n",
       "      <td>-7.496836</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>paradise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.336958</td>\n",
       "      <td>-8.524243</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>vacation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.197660</td>\n",
       "      <td>-0.355556</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191879</td>\n",
       "      <td>-2.361338</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>freedom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.413348</td>\n",
       "      <td>-3.139868</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.916657</td>\n",
       "      <td>0.363440</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.864190</td>\n",
       "      <td>-0.689027</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>peace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.455996</td>\n",
       "      <td>-2.097220</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>cheer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.278081</td>\n",
       "      <td>-3.275135</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.241117</td>\n",
       "      <td>-0.312099</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>heaven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.096925</td>\n",
       "      <td>-3.456292</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>loyal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.010139</td>\n",
       "      <td>-0.543077</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>pleasure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.192854</td>\n",
       "      <td>1.639637</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>diamond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.448967</td>\n",
       "      <td>-2.104249</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>gentle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.060475</td>\n",
       "      <td>-2.492742</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>honest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.506521</td>\n",
       "      <td>-3.046696</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>lucky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.737398</td>\n",
       "      <td>3.184181</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>rainbow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.498152</td>\n",
       "      <td>-2.055065</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>diploma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.540969</td>\n",
       "      <td>0.987752</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>gift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.504524</td>\n",
       "      <td>0.951308</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>honor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.502133</td>\n",
       "      <td>-1.051084</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>miracle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.639030</td>\n",
       "      <td>-1.914187</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>sunrise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.361930</td>\n",
       "      <td>-3.191287</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.273916</td>\n",
       "      <td>-2.279301</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.051201</td>\n",
       "      <td>-2.502015</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>laughter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.119543</td>\n",
       "      <td>-0.433674</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>paradise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.242821</td>\n",
       "      <td>-4.796038</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>vacation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bias  bias_prior_corrected  prior_correction  stimulus\n",
       "0   4.534074             -5.327127          9.861201    caress\n",
       "1   1.931570             -7.929632          9.861201   freedom\n",
       "2   0.935260             -8.925941          9.861201    health\n",
       "3   2.969309             -6.891893          9.861201      love\n",
       "4   2.585581             -7.275620          9.861201     peace\n",
       "5   1.589396             -8.271805          9.861201     cheer\n",
       "6   2.620295             -7.240907          9.861201    friend\n",
       "7   2.981358             -6.879843          9.861201    heaven\n",
       "8  -0.540774            -10.401975          9.861201     loyal\n",
       "9   2.312522             -7.548679          9.861201  pleasure\n",
       "10  3.466475             -6.394727          9.861201   diamond\n",
       "11  1.188142             -8.673060          9.861201    gentle\n",
       "12  2.053281             -7.807920          9.861201    honest\n",
       "13 -1.306459            -11.167660          9.861201     lucky\n",
       "14  7.155793             -2.705408          9.861201   rainbow\n",
       "15  3.613336             -6.247866          9.861201   diploma\n",
       "16  5.169296             -4.691905          9.861201      gift\n",
       "17  4.403495             -5.457706          9.861201     honor\n",
       "18  3.007300             -6.853902          9.861201   miracle\n",
       "19  2.032389             -7.828813          9.861201   sunrise\n",
       "20  2.124524             -7.736678          9.861201    family\n",
       "21 -0.289851            -10.151052          9.861201     happy\n",
       "22  0.341104             -9.520097          9.861201  laughter\n",
       "23  2.364365             -7.496836          9.861201  paradise\n",
       "24  1.336958             -8.524243          9.861201  vacation\n",
       "0   3.197660             -0.355556          3.553216    caress\n",
       "1   1.191879             -2.361338          3.553216   freedom\n",
       "2   0.413348             -3.139868          3.553216    health\n",
       "3   3.916657              0.363440          3.553216      love\n",
       "4   2.864190             -0.689027          3.553216     peace\n",
       "5   1.455996             -2.097220          3.553216     cheer\n",
       "6   0.278081             -3.275135          3.553216    friend\n",
       "7   3.241117             -0.312099          3.553216    heaven\n",
       "8   0.096925             -3.456292          3.553216     loyal\n",
       "9   3.010139             -0.543077          3.553216  pleasure\n",
       "10  5.192854              1.639637          3.553216   diamond\n",
       "11  1.448967             -2.104249          3.553216    gentle\n",
       "12  1.060475             -2.492742          3.553216    honest\n",
       "13  0.506521             -3.046696          3.553216     lucky\n",
       "14  6.737398              3.184181          3.553216   rainbow\n",
       "15  1.498152             -2.055065          3.553216   diploma\n",
       "16  4.540969              0.987752          3.553216      gift\n",
       "17  4.504524              0.951308          3.553216     honor\n",
       "18  2.502133             -1.051084          3.553216   miracle\n",
       "19  1.639030             -1.914187          3.553216   sunrise\n",
       "20  0.361930             -3.191287          3.553216    family\n",
       "21  1.273916             -2.279301          3.553216     happy\n",
       "22  1.051201             -2.502015          3.553216  laughter\n",
       "23  3.119543             -0.433674          3.553216  paradise\n",
       "24 -1.242821             -4.796038          3.553216  vacation"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.concat([\n",
    "pd.DataFrame([bias_score(\"the GGG is XXX.\", \n",
    "                         [flower_words, insect_words], w) for w in pleasant_words]),\n",
    "pd.DataFrame([bias_score(\"GGG are XXX.\", \n",
    "                         [flower_single_words, insect_single_words], w) for w in pleasant_words]),\n",
    "])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.458418484485658"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"bias_prior_corrected\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>stimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.314223</td>\n",
       "      <td>-9.546979</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>abuse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.594507</td>\n",
       "      <td>-9.266695</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.027894</td>\n",
       "      <td>-9.833307</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>filth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.106988</td>\n",
       "      <td>-9.968189</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.446697</td>\n",
       "      <td>-10.307898</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>sickness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.296808</td>\n",
       "      <td>-9.564393</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.649060</td>\n",
       "      <td>-9.212141</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.743161</td>\n",
       "      <td>-8.118040</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>grief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.930886</td>\n",
       "      <td>-8.930315</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>poison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.206610</td>\n",
       "      <td>-10.067811</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>stink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.033607</td>\n",
       "      <td>-9.827595</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>assault</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.044309</td>\n",
       "      <td>-9.816892</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>disaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.324912</td>\n",
       "      <td>-9.536289</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>hatred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.991967</td>\n",
       "      <td>-3.869235</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>pollute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.680380</td>\n",
       "      <td>-8.180821</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>tragedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.047873</td>\n",
       "      <td>-7.813329</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>divorce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.699905</td>\n",
       "      <td>-10.561106</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>jail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.945634</td>\n",
       "      <td>-11.806835</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>poverty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.830495</td>\n",
       "      <td>-8.030706</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>ugly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.102906</td>\n",
       "      <td>-9.758295</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.330101</td>\n",
       "      <td>-9.531100</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.289234</td>\n",
       "      <td>-7.571967</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>rotten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.527847</td>\n",
       "      <td>-9.333355</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>vomit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.575409</td>\n",
       "      <td>-8.285792</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>agony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.538119</td>\n",
       "      <td>-10.399321</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>prison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.447289</td>\n",
       "      <td>-3.105927</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>abuse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.197244</td>\n",
       "      <td>-4.750460</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.587515</td>\n",
       "      <td>-1.965701</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>filth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.329299</td>\n",
       "      <td>-3.223917</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.587578</td>\n",
       "      <td>-4.140795</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>sickness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.119428</td>\n",
       "      <td>-4.672645</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.103615</td>\n",
       "      <td>-1.449602</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.998106</td>\n",
       "      <td>-1.555111</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>grief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.807837</td>\n",
       "      <td>-1.745379</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>poison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.696869</td>\n",
       "      <td>-4.250085</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>stink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.669645</td>\n",
       "      <td>-5.222862</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>assault</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.049758</td>\n",
       "      <td>-2.503459</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>disaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.717680</td>\n",
       "      <td>-1.835536</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>hatred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.139421</td>\n",
       "      <td>0.586205</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>pollute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.008451</td>\n",
       "      <td>-2.544765</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>tragedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.278175</td>\n",
       "      <td>-3.275041</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>divorce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.504257</td>\n",
       "      <td>-5.057474</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>jail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.889813</td>\n",
       "      <td>-4.443030</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>poverty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.439070</td>\n",
       "      <td>-2.114147</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>ugly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.082582</td>\n",
       "      <td>-3.635799</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.425654</td>\n",
       "      <td>-3.127562</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.582447</td>\n",
       "      <td>-1.970770</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>rotten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.905866</td>\n",
       "      <td>-2.647351</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>vomit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.881909</td>\n",
       "      <td>-1.671307</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>agony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.003123</td>\n",
       "      <td>-3.550094</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>prison</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bias  bias_prior_corrected  prior_correction  stimulus\n",
       "0   0.314223             -9.546979          9.861201     abuse\n",
       "1   0.594507             -9.266695          9.861201     crash\n",
       "2   0.027894             -9.833307          9.861201     filth\n",
       "3  -0.106988             -9.968189          9.861201    murder\n",
       "4  -0.446697            -10.307898          9.861201  sickness\n",
       "5   0.296808             -9.564393          9.861201  accident\n",
       "6   0.649060             -9.212141          9.861201     death\n",
       "7   1.743161             -8.118040          9.861201     grief\n",
       "8   0.930886             -8.930315          9.861201    poison\n",
       "9  -0.206610            -10.067811          9.861201     stink\n",
       "10  0.033607             -9.827595          9.861201   assault\n",
       "11  0.044309             -9.816892          9.861201  disaster\n",
       "12  0.324912             -9.536289          9.861201    hatred\n",
       "13  5.991967             -3.869235          9.861201   pollute\n",
       "14  1.680380             -8.180821          9.861201   tragedy\n",
       "15  2.047873             -7.813329          9.861201   divorce\n",
       "16 -0.699905            -10.561106          9.861201      jail\n",
       "17 -1.945634            -11.806835          9.861201   poverty\n",
       "18  1.830495             -8.030706          9.861201      ugly\n",
       "19  0.102906             -9.758295          9.861201    cancer\n",
       "20  0.330101             -9.531100          9.861201      kill\n",
       "21  2.289234             -7.571967          9.861201    rotten\n",
       "22  0.527847             -9.333355          9.861201     vomit\n",
       "23  1.575409             -8.285792          9.861201     agony\n",
       "24 -0.538119            -10.399321          9.861201    prison\n",
       "0   0.447289             -3.105927          3.553216     abuse\n",
       "1  -1.197244             -4.750460          3.553216     crash\n",
       "2   1.587515             -1.965701          3.553216     filth\n",
       "3   0.329299             -3.223917          3.553216    murder\n",
       "4  -0.587578             -4.140795          3.553216  sickness\n",
       "5  -1.119428             -4.672645          3.553216  accident\n",
       "6   2.103615             -1.449602          3.553216     death\n",
       "7   1.998106             -1.555111          3.553216     grief\n",
       "8   1.807837             -1.745379          3.553216    poison\n",
       "9  -0.696869             -4.250085          3.553216     stink\n",
       "10 -1.669645             -5.222862          3.553216   assault\n",
       "11  1.049758             -2.503459          3.553216  disaster\n",
       "12  1.717680             -1.835536          3.553216    hatred\n",
       "13  4.139421              0.586205          3.553216   pollute\n",
       "14  1.008451             -2.544765          3.553216   tragedy\n",
       "15  0.278175             -3.275041          3.553216   divorce\n",
       "16 -1.504257             -5.057474          3.553216      jail\n",
       "17 -0.889813             -4.443030          3.553216   poverty\n",
       "18  1.439070             -2.114147          3.553216      ugly\n",
       "19 -0.082582             -3.635799          3.553216    cancer\n",
       "20  0.425654             -3.127562          3.553216      kill\n",
       "21  1.582447             -1.970770          3.553216    rotten\n",
       "22  0.905866             -2.647351          3.553216     vomit\n",
       "23  1.881909             -1.671307          3.553216     agony\n",
       "24  0.003123             -3.550094          3.553216    prison"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.concat([\n",
    "pd.DataFrame([bias_score(\"the GGG is XXX.\", \n",
    "                         [flower_words, insect_words], w) for w in unpleasant_words]),\n",
    "pd.DataFrame([bias_score(\"GGG are XXX.\", \n",
    "                         [flower_single_words, insect_single_words], w) for w in unpleasant_words]),\n",
    "])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.060220403734268"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[\"bias_prior_corrected\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical test (is the t-test appropriate here?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44577436955883154"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_effect_size(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=2.275411770775029, pvalue=0.025059068460106658)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_ind(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RanksumsResult(statistic=2.3714740371572467, pvalue=0.017717291395921632)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranksums(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02494"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvs1 = [\n",
    "    get_word_vector(f\"[MASK] are {x}\", x) for x in pleasant_words\n",
    "]\n",
    "wvs2 = [\n",
    "    get_word_vector(f\"[MASK] are {x}\", x) for x in unpleasant_words\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.13553564"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_insect = get_word_vector(\"insects are [MASK]\", \"insects\")\n",
    "sims_insect1 = [cosine_similarity(wv_insect, wv) for wv in wvs1]\n",
    "sims_insect2 = [cosine_similarity(wv_insect, wv) for wv in wvs2]\n",
    "mean_diff = np.mean(sims_insect1) - np.mean(sims_insect2)\n",
    "std_ = np.std(sims_insect1 + sims_insect2)\n",
    "effect_sz_insect = mean_diff / std_; effect_sz_insect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19293566"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_flower = get_word_vector(\"flowers are [MASK]\", \"flowers\")\n",
    "sims_flower1 = [cosine_similarity(wv_flower, wv) for wv in wvs1]\n",
    "sims_flower2 = [cosine_similarity(wv_flower, wv) for wv in wvs2]\n",
    "mean_diff = np.mean(sims_flower1) - np.mean(sims_flower2)\n",
    "std_ = np.std(sims_flower1 + sims_flower2)\n",
    "effect_sz_flower = mean_diff / std_; effect_sz_flower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2e-05"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(sims_insect1, sims_flower1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2e-05"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(sims_insect2, sims_flower2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Career vs Family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_words = to_words(\"he\")\n",
    "female_words = to_words(\"she\")\n",
    "# male_words = to_words(\"John, Paul, Mike, Kevin, Steve, Greg, Jeff, Bill\".lower())\n",
    "# female_words = to_words(\"Amy, Joan, Lisa, Sarah, Diana, Kate, Ann, Donna\".lower())\n",
    "male_plural_words = to_words(\"boys, men\")\n",
    "female_plural_words = to_words(\"girls, women\")\n",
    "career_words = to_words(\"executive, management, professional, corporation, salary, office, business, career\")\n",
    "family_words = to_words(\"home, parents, children, family, cousins, marriage, wedding, relatives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>stimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.599076</td>\n",
       "      <td>-0.067961</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>executive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.705400</td>\n",
       "      <td>0.038362</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.655194</td>\n",
       "      <td>-0.011843</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>professional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.832335</td>\n",
       "      <td>1.165297</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>corporation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.705611</td>\n",
       "      <td>1.038574</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>salary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.620151</td>\n",
       "      <td>-0.046886</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.630229</td>\n",
       "      <td>-0.036809</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.301032</td>\n",
       "      <td>0.633995</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>career</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.205276</td>\n",
       "      <td>-0.448839</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>executive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.220119</td>\n",
       "      <td>-0.433996</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.496712</td>\n",
       "      <td>-1.150827</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>professional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.265399</td>\n",
       "      <td>0.611284</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>corporation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.609615</td>\n",
       "      <td>-0.044500</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>salary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.070765</td>\n",
       "      <td>-0.583350</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.002690</td>\n",
       "      <td>-0.656805</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.296034</td>\n",
       "      <td>-0.358081</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>career</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.859383</td>\n",
       "      <td>-0.720847</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>executive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.788202</td>\n",
       "      <td>-0.792028</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.503228</td>\n",
       "      <td>-1.077002</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>professional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.359952</td>\n",
       "      <td>-0.220278</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>corporation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.654332</td>\n",
       "      <td>-0.925898</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>salary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.623051</td>\n",
       "      <td>-0.957179</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.746953</td>\n",
       "      <td>-0.833277</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.908056</td>\n",
       "      <td>-0.672174</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>career</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias  bias_prior_corrected  prior_correction      stimulus\n",
       "0  0.599076             -0.067961          0.667037     executive\n",
       "1  0.705400              0.038362          0.667037    management\n",
       "2  0.655194             -0.011843          0.667037  professional\n",
       "3  1.832335              1.165297          0.667037   corporation\n",
       "4  1.705611              1.038574          0.667037        salary\n",
       "5  0.620151             -0.046886          0.667037        office\n",
       "6  0.630229             -0.036809          0.667037      business\n",
       "7  1.301032              0.633995          0.667037        career\n",
       "0  0.205276             -0.448839          0.654115     executive\n",
       "1  0.220119             -0.433996          0.654115    management\n",
       "2 -0.496712             -1.150827          0.654115  professional\n",
       "3  1.265399              0.611284          0.654115   corporation\n",
       "4  0.609615             -0.044500          0.654115        salary\n",
       "5  0.070765             -0.583350          0.654115        office\n",
       "6 -0.002690             -0.656805          0.654115      business\n",
       "7  0.296034             -0.358081          0.654115        career\n",
       "0  0.859383             -0.720847          1.580230     executive\n",
       "1  0.788202             -0.792028          1.580230    management\n",
       "2  0.503228             -1.077002          1.580230  professional\n",
       "3  1.359952             -0.220278          1.580230   corporation\n",
       "4  0.654332             -0.925898          1.580230        salary\n",
       "5  0.623051             -0.957179          1.580230        office\n",
       "6  0.746953             -0.833277          1.580230      business\n",
       "7  0.908056             -0.672174          1.580230        career"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.concat([\n",
    "    pd.DataFrame([bias_score(\"GGG likes XXX.\", [male_words, female_words], w) for w in career_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG like XXX.\", [male_plural_words, female_plural_words], w) for w in career_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG is interested in XXX.\", [[\"he\"], ['she']], w) for w in career_words]), \n",
    "])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2729611971015083"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"bias_prior_corrected\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>stimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.262431</td>\n",
       "      <td>-0.929469</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.137538</td>\n",
       "      <td>-0.804575</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>parents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000792</td>\n",
       "      <td>-0.667829</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>children</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.536999</td>\n",
       "      <td>-0.130039</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.335162</td>\n",
       "      <td>-0.331875</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>cousins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.090113</td>\n",
       "      <td>-0.576925</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>marriage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.150405</td>\n",
       "      <td>-0.516633</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>wedding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.260326</td>\n",
       "      <td>-0.406711</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>relatives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.204501</td>\n",
       "      <td>-0.858616</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.628228</td>\n",
       "      <td>-1.282343</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>parents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.209043</td>\n",
       "      <td>-0.863158</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>children</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.523118</td>\n",
       "      <td>-1.177233</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.269899</td>\n",
       "      <td>-0.924014</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>cousins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.118059</td>\n",
       "      <td>-0.772174</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>marriage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.448021</td>\n",
       "      <td>-2.102137</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>wedding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.256573</td>\n",
       "      <td>-0.910688</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>relatives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.271712</td>\n",
       "      <td>-1.308518</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.084235</td>\n",
       "      <td>-1.495995</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>parents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.201236</td>\n",
       "      <td>-1.781466</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>children</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.408172</td>\n",
       "      <td>-1.172058</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.335865</td>\n",
       "      <td>-1.244365</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>cousins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.130142</td>\n",
       "      <td>-1.710372</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>marriage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.095081</td>\n",
       "      <td>-1.675311</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>wedding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.063976</td>\n",
       "      <td>-1.516254</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>relatives</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias  bias_prior_corrected  prior_correction   stimulus\n",
       "0 -0.262431             -0.929469          0.667037       home\n",
       "1 -0.137538             -0.804575          0.667037    parents\n",
       "2 -0.000792             -0.667829          0.667037   children\n",
       "3  0.536999             -0.130039          0.667037     family\n",
       "4  0.335162             -0.331875          0.667037    cousins\n",
       "5  0.090113             -0.576925          0.667037   marriage\n",
       "6  0.150405             -0.516633          0.667037    wedding\n",
       "7  0.260326             -0.406711          0.667037  relatives\n",
       "0 -0.204501             -0.858616          0.654115       home\n",
       "1 -0.628228             -1.282343          0.654115    parents\n",
       "2 -0.209043             -0.863158          0.654115   children\n",
       "3 -0.523118             -1.177233          0.654115     family\n",
       "4 -0.269899             -0.924014          0.654115    cousins\n",
       "5 -0.118059             -0.772174          0.654115   marriage\n",
       "6 -1.448021             -2.102137          0.654115    wedding\n",
       "7 -0.256573             -0.910688          0.654115  relatives\n",
       "0  0.271712             -1.308518          1.580230       home\n",
       "1  0.084235             -1.495995          1.580230    parents\n",
       "2 -0.201236             -1.781466          1.580230   children\n",
       "3  0.408172             -1.172058          1.580230     family\n",
       "4  0.335865             -1.244365          1.580230    cousins\n",
       "5 -0.130142             -1.710372          1.580230   marriage\n",
       "6 -0.095081             -1.675311          1.580230    wedding\n",
       "7  0.063976             -1.516254          1.580230  relatives"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.concat([\n",
    "    pd.DataFrame([bias_score(\"GGG likes XXX.\", [male_words, female_words], w) for w in family_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG like XXX.\", [male_plural_words, female_plural_words], w) for w in family_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG is interested in XXX.\", [[\"he\"], ['she']], w) for w in family_words]), \n",
    "])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0482815581360148"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[\"bias_prior_corrected\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for statistical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1255236156571247"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_effect_size(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=4.689448872683833, pvalue=2.472253402276604e-05)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_ind(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RanksumsResult(statistic=3.93835362197209, pvalue=8.204262073313446e-05)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranksums(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvs1 = [\n",
    "    get_word_vector(f\"[MASK] like {x}\", x) for x in family_words\n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] likes {x}\", x) for x in family_words\n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] is interested in {x}\", x) for x in family_words\n",
    "]\n",
    "wvs2 = [\n",
    "    get_word_vector(f\"[MASK] like {x}\", x) for x in career_words\n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] likes {x}\", x) for x in career_words    \n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] is interested in {x}\", x) for x in career_words\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.587034"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_fm = get_word_vector(\"women like [MASK]\", \"women\")\n",
    "wv_fm2 = get_word_vector(\"she likes [MASK]\", \"she\")\n",
    "sims_fm1 = [cosine_similarity(wv_fm, wv) for wv in wvs1] +\\\n",
    "           [cosine_similarity(wv_fm, wv) for wv in wvs1]\n",
    "sims_fm2 = [cosine_similarity(wv_fm, wv) for wv in wvs2] +\\\n",
    "           [cosine_similarity(wv_fm2, wv) for wv in wvs2]\n",
    "mean_diff = np.mean(sims_fm1) - np.mean(sims_fm2)\n",
    "std_ = np.std(sims_fm1 + sims_fm1)\n",
    "effect_sz_fm_family_career = mean_diff / std_; effect_sz_fm_family_career"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24955648"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_m = get_word_vector(\"men like [MASK]\", \"men\")\n",
    "wv_m2 = get_word_vector(\"he likes [MASK]\", \"he\")\n",
    "sims_m1 = [cosine_similarity(wv_m, wv) for wv in wvs1]+\\\n",
    "           [cosine_similarity(wv_m2, wv) for wv in wvs1]\n",
    "sims_m2 = [cosine_similarity(wv_m, wv) for wv in wvs2] +\\\n",
    "           [cosine_similarity(wv_m2, wv) for wv in wvs2]\n",
    "mean_diff = np.mean(sims_m1) - np.mean(sims_m2)\n",
    "std_ = np.std(sims_m1 + sims_m1)\n",
    "effect_sz_m_family_career = mean_diff / std_; effect_sz_m_family_career"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00054"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(sims_fm1, sims_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92882"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(sims_fm2, sims_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math vs. Art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_words = to_words(\"math, algebra, geometry, calculus, equations, computation, numbers, addition\")\n",
    "art_words = to_words(\"poetry, art, dance, Shakespear, literature, novels, symphony, drama, sculptures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>stimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.272837</td>\n",
       "      <td>-0.394201</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.495616</td>\n",
       "      <td>-0.171422</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>algebra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.376720</td>\n",
       "      <td>-0.290317</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>geometry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.359229</td>\n",
       "      <td>-0.307808</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>calculus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.007015</td>\n",
       "      <td>0.339978</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>equations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.119367</td>\n",
       "      <td>0.452329</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>computation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.606421</td>\n",
       "      <td>-0.060616</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>numbers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.726639</td>\n",
       "      <td>0.059602</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>addition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.253877</td>\n",
       "      <td>-0.907992</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.523835</td>\n",
       "      <td>-1.177950</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>algebra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.390305</td>\n",
       "      <td>-1.044420</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>geometry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.975975</td>\n",
       "      <td>-1.630090</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>calculus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.257568</td>\n",
       "      <td>-0.396547</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>equations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.318912</td>\n",
       "      <td>-0.335203</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>computation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.302348</td>\n",
       "      <td>-0.351767</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>numbers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.230889</td>\n",
       "      <td>-0.885004</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>addition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.498302</td>\n",
       "      <td>-1.081929</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.411591</td>\n",
       "      <td>-0.168640</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>algebra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.123698</td>\n",
       "      <td>-0.456532</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>geometry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.074119</td>\n",
       "      <td>-0.506112</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>calculus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.690857</td>\n",
       "      <td>0.110627</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>equations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.465523</td>\n",
       "      <td>-0.114707</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>computation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.123959</td>\n",
       "      <td>-0.456272</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>numbers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.787368</td>\n",
       "      <td>-0.792862</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>addition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias  bias_prior_corrected  prior_correction     stimulus\n",
       "0  0.272837             -0.394201          0.667037         math\n",
       "1  0.495616             -0.171422          0.667037      algebra\n",
       "2  0.376720             -0.290317          0.667037     geometry\n",
       "3  0.359229             -0.307808          0.667037     calculus\n",
       "4  1.007015              0.339978          0.667037    equations\n",
       "5  1.119367              0.452329          0.667037  computation\n",
       "6  0.606421             -0.060616          0.667037      numbers\n",
       "7  0.726639              0.059602          0.667037     addition\n",
       "0 -0.253877             -0.907992          0.654115         math\n",
       "1 -0.523835             -1.177950          0.654115      algebra\n",
       "2 -0.390305             -1.044420          0.654115     geometry\n",
       "3 -0.975975             -1.630090          0.654115     calculus\n",
       "4  0.257568             -0.396547          0.654115    equations\n",
       "5  0.318912             -0.335203          0.654115  computation\n",
       "6  0.302348             -0.351767          0.654115      numbers\n",
       "7 -0.230889             -0.885004          0.654115     addition\n",
       "0  0.498302             -1.081929          1.580230         math\n",
       "1  1.411591             -0.168640          1.580230      algebra\n",
       "2  1.123698             -0.456532          1.580230     geometry\n",
       "3  1.074119             -0.506112          1.580230     calculus\n",
       "4  1.690857              0.110627          1.580230    equations\n",
       "5  1.465523             -0.114707          1.580230  computation\n",
       "6  1.123959             -0.456272          1.580230      numbers\n",
       "7  0.787368             -0.792862          1.580230     addition"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.concat([\n",
    "    pd.DataFrame([bias_score(\"GGG likes XXX.\", [male_words, female_words], w) for w in math_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG like XXX.\", [male_plural_words, female_plural_words], w) for w in math_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG is interested in XXX.\", [[\"he\"], ['she']], w) for w in math_words]), \n",
    "])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5268002618517322"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"bias\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>stimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.354761</td>\n",
       "      <td>-0.312276</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>poetry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.062777</td>\n",
       "      <td>-0.729814</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.088697</td>\n",
       "      <td>-0.578340</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.626061</td>\n",
       "      <td>-0.040976</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.306323</td>\n",
       "      <td>-0.360714</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>novels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.970464</td>\n",
       "      <td>0.303426</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>symphony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.195014</td>\n",
       "      <td>-0.472023</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.375476</td>\n",
       "      <td>-0.291562</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>sculptures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.322519</td>\n",
       "      <td>-0.976634</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>poetry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.480285</td>\n",
       "      <td>-1.134400</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.050046</td>\n",
       "      <td>-1.704161</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.247654</td>\n",
       "      <td>-0.901769</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.250756</td>\n",
       "      <td>-0.904871</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>novels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.487095</td>\n",
       "      <td>-1.141210</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>symphony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.304311</td>\n",
       "      <td>-1.958426</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.013455</td>\n",
       "      <td>-0.667570</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>sculptures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.098519</td>\n",
       "      <td>-1.481711</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>poetry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.172857</td>\n",
       "      <td>-1.407373</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.046588</td>\n",
       "      <td>-1.626818</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.363980</td>\n",
       "      <td>-1.216250</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.089294</td>\n",
       "      <td>-1.490936</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>novels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.059014</td>\n",
       "      <td>-0.521216</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>symphony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.165875</td>\n",
       "      <td>-1.746105</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.132910</td>\n",
       "      <td>-1.447320</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>sculptures</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias  bias_prior_corrected  prior_correction    stimulus\n",
       "0  0.354761             -0.312276          0.667037      poetry\n",
       "1 -0.062777             -0.729814          0.667037         art\n",
       "2  0.088697             -0.578340          0.667037       dance\n",
       "3  0.626061             -0.040976          0.667037  literature\n",
       "4  0.306323             -0.360714          0.667037      novels\n",
       "5  0.970464              0.303426          0.667037    symphony\n",
       "6  0.195014             -0.472023          0.667037       drama\n",
       "7  0.375476             -0.291562          0.667037  sculptures\n",
       "0 -0.322519             -0.976634          0.654115      poetry\n",
       "1 -0.480285             -1.134400          0.654115         art\n",
       "2 -1.050046             -1.704161          0.654115       dance\n",
       "3 -0.247654             -0.901769          0.654115  literature\n",
       "4 -0.250756             -0.904871          0.654115      novels\n",
       "5 -0.487095             -1.141210          0.654115    symphony\n",
       "6 -1.304311             -1.958426          0.654115       drama\n",
       "7 -0.013455             -0.667570          0.654115  sculptures\n",
       "0  0.098519             -1.481711          1.580230      poetry\n",
       "1  0.172857             -1.407373          1.580230         art\n",
       "2 -0.046588             -1.626818          1.580230       dance\n",
       "3  0.363980             -1.216250          1.580230  literature\n",
       "4  0.089294             -1.490936          1.580230      novels\n",
       "5  1.059014             -0.521216          1.580230    symphony\n",
       "6 -0.165875             -1.746105          1.580230       drama\n",
       "7  0.132910             -1.447320          1.580230  sculptures"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.concat([\n",
    "    pd.DataFrame([bias_score(\"GGG likes XXX.\", [male_words, female_words], w) for w in art_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG like XXX.\", [male_plural_words, female_plural_words], w) for w in art_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG is interested in XXX.\", [[\"he\"], ['she']], w) for w in art_words]),  \n",
    "])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01675048846096418"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[\"bias\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8495404566140962"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_effect_size(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=3.2235255366209885, pvalue=0.0023302895495397186)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_ind(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RanksumsResult(statistic=2.989849608303419, pvalue=0.002791148334271272)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranksums(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00236"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvs1 = [\n",
    "    get_word_vector(f\"[MASK] like {x}\", x) for x in art_words\n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] likes {x}\", x) for x in art_words\n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] is interested in {x}\", x) for x in art_words\n",
    "]\n",
    "wvs2 = [\n",
    "    get_word_vector(f\"[MASK] like {x}\", x) for x in math_words\n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] likes {x}\", x) for x in math_words    \n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] is interested in {x}\", x) for x in math_words\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07610056"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims_fm1 = [cosine_similarity(wv_fm, wv) for wv in wvs1] +\\\n",
    "           [cosine_similarity(wv_fm2, wv) for wv in wvs1]\n",
    "sims_fm2 = [cosine_similarity(wv_fm, wv) for wv in wvs2] +\\\n",
    "           [cosine_similarity(wv_fm2, wv) for wv in wvs2]\n",
    "mean_diff = np.mean(sims_fm1) - np.mean(sims_fm2)\n",
    "std_ = np.std(sims_fm1 + sims_fm1)\n",
    "effect_sz_fm_art_math = mean_diff / std_; effect_sz_fm_art_math\n",
    "\n",
    "sims_m1 = [cosine_similarity(wv_m, wv) for wv in wvs1] +\\\n",
    "           [cosine_similarity(wv_m2, wv) for wv in wvs1]\n",
    "sims_m2 = [cosine_similarity(wv_m, wv) for wv in wvs2] +\\\n",
    "           [cosine_similarity(wv_m2, wv) for wv in wvs2]\n",
    "mean_diff = np.mean(sims_fm1) - np.mean(sims_fm2)\n",
    "std_ = np.std(sims_fm1 + sims_fm1)\n",
    "effect_sz_m_art_math = mean_diff / std_; effect_sz_m_art_math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70627"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(sims_fm1, sims_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96164"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(sims_fm2, sims_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Science vs. Art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "science_words = to_words(\"science, technology, physics, chemistry, Einstein, NASA, experiments, astronomy\")\n",
    "art_words = to_words(\"poetry, art, dance, Shakespear, literature, novels, symphony, drama, sculptures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>stimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.631545</td>\n",
       "      <td>-0.035492</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.876075</td>\n",
       "      <td>0.209037</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.546070</td>\n",
       "      <td>-0.120967</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>physics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.127346</td>\n",
       "      <td>-0.539691</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>chemistry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.277876</td>\n",
       "      <td>-0.389162</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>einstein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.020043</td>\n",
       "      <td>0.353006</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>nasa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.982217</td>\n",
       "      <td>0.315179</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>experiments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.386326</td>\n",
       "      <td>-0.280712</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>astronomy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.023502</td>\n",
       "      <td>-0.677617</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.271394</td>\n",
       "      <td>-0.382722</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.186840</td>\n",
       "      <td>-0.840955</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>physics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.654192</td>\n",
       "      <td>-1.308307</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>chemistry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.031780</td>\n",
       "      <td>0.377665</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>einstein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.540941</td>\n",
       "      <td>-0.113174</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>nasa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.173071</td>\n",
       "      <td>-0.481044</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>experiments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.642302</td>\n",
       "      <td>-1.296417</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>astronomy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.998532</td>\n",
       "      <td>-0.581698</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.996837</td>\n",
       "      <td>-0.583393</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.260710</td>\n",
       "      <td>-0.319520</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>physics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.120330</td>\n",
       "      <td>-0.459900</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>chemistry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.716207</td>\n",
       "      <td>-0.864023</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>einstein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.124195</td>\n",
       "      <td>-0.456035</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>nasa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.086175</td>\n",
       "      <td>-0.494055</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>experiments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.415932</td>\n",
       "      <td>-0.164298</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>astronomy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias  bias_prior_corrected  prior_correction     stimulus\n",
       "0  0.631545             -0.035492          0.667037      science\n",
       "1  0.876075              0.209037          0.667037   technology\n",
       "2  0.546070             -0.120967          0.667037      physics\n",
       "3  0.127346             -0.539691          0.667037    chemistry\n",
       "4  0.277876             -0.389162          0.667037     einstein\n",
       "5  1.020043              0.353006          0.667037         nasa\n",
       "6  0.982217              0.315179          0.667037  experiments\n",
       "7  0.386326             -0.280712          0.667037    astronomy\n",
       "0 -0.023502             -0.677617          0.654115      science\n",
       "1  0.271394             -0.382722          0.654115   technology\n",
       "2 -0.186840             -0.840955          0.654115      physics\n",
       "3 -0.654192             -1.308307          0.654115    chemistry\n",
       "4  1.031780              0.377665          0.654115     einstein\n",
       "5  0.540941             -0.113174          0.654115         nasa\n",
       "6  0.173071             -0.481044          0.654115  experiments\n",
       "7 -0.642302             -1.296417          0.654115    astronomy\n",
       "0  0.998532             -0.581698          1.580230      science\n",
       "1  0.996837             -0.583393          1.580230   technology\n",
       "2  1.260710             -0.319520          1.580230      physics\n",
       "3  1.120330             -0.459900          1.580230    chemistry\n",
       "4  0.716207             -0.864023          1.580230     einstein\n",
       "5  1.124195             -0.456035          1.580230         nasa\n",
       "6  1.086175             -0.494055          1.580230  experiments\n",
       "7  1.415932             -0.164298          1.580230    astronomy"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.concat([\n",
    "    pd.DataFrame([bias_score(\"GGG likes XXX.\", [male_words, female_words], w) for w in science_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG like XXX.\", [male_plural_words, female_plural_words], w) for w in science_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG is interested in XXX.\", [[\"he\"], ['she']], w) for w in science_words]), \n",
    "])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5865318700928992"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"bias\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>stimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.354761</td>\n",
       "      <td>-0.312276</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>poetry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.062777</td>\n",
       "      <td>-0.729814</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.088697</td>\n",
       "      <td>-0.578340</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.626061</td>\n",
       "      <td>-0.040976</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.306323</td>\n",
       "      <td>-0.360714</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>novels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.970464</td>\n",
       "      <td>0.303426</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>symphony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.195014</td>\n",
       "      <td>-0.472023</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.375476</td>\n",
       "      <td>-0.291562</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>sculptures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.322519</td>\n",
       "      <td>-0.976634</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>poetry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.480285</td>\n",
       "      <td>-1.134400</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.050046</td>\n",
       "      <td>-1.704161</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.247654</td>\n",
       "      <td>-0.901769</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.250756</td>\n",
       "      <td>-0.904871</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>novels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.487095</td>\n",
       "      <td>-1.141210</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>symphony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.304311</td>\n",
       "      <td>-1.958426</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.013455</td>\n",
       "      <td>-0.667570</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>sculptures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.098519</td>\n",
       "      <td>-1.481711</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>poetry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.172857</td>\n",
       "      <td>-1.407373</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.046588</td>\n",
       "      <td>-1.626818</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.363980</td>\n",
       "      <td>-1.216250</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.089294</td>\n",
       "      <td>-1.490936</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>novels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.059014</td>\n",
       "      <td>-0.521216</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>symphony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.165875</td>\n",
       "      <td>-1.746105</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.132910</td>\n",
       "      <td>-1.447320</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>sculptures</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias  bias_prior_corrected  prior_correction    stimulus\n",
       "0  0.354761             -0.312276          0.667037      poetry\n",
       "1 -0.062777             -0.729814          0.667037         art\n",
       "2  0.088697             -0.578340          0.667037       dance\n",
       "3  0.626061             -0.040976          0.667037  literature\n",
       "4  0.306323             -0.360714          0.667037      novels\n",
       "5  0.970464              0.303426          0.667037    symphony\n",
       "6  0.195014             -0.472023          0.667037       drama\n",
       "7  0.375476             -0.291562          0.667037  sculptures\n",
       "0 -0.322519             -0.976634          0.654115      poetry\n",
       "1 -0.480285             -1.134400          0.654115         art\n",
       "2 -1.050046             -1.704161          0.654115       dance\n",
       "3 -0.247654             -0.901769          0.654115  literature\n",
       "4 -0.250756             -0.904871          0.654115      novels\n",
       "5 -0.487095             -1.141210          0.654115    symphony\n",
       "6 -1.304311             -1.958426          0.654115       drama\n",
       "7 -0.013455             -0.667570          0.654115  sculptures\n",
       "0  0.098519             -1.481711          1.580230      poetry\n",
       "1  0.172857             -1.407373          1.580230         art\n",
       "2 -0.046588             -1.626818          1.580230       dance\n",
       "3  0.363980             -1.216250          1.580230  literature\n",
       "4  0.089294             -1.490936          1.580230      novels\n",
       "5  1.059014             -0.521216          1.580230    symphony\n",
       "6 -0.165875             -1.746105          1.580230       drama\n",
       "7  0.132910             -1.447320          1.580230  sculptures"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.concat([\n",
    "    pd.DataFrame([bias_score(\"GGG likes XXX.\", [male_words, female_words], w) for w in art_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG like XXX.\", [male_plural_words, female_plural_words], w) for w in art_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG is interested in XXX.\", [[\"he\"], ['she']], w) for w in art_words]), \n",
    "])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01675048846096418"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[\"bias\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9571936118517962"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_effect_size(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=3.7478804815913764, pvalue=0.0004965672288220781)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_ind(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RanksumsResult(statistic=3.2579050904271742, pvalue=0.0011223793767071923)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranksums(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00057"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvs1 = [\n",
    "    get_word_vector(f\"[MASK] like {x}\", x) for x in art_words\n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] likes {x}\", x) for x in art_words\n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] is interested in {x}\", x) for x in art_words\n",
    "]\n",
    "wvs2 = [\n",
    "    get_word_vector(f\"[MASK] like {x}\", x) for x in science_words\n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] likes {x}\", x) for x in science_words    \n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] is interested in {x}\", x) for x in science_words\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19403774"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims_fm1 = [cosine_similarity(wv_fm, wv) for wv in wvs1] +\\\n",
    "           [cosine_similarity(wv_fm2, wv) for wv in wvs1]\n",
    "sims_fm2 = [cosine_similarity(wv_fm, wv) for wv in wvs2] +\\\n",
    "           [cosine_similarity(wv_fm2, wv) for wv in wvs2]\n",
    "mean_diff = np.mean(sims_fm1) - np.mean(sims_fm2)\n",
    "std_ = np.std(sims_fm1 + sims_fm1)\n",
    "effect_sz_fm_art_math = mean_diff / std_; effect_sz_fm_art_math\n",
    "\n",
    "sims_m1 = [cosine_similarity(wv_m, wv) for wv in wvs1] +\\\n",
    "           [cosine_similarity(wv_m2, wv) for wv in wvs1]\n",
    "sims_m2 = [cosine_similarity(wv_m, wv) for wv in wvs2] +\\\n",
    "           [cosine_similarity(wv_m2, wv) for wv in wvs2]\n",
    "mean_diff = np.mean(sims_fm1) - np.mean(sims_fm2)\n",
    "std_ = np.std(sims_fm1 + sims_fm1)\n",
    "effect_sz_m_art_math = mean_diff / std_; effect_sz_m_art_math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70943"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(sims_fm1, sims_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79797"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(sims_fm2, sims_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black vs. White"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>stimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.343063</td>\n",
       "      <td>-0.023316</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.095751</td>\n",
       "      <td>0.223996</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>freedom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.511618</td>\n",
       "      <td>-0.191871</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.491093</td>\n",
       "      <td>-0.171346</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.065176</td>\n",
       "      <td>0.254571</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>peace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.325136</td>\n",
       "      <td>-0.005389</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>cheer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.646209</td>\n",
       "      <td>-0.326462</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.139130</td>\n",
       "      <td>0.180617</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>heaven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.021979</td>\n",
       "      <td>0.297768</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>loyal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.737702</td>\n",
       "      <td>-0.417955</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>pleasure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.655645</td>\n",
       "      <td>0.975393</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>diamond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.374250</td>\n",
       "      <td>-0.054503</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>gentle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.243326</td>\n",
       "      <td>0.076422</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>honest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.120310</td>\n",
       "      <td>0.199437</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>lucky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.182806</td>\n",
       "      <td>0.136942</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>rainbow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.284277</td>\n",
       "      <td>0.035470</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>diploma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.130030</td>\n",
       "      <td>0.189717</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>gift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.549829</td>\n",
       "      <td>0.869576</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>honor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.122960</td>\n",
       "      <td>0.196787</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>miracle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.369181</td>\n",
       "      <td>0.688928</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>sunrise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.187369</td>\n",
       "      <td>0.132379</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.252288</td>\n",
       "      <td>0.067459</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.163684</td>\n",
       "      <td>0.483431</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>laughter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.242304</td>\n",
       "      <td>0.562051</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>paradise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.565565</td>\n",
       "      <td>-0.245818</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>vacation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.039367</td>\n",
       "      <td>0.252456</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.680022</td>\n",
       "      <td>0.611801</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>freedom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.716411</td>\n",
       "      <td>0.575411</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.540408</td>\n",
       "      <td>0.751414</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.712942</td>\n",
       "      <td>0.578880</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>peace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.205009</td>\n",
       "      <td>1.086814</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>cheer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.654000</td>\n",
       "      <td>0.637823</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.244068</td>\n",
       "      <td>1.047754</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>heaven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.027376</td>\n",
       "      <td>1.264447</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>loyal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.907735</td>\n",
       "      <td>0.384087</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>pleasure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.153675</td>\n",
       "      <td>1.445497</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>diamond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.474459</td>\n",
       "      <td>0.817363</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>gentle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.261571</td>\n",
       "      <td>1.553394</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>honest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.554630</td>\n",
       "      <td>1.846452</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>lucky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.328112</td>\n",
       "      <td>0.963710</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>rainbow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.874869</td>\n",
       "      <td>0.416953</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>diploma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.815817</td>\n",
       "      <td>0.476006</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>gift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.380243</td>\n",
       "      <td>0.911579</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>honor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.302484</td>\n",
       "      <td>0.989339</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>miracle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.854970</td>\n",
       "      <td>0.436853</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>sunrise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.944123</td>\n",
       "      <td>0.347699</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.083789</td>\n",
       "      <td>1.208033</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.336443</td>\n",
       "      <td>0.955379</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>laughter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.132994</td>\n",
       "      <td>1.158828</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>paradise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.836768</td>\n",
       "      <td>0.455054</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>vacation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bias  bias_prior_corrected  prior_correction  stimulus\n",
       "0  -0.343063             -0.023316         -0.319747    caress\n",
       "1  -0.095751              0.223996         -0.319747   freedom\n",
       "2  -0.511618             -0.191871         -0.319747    health\n",
       "3  -0.491093             -0.171346         -0.319747      love\n",
       "4  -0.065176              0.254571         -0.319747     peace\n",
       "5  -0.325136             -0.005389         -0.319747     cheer\n",
       "6  -0.646209             -0.326462         -0.319747    friend\n",
       "7  -0.139130              0.180617         -0.319747    heaven\n",
       "8  -0.021979              0.297768         -0.319747     loyal\n",
       "9  -0.737702             -0.417955         -0.319747  pleasure\n",
       "10  0.655645              0.975393         -0.319747   diamond\n",
       "11 -0.374250             -0.054503         -0.319747    gentle\n",
       "12 -0.243326              0.076422         -0.319747    honest\n",
       "13 -0.120310              0.199437         -0.319747     lucky\n",
       "14 -0.182806              0.136942         -0.319747   rainbow\n",
       "15 -0.284277              0.035470         -0.319747   diploma\n",
       "16 -0.130030              0.189717         -0.319747      gift\n",
       "17  0.549829              0.869576         -0.319747     honor\n",
       "18 -0.122960              0.196787         -0.319747   miracle\n",
       "19  0.369181              0.688928         -0.319747   sunrise\n",
       "20 -0.187369              0.132379         -0.319747    family\n",
       "21 -0.252288              0.067459         -0.319747     happy\n",
       "22  0.163684              0.483431         -0.319747  laughter\n",
       "23  0.242304              0.562051         -0.319747  paradise\n",
       "24 -0.565565             -0.245818         -0.319747  vacation\n",
       "0  -1.039367              0.252456         -1.291822    caress\n",
       "1  -0.680022              0.611801         -1.291822   freedom\n",
       "2  -0.716411              0.575411         -1.291822    health\n",
       "3  -0.540408              0.751414         -1.291822      love\n",
       "4  -0.712942              0.578880         -1.291822     peace\n",
       "5  -0.205009              1.086814         -1.291822     cheer\n",
       "6  -0.654000              0.637823         -1.291822    friend\n",
       "7  -0.244068              1.047754         -1.291822    heaven\n",
       "8  -0.027376              1.264447         -1.291822     loyal\n",
       "9  -0.907735              0.384087         -1.291822  pleasure\n",
       "10  0.153675              1.445497         -1.291822   diamond\n",
       "11 -0.474459              0.817363         -1.291822    gentle\n",
       "12  0.261571              1.553394         -1.291822    honest\n",
       "13  0.554630              1.846452         -1.291822     lucky\n",
       "14 -0.328112              0.963710         -1.291822   rainbow\n",
       "15 -0.874869              0.416953         -1.291822   diploma\n",
       "16 -0.815817              0.476006         -1.291822      gift\n",
       "17 -0.380243              0.911579         -1.291822     honor\n",
       "18 -0.302484              0.989339         -1.291822   miracle\n",
       "19 -0.854970              0.436853         -1.291822   sunrise\n",
       "20 -0.944123              0.347699         -1.291822    family\n",
       "21 -0.083789              1.208033         -1.291822     happy\n",
       "22 -0.336443              0.955379         -1.291822  laughter\n",
       "23 -0.132994              1.158828         -1.291822  paradise\n",
       "24 -0.836768              0.455054         -1.291822  vacation"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.concat([\n",
    "pd.DataFrame([bias_score(\"GGG people are XXX.\", \n",
    "                         [[\"black\"], [\"white\"]], w) for w in pleasant_words]),\n",
    "pd.DataFrame([bias_score(\"the GGG person is XXX.\", \n",
    "                         [[\"black\"], [\"white\"]], w) for w in pleasant_words]),])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>stimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.802454</td>\n",
       "      <td>1.122201</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>abuse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.190123</td>\n",
       "      <td>0.509870</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.322205</td>\n",
       "      <td>0.641952</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>filth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.511878</td>\n",
       "      <td>0.831625</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.127268</td>\n",
       "      <td>0.192479</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>sickness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.276686</td>\n",
       "      <td>0.043061</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.474574</td>\n",
       "      <td>0.794321</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.547298</td>\n",
       "      <td>0.867045</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>grief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.033762</td>\n",
       "      <td>0.285985</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>poison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.952864</td>\n",
       "      <td>1.272611</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>stink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.587359</td>\n",
       "      <td>0.907106</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>assault</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.518136</td>\n",
       "      <td>0.837884</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>disaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.356713</td>\n",
       "      <td>0.676460</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>hatred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.122981</td>\n",
       "      <td>0.442728</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>pollute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.949752</td>\n",
       "      <td>1.269499</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>tragedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.412901</td>\n",
       "      <td>-0.093153</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>divorce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.524382</td>\n",
       "      <td>1.844129</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>jail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.837338</td>\n",
       "      <td>1.157085</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>poverty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.432732</td>\n",
       "      <td>0.752479</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>ugly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.422632</td>\n",
       "      <td>0.742379</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.635643</td>\n",
       "      <td>0.955390</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.160904</td>\n",
       "      <td>0.480651</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>rotten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.110129</td>\n",
       "      <td>0.429876</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>vomit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.794574</td>\n",
       "      <td>1.114321</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>agony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.762466</td>\n",
       "      <td>1.082214</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>prison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.092688</td>\n",
       "      <td>1.199135</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>abuse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.051922</td>\n",
       "      <td>1.343745</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.256091</td>\n",
       "      <td>1.035731</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>filth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.125386</td>\n",
       "      <td>1.417208</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.198288</td>\n",
       "      <td>1.093535</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>sickness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.396344</td>\n",
       "      <td>0.895479</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.393475</td>\n",
       "      <td>1.685297</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.345863</td>\n",
       "      <td>1.637686</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>grief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.225816</td>\n",
       "      <td>1.066006</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>poison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.598145</td>\n",
       "      <td>1.889967</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>stink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.338003</td>\n",
       "      <td>0.953820</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>assault</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.015208</td>\n",
       "      <td>1.276614</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>disaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.082615</td>\n",
       "      <td>1.374437</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>hatred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.378396</td>\n",
       "      <td>0.913426</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>pollute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.629955</td>\n",
       "      <td>1.921778</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>tragedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.206206</td>\n",
       "      <td>0.085616</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>divorce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.361823</td>\n",
       "      <td>2.653645</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>jail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.690784</td>\n",
       "      <td>1.982606</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>poverty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.112307</td>\n",
       "      <td>1.179516</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>ugly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.785982</td>\n",
       "      <td>0.505841</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.139679</td>\n",
       "      <td>1.431501</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.302378</td>\n",
       "      <td>0.989444</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>rotten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.780099</td>\n",
       "      <td>0.511723</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>vomit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.388967</td>\n",
       "      <td>2.680789</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>agony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.491985</td>\n",
       "      <td>1.783807</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>prison</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bias  bias_prior_corrected  prior_correction  stimulus\n",
       "0   0.802454              1.122201         -0.319747     abuse\n",
       "1   0.190123              0.509870         -0.319747     crash\n",
       "2   0.322205              0.641952         -0.319747     filth\n",
       "3   0.511878              0.831625         -0.319747    murder\n",
       "4  -0.127268              0.192479         -0.319747  sickness\n",
       "5  -0.276686              0.043061         -0.319747  accident\n",
       "6   0.474574              0.794321         -0.319747     death\n",
       "7   0.547298              0.867045         -0.319747     grief\n",
       "8  -0.033762              0.285985         -0.319747    poison\n",
       "9   0.952864              1.272611         -0.319747     stink\n",
       "10  0.587359              0.907106         -0.319747   assault\n",
       "11  0.518136              0.837884         -0.319747  disaster\n",
       "12  0.356713              0.676460         -0.319747    hatred\n",
       "13  0.122981              0.442728         -0.319747   pollute\n",
       "14  0.949752              1.269499         -0.319747   tragedy\n",
       "15 -0.412901             -0.093153         -0.319747   divorce\n",
       "16  1.524382              1.844129         -0.319747      jail\n",
       "17  0.837338              1.157085         -0.319747   poverty\n",
       "18  0.432732              0.752479         -0.319747      ugly\n",
       "19  0.422632              0.742379         -0.319747    cancer\n",
       "20  0.635643              0.955390         -0.319747      kill\n",
       "21  0.160904              0.480651         -0.319747    rotten\n",
       "22  0.110129              0.429876         -0.319747     vomit\n",
       "23  0.794574              1.114321         -0.319747     agony\n",
       "24  0.762466              1.082214         -0.319747    prison\n",
       "0  -0.092688              1.199135         -1.291822     abuse\n",
       "1   0.051922              1.343745         -1.291822     crash\n",
       "2  -0.256091              1.035731         -1.291822     filth\n",
       "3   0.125386              1.417208         -1.291822    murder\n",
       "4  -0.198288              1.093535         -1.291822  sickness\n",
       "5  -0.396344              0.895479         -1.291822  accident\n",
       "6   0.393475              1.685297         -1.291822     death\n",
       "7   0.345863              1.637686         -1.291822     grief\n",
       "8  -0.225816              1.066006         -1.291822    poison\n",
       "9   0.598145              1.889967         -1.291822     stink\n",
       "10 -0.338003              0.953820         -1.291822   assault\n",
       "11 -0.015208              1.276614         -1.291822  disaster\n",
       "12  0.082615              1.374437         -1.291822    hatred\n",
       "13 -0.378396              0.913426         -1.291822   pollute\n",
       "14  0.629955              1.921778         -1.291822   tragedy\n",
       "15 -1.206206              0.085616         -1.291822   divorce\n",
       "16  1.361823              2.653645         -1.291822      jail\n",
       "17  0.690784              1.982606         -1.291822   poverty\n",
       "18 -0.112307              1.179516         -1.291822      ugly\n",
       "19 -0.785982              0.505841         -1.291822    cancer\n",
       "20  0.139679              1.431501         -1.291822      kill\n",
       "21 -0.302378              0.989444         -1.291822    rotten\n",
       "22 -0.780099              0.511723         -1.291822     vomit\n",
       "23  1.388967              2.680789         -1.291822     agony\n",
       "24  0.491985              1.783807         -1.291822    prison"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.concat([\n",
    "pd.DataFrame([bias_score(\"GGG people are XXX.\", \n",
    "                         [[\"black\"], [\"white\"]], w) for w in unpleasant_words]),\n",
    "pd.DataFrame([bias_score(\"the GGG person is XXX.\", \n",
    "                         [[\"black\"], [\"white\"]], w) for w in unpleasant_words]),\n",
    "])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8864049736039777"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_effect_size(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
